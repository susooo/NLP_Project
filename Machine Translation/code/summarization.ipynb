{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X4cRE8IbIrIV"
   },
   "source": [
    "If you're opening this Notebook on colab, you will probably need to install 🤗 Transformers and 🤗 Datasets as well as other dependencies. Uncomment the following cell and run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "MOsHUjgdIrIW",
    "outputId": "f84a093e-147f-470e-aad9-80fb51193c8e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /home/robust/anaconda3/lib/python3.9/site-packages (2.8.0)\n",
      "Requirement already satisfied: transformers in /home/robust/anaconda3/lib/python3.9/site-packages (4.25.1)\n",
      "Requirement already satisfied: rouge-score in /home/robust/anaconda3/lib/python3.9/site-packages (0.1.2)\n",
      "Requirement already satisfied: nltk in /home/robust/anaconda3/lib/python3.9/site-packages (3.7)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /home/robust/anaconda3/lib/python3.9/site-packages (from datasets) (2022.7.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.2.0 in /home/robust/anaconda3/lib/python3.9/site-packages (from datasets) (0.11.1)\n",
      "Requirement already satisfied: aiohttp in /home/robust/anaconda3/lib/python3.9/site-packages (from datasets) (3.8.3)\n",
      "Requirement already satisfied: responses<0.19 in /home/robust/anaconda3/lib/python3.9/site-packages (from datasets) (0.18.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/robust/anaconda3/lib/python3.9/site-packages (from datasets) (6.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in /home/robust/anaconda3/lib/python3.9/site-packages (from datasets) (2.28.1)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in /home/robust/anaconda3/lib/python3.9/site-packages (from datasets) (10.0.1)\n",
      "Requirement already satisfied: pandas in /home/robust/anaconda3/lib/python3.9/site-packages (from datasets) (1.4.4)\n",
      "Requirement already satisfied: dill<0.3.7 in /home/robust/anaconda3/lib/python3.9/site-packages (from datasets) (0.3.6)\n",
      "Requirement already satisfied: xxhash in /home/robust/anaconda3/lib/python3.9/site-packages (from datasets) (3.2.0)\n",
      "Requirement already satisfied: packaging in /home/robust/anaconda3/lib/python3.9/site-packages (from datasets) (21.3)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /home/robust/anaconda3/lib/python3.9/site-packages (from datasets) (4.64.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/robust/anaconda3/lib/python3.9/site-packages (from datasets) (1.21.5)\n",
      "Requirement already satisfied: multiprocess in /home/robust/anaconda3/lib/python3.9/site-packages (from datasets) (0.70.14)\n",
      "Requirement already satisfied: filelock in /home/robust/anaconda3/lib/python3.9/site-packages (from transformers) (3.6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/robust/anaconda3/lib/python3.9/site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /home/robust/anaconda3/lib/python3.9/site-packages (from transformers) (0.13.2)\n",
      "Requirement already satisfied: six>=1.14.0 in /home/robust/anaconda3/lib/python3.9/site-packages (from rouge-score) (1.16.0)\n",
      "Requirement already satisfied: absl-py in /home/robust/anaconda3/lib/python3.9/site-packages (from rouge-score) (1.4.0)\n",
      "Requirement already satisfied: joblib in /home/robust/anaconda3/lib/python3.9/site-packages (from nltk) (1.1.0)\n",
      "Requirement already satisfied: click in /home/robust/anaconda3/lib/python3.9/site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/robust/anaconda3/lib/python3.9/site-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (4.3.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/robust/anaconda3/lib/python3.9/site-packages (from packaging->datasets) (3.0.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/robust/anaconda3/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (2022.9.14)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/robust/anaconda3/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (1.26.11)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/robust/anaconda3/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (3.3)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/robust/anaconda3/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (2.0.4)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/robust/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets) (21.4.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/robust/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/robust/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets) (1.3.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/robust/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets) (1.8.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/robust/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/robust/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/robust/anaconda3/lib/python3.9/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/robust/anaconda3/lib/python3.9/site-packages (from pandas->datasets) (2022.1)\n"
     ]
    }
   ],
   "source": [
    "! pip install datasets transformers rouge-score nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're opening this notebook locally, make sure your environment has an install from the last version of those libraries.\n",
    "\n",
    "To be able to share your model with the community and generate results like the one shown in the picture below via the inference API, there are a few more steps to follow.\n",
    "\n",
    "First you have to store your authentication token from the Hugging Face website (sign up [here](https://huggingface.co/join) if you haven't already!) then execute the following cell and input your username and password:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/robust/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid.\n",
      "Your token has been saved to /home/robust/.huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "login('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') # 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"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then you need to install Git-LFS. Uncomment the following instructions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !apt install git-lfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure your version of Transformers is at least 4.11.0 since the functionality was introduced in that version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.25.1\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HFASsisvIrIb"
   },
   "source": [
    "You can find a script version of this notebook to fine-tune your model in a distributed fashion using multiple GPUs or TPUs [here](https://github.com/huggingface/transformers/tree/master/examples/seq2seq)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rEJBSTyZIrIb"
   },
   "source": [
    "# Fine-tuning a model on a summarization task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kTCFado4IrIc"
   },
   "source": [
    "In this notebook, we will see how to fine-tune one of the [🤗 Transformers](https://github.com/huggingface/transformers) model for a summarization task. We will use the [XSum dataset](https://arxiv.org/pdf/1808.08745.pdf) (for extreme summarization) which contains BBC articles accompanied with single-sentence summaries.\n",
    "\n",
    "![Widget inference on a summarization task](images/summarization.png)\n",
    "\n",
    "We will see how to easily load the dataset for this task using 🤗 Datasets and how to fine-tune a model on it using the `Trainer` API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"./et5\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4RRkXuteIrIh"
   },
   "source": [
    "This notebook is built to run  with any model checkpoint from the [Model Hub](https://huggingface.co/models) as long as that model has a sequence-to-sequence version in the Transformers library. Here we picked the [`t5-small`](https://huggingface.co/t5-small) checkpoint. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "whPRbBNbIrIl"
   },
   "source": [
    "## Loading the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W7QYTpxXIrIl"
   },
   "source": [
    "We will use the [🤗 Datasets](https://github.com/huggingface/datasets) library to download the data and get the metric we need to use for evaluation (to compare our model to the benchmark). This can be easily done with the functions `load_dataset` and `load_metric`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "IreSlFmlIrIm"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-data_dir=data\n",
      "Found cached dataset aihub_paper_summarization (/home/robust/.cache/huggingface/datasets/KETI-AIR___aihub_paper_summarization/default-data_dir=data/0.0.0/b70439030637fcd143bbefc3f5c8cc9fd6e577b74d342b300f4fc50df4709e1e)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da441872375a4af5861afe0268e1c188",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9604/3433594697.py:5: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric(\"rouge\")\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, load_metric\n",
    "\n",
    "raw_datasets = load_dataset('KETI-AIR/aihub_paper_summarization', data_dir='data')\n",
    "#raw_datasets['train'] = raw_datasets['train'].shard(num_shards=3, index=0)\n",
    "metric = load_metric(\"rouge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RzfPtOMoIrIu"
   },
   "source": [
    "The `dataset` object itself is [`DatasetDict`](https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasetdict), which contains one key for the training, validation and test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "GWiVUF0jIrIv",
    "outputId": "35e3ea43-f397-4a54-c90c-f2cf8d36873e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['doc_id', 'summary_type', 'original_text', 'summary_text'],\n",
       "        num_rows: 565160\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['doc_id', 'summary_type', 'original_text', 'summary_text'],\n",
       "        num_rows: 72122\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u3EtYfeHIrIz"
   },
   "source": [
    "To access an actual element, you need to select a split first, then give an index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "X6HrpprwIrIz",
    "outputId": "d7670bc0-42e4-4c09-8a6a-5c018ded7d95"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'doc_id': 'A201008176694',\n",
       " 'summary_type': 'entire',\n",
       " 'original_text': '본 연구는 경제협력개발기구(Organization for Economic Cooperation and Development, OECD)가 실시한 교수-학습 국제조사(Teaching and Learning International Survey 2013, TALIS) 중 교직 선택에 대한 교사 후회에 관한조사를 연구의 출발점으로 삼았다. 한국의 경우 중학교 교사가 참여한 이 조사에서 응답 교사의 20.1%가 교사가 된 것을 후회한다는 결과가 나오자 사회적 관심이 집중되었다. 교직 선택을 후회하는 교사 비율은 조사대상국 중 최고로 높은 수치로 이는 곧 교사의 낮은 사기와 만족도를 표현하는 것으로 해석되었고 교원 정책에 대한 비판의 근거로 제시되었다. 이 연구는 교사가 경험하는 후회의 감정은 매우 복잡한 현상이고 다양한 변인이 작용하는 심리적메커니즘을 가지고 있다는 선행연구에 기초하여 이러한 단순한 해석이 갖는위험을 지적하고 탐색적인 차원에서 대안적인 해석의 가능성을 제시하였다. 기회(대안)와 후회, 정상(norm)과 후회, 후회의 기능이라는 3가지 영역을검토하면서, 교사 후회 현상은 다양한 변인의 영향을 받을 수 있음을 지적하고 관련 정책적 시사점도 아울러 모색하였다. 특히, 교사 후회는 교직 선택동기에 관한 연구, 교직을 선택한 시기에 관한 조사, 교직 선택이 당연한 집단이 가진 특성, 후회를 감소시키기 위한 교사의 교직 행동 등과 관련지어새롭게 해석될 수 있음을 보여주었다. 앞으로 후회 감정에 관련된 심리적 비교에서 고려되는 요소, 행위/비행위 사이 차이, 후회에 관련된 심리적 처리의휴리스틱과 편향 등 후속 연구가 적절히 이루어질 때 교사 후회 현상에 대한제대로 된 해석이 가능하고, 교사 후회 현상을 바탕으로 한 정책 개발 및 평가도 타당하게 이루어질 수 있을 것이다.',\n",
       " 'summary_text': '본 논문은 경제협력개발기구가 실시한 교수-학습 국제조사 가운데 교직 선택에 대한 교사 후회에 대한 조사를 연구의 출발점으로 하였다. 한국의 경우 중학교 교사가 참여한 이 조사에서 응답 교사의 20.1%가 교사가 된 것을 후회한다는 결과가 나와 사회적 관심을 받았다.'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WHUmphG3IrI3"
   },
   "source": [
    "To get a sense of what the data looks like, the following function will show some examples picked randomly in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "i3j8APAoIrI3"
   },
   "outputs": [],
   "source": [
    "import datasets\n",
    "import random\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "def show_random_elements(dataset, num_examples=5):\n",
    "    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n",
    "    picks = []\n",
    "    for _ in range(num_examples):\n",
    "        pick = random.randint(0, len(dataset)-1)\n",
    "        while pick in picks:\n",
    "            pick = random.randint(0, len(dataset)-1)\n",
    "        picks.append(pick)\n",
    "    \n",
    "    df = pd.DataFrame(dataset[picks])\n",
    "    for column, typ in dataset.features.items():\n",
    "        if isinstance(typ, datasets.ClassLabel):\n",
    "            df[column] = df[column].transform(lambda i: typ.names[i])\n",
    "    display(HTML(df.to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "SZy5tRB_IrI7",
    "outputId": "ba8f2124-e485-488f-8c0c-254f34f24f13"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>summary_type</th>\n",
       "      <th>original_text</th>\n",
       "      <th>summary_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A201007205078</td>\n",
       "      <td>section</td>\n",
       "      <td>플래시 메모리는 집적도 향상 및 양산 기술의 발전으로 소형화, 대용량화, 전송 속도의 고속화가 지속되고 있다. 따라서 플래시 메모리 기반의 이동식 저장장치(USB, OTG)는 일상생활에서 쉽게 접할 수 있는 일반적인 휴대기기로 자리매김하였다.그러나 이러한 이동식 저장장치의 사용이 일반화됨에 따라 기업 또는 연구소 등에서 기밀 정보의 유출에 대한 우려가 높아지게 되었다.이러한 우려에 의해 현재 기업 및 연구소 등에서는 기밀 데이터의 유출을 막기 위해 다양한 통제 정책을 적용하고 있다. 일예로써, 중앙관리서버 기반의 보안 USB 메모리가 있다. 시중에 상용화 되어 있는 보안 USB 메모리는 하나의 솔루션 형태로 제공되는데, 솔루션은 중앙관리서버를 통해 운용이 되고, 관리자가 외부 사용 보안정책, 로그인 어플리케이션, 보안 백신을 USB 메모리에 내장하여 외부사용 인가하는 과정을 포함한다. 또한 보안 USB 메모리에 암호화 칩이 내장되어 파일에 대한 암호화/복호화가 실시된다.보안 USB 메모리에 제공되는 기능은 인쇄, 복사, 캡쳐, USB 잠금, 원격 data 파괴, 중앙관리서버로 사용 로그(시간, 유저 이름, IP, 이벤트 등) 전송 등이 있다.보안 USB 메모리는 사용자 인증을 위해 호스트 또는 서버에서 인증 프로토콜이 필수적이며, 네트워크를 통해서 로그의 전송이 이루어지기 때문에 네트워크가 없는 환경에서는 파일을 조회할 수 없고, 네트워크 패킷 캡쳐 및 변조 IP우회를 이용한 악의적인 해킹 공격에는 취약점을 보이는 문제가 있었다.이러한 보안 USB 메모리를 이용하는 노력에도 불구하고, 시스템의 취약점을 이용하여 이동식 저장장치로 기밀 정보 탈취에 성공하는 사례가 여전히 발생하고 있다.문제는 기밀 정보 탈취에 이용된 이동식 저장장치를 증거물로 확보하더라도 기밀 정보의 유출 후 포맷을 실행했거나, 해당 전자파일을 삭제한 후 다른 전자파일을 반복적으로 저장하고 삭제하면 기밀 정보가 해당 이동식 저장장치에 저장되었었는지 알 수 없었다.또한, 이동식 저장장치에 저장된 데이터 접근 로그를 분석하더라도 포함된 정보는 호스트에서 요청한 블록 번호, 읽기/쓰기 여부, 시간 정보만 포함되어 있기 때문에 사실상 기밀 정보가 저장되었었는지 여부도 파악할 수 없었다.</td>\n",
       "      <td>본 발명은 이동식저장장치의 데이터 접근에 대한 로그를 저장하는 기술에 관한 것으로, 상세하게는 호스트로부터 파일 이벤트가 발생하면 이벤트의 종류와 대상이 되는 전자파일의 파일명, 호스트의 고유식별정보를 획득하여 로그로 저장하는 이동식저장장치의 데이터 접근 이벤트 로깅 방법 및 시스템에 관한 것이다.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A201008153083</td>\n",
       "      <td>entire</td>\n",
       "      <td>\\n[ 발명의 명칭 ]\\n개선된 구조의 압력센서를 갖는 글로우 플러그GLOW-PLUG WITH PRESSURE SENSOR OF IMPROVED STRUCTURE\\n[ 기술분야 ]\\n본 발명은 디젤 기관에서 연료의 착화를 용이하게 하도록 연소실을 예열하는 글로우 플러그에 관한 것으로서 특히 압력센서를 포함하는 글로우 플러그에 관한 것이다.\\n[ 배경기술 ]\\n디젤 엔진은 스파크를 일으켜 강제로 불꽃을 점화시키는 방식 대신에, 혼합 연료를 압축시켜 자발적으로 점화 온도에 도달하여 연소되는 압축 착화 방식을 취하고 있다. 그러나, 공기를 압축하면 누설과 열 손실이 발생하기 때문에 압력 및 온도가 낮아져 시동이 어렵다. 이에 따라, 글로우 플러그가 압축 공기를 자체 점화 온도에 도달할 수 있도록 예열시키기 위하여 사용된다.글로우 플러그는 엔진 실린더 내의 압력 변화에 따라 글러우 플러그의 축방향으로 상하 운동을 하는 발열체의 움직임에 의해 실린더 내의 압력을 측정할 수 있다. 이를 위해, 글로우 플러그에는 별도의 압력센서가 배치된다.발열체는 연소압에 의해 가압되어 글로우 플러그의 축방향으로 이동된다. 이 때, 압력센서는 이런 이동에 의한 변위차를 감지하여 압력을 측정한다. 이를 위해, 종래 발열체와 압력센서 사이에는 변위차를 전달할 수 있는 이너튜브가 사용되었다.종래, 글로우 플러그를 생산할 때 이너튜브는 압력센서와 발열체의 양 측에 각각 고정 결합된다. 이 때, 결합은 용접 공정에 의한다. 다만, 용접 공정은 수작업에 의해 이루어지는 이유로 각 구성 사이에 동심이 서로 맞지 않는 등의 작업 불량이 초래될 수 있다. 즉, 종래 방식은 2회의 용접 공정을 요구하여 공정의 복잡화, 불량률의 증가, 생산 비용의 증가 등의 문제점을 갖고 있었다.\\n[ 선행기술문헌 ]\\n\\n[ 특허문헌 ]\\n대한민국 등록특허 제10-1311381호 (2013.09.23. 등록)대한민국 등록특허 제10-1115006호 (2012.02.03. 등록)대한민국 등록특허 제10-1150851호 (2012.05.22. 등록)대한민국 등록특허 제10-1237886호 (2013.02.21. 등록)\\n[ 발명의 개요 ]\\n\\n[ 해결하려는 과제 ]\\n본 발명의 실시예는 상기와 같은 문제점을 해결하기 위해 안출된 것으로서, 압력 변화에 따라 상하 운동을 하는 발열체와 연소실의 압력을 측정하기 위한 압력센서모듈 사이를 연결하던 이너튜브의 사용을 대체할 수 있는 구조를 포함하는 글로우 플러그를 제공하고자 한다.종래 이너튜브의 사용으로 인한 용접 공정의 수를 감소하여 동심 불량에 따른 불량률을 감소시킬 수 있는 구조를 포함하는 글로우 플러그를 제공하고자 한다.\\n[ 과제의 해결 수단 ]\\n본 발명의 실시예는 상기와 같은 과제를 해결하고자, 중공이 형성된 금구; 상기 금구 내에 삽입되며, 발열체와 인가된 전압을 상기 발열체로 전달하는 이너폴을 포함하는 핀어셈블리; 및 상기 금구의 단부에 배치되는 센서헤드와 상기 금구 내에 삽입되며 상기 센서헤드에서 연장되는 압력전달튜브와 상기 센서헤드에 결합되어 변위량을 감지하는 센서부를 포함하는 압력센서모듈;을 포함하는 개선된 구조의 압력센서를 갖는 글로우 플러그를 제공한다.상기 발열체는 메탈발열체이고, 상기 발열체는 발열튜브; 및 일단이 상기 이너폴과 결합되고 타측이 상기 발열튜브 내측으로 삽입되는 발열조절코일;을 포함할 수 있다.상기 센서헤드와 상기 압력전달튜브는 일체로 형성되며, 상기 압력전달튜브의 일단에는 상기 발열튜브가 삽입되어 끼움 결합될 수 있다.상기 발열튜브의 상단 내주면에는 오링이 설치될 수 있다.상기 압력전달튜브를 통해 삽입되는 이너폴과 외부 전원이 인가되는 단자는 상기 압력센서모듈 내부에서 코일스프링에 의해 연결될 수 있다.상기 압력전달튜브는 금속 재질로 형성되어, 상기 발열체의 변위량 만큼 상기 압력센서모듈을 이동시킬 수 있다.상기 압력전달튜브와 상기 발열튜브의 결합 부위는 레이저 용접될 수 있다.\\n[ 발명의 효과 ]\\n이상에서 살펴본 바와 같은 본 발명의 과제해결 수단에 의하면 다음과 같은 사항을 포함하는 다양한 효과를 기대할 수 있다. 다만, 본 발명이 하기와 같은 효과를 모두 발휘해야 성립되는 것은 아니다.일 실시예에 따른 글로우 플러그는 종래 발열체와 압력센서모듈 사이를 연결하던 이너튜브 대신에 압력전달튜브를 압력센서모듈의 본체에 일체로 형성하여 부품 수의 감소, 용접 공정의 감소 등의 효과를 갖는다.또한, 생산자는 압력전달튜브와 발열튜브 사이의 결합 부위만을 용접하면 충분하여 종래 2회의 용접 공정에 따른 동심 불량의 발생 빈도를 개선할 수 있다.\\n[ 도면의 간단한 설명 ]\\n도 1은 본 발명의 일 실시예에 따른 글로우 플러그의 사시도.도 2는 도 1의 분해 사시도.도 3은 도 1의 Ⅲ-Ⅲ＇방향 단면도.도 4는 도 2의 압력센서모듈에 대한 사시도.도 5는 도 3의 상측 부분 확대도.도 6은 도 3의 하측 부분 확대도.\\n[ 발명을 실시하기 위한 구체적인 내용 ]\\n이하, 도면을 참조하여 본 발명의 구체적인 실시예를 상세히 설명한다.도 1은 본 발명의 일 실시예에 따른 글로우 플러그의 사시도이고, 도 2는 도 1의 분해 사시도이며, 도 3은 도 1의 Ⅲ-Ⅲ＇방향 단면도이고, 도 4는 도 2의 압력센서모듈에 대한 사시도이며, 도 5는 도 3의 상측 부분 확대도이고, 도 6은 도 3의 하측 부분 확대도이다.도 1 내지 도 6을 참조하면 압력센서를 갖는 글로우 플러그는 금구(10), 핀어셈블리(100), 압력센서모듈(200) 등을 포함한다. 금구(10)는 글로우 플러그의 바디에 해당되며, 엔진과 결합되는 부분이다. 금구(10)는 금속 재질이며, 축방향으로 중공이 형성되는 원통형 펜대 형상을 갖는다. 금구(10)는 금구상부(10a)와 금구하부(10b)로 구분될 수 있다. 금구하부(10b)는 엔진과 결합될 때 기밀성을 제공한다.핀어셈블리(100)는 외부에서 인가되는 전원에 의해 발열을 한다. 핀어셈블리(100)는 발열체(110)와 인가된 전압을 발열체(110)로 전달하는 이너폴(130)을 포함한다. 구체적으로, 발열체는 발열튜브(120)와, 일단이 이너폴(130)과 결합되고, 타측이 발열튜브(120) 내측으로 삽입되는 발열조절코일(140)을 포함한다. 또한, 핀어셈블리(100)는 발열튜브(120)와 금구하부(10b) 사이에 개재되어 기밀을 유지시키는 벨로우즈(20) 등을 더 포함한다.발열체(110)는 연소실 내부에 열에너지를 제공한다. 일 실시예에 따른 발열체(110)는 메탈발열체이고, 그 내부에는 발열조절코일(140)이 배치된다. 발열조절코일(140)은 아르곤(Ar) 용접 등에 의해 발열튜브(120)와 연결된다. 한편, 발열튜브(120) 내부에는 분말 형태를 갖는 산화마그네슘(MgO)이 충진된다.한편, 발열튜브(120)의 상단 내주면에는 오링(40)이 설치될 수 있다. 오링(140)은 산화마그네슘(MgO)이 발열튜브(120)로부터 빠져나오는 것을 방지하며, 동시에 수분이나 이물질 등이 발열튜브(120) 내부로 유입되는 것을 방지한다.발열튜브(120)의 상단에는 이너폴(130)이 삽입 결합된다. 이 때, 이너폴(130)은 오링(40)과 끼움 결합된다. 구체적으로, 이너폴(130)의 일단에는 노출되는 발열조절코일(140)이 코킹(caulking)으로 결합되고, 타단에는 코일스프링(240)에 의해 외부 전원이 인가되는 단자(30)가 연결된다.코일스프링(240)은 외부에서 인가된 전압을 단자(30)에서 이너폴(130)로 전달하고, 발열조절코일(140)은 인가된 전압에 의해 발열을 한다. 한편, 코일스프링(240)은 연소실의 압력 변화에 따라 발생하는 핀어셈블리(100) 내의 움직임을 완충한다.벨로우즈(20)는 얇은 금속 재질의 주름 잡힌 원통 형상으로, 압력이 가해지면 내부와 외부의 압력 차이에 의해 축방향으로 신축한다.압력센서모듈(200)은 글로우 플러그가 엔진에 설치되면 실린더 내의 연소실 압력을 측정할 수 있도록 한다. 압력센서모듈(200)은 연소실 내에 노출되는 발열체(110)가 압력 변화에 따라 글로우 플러그의 축방향으로 이동되는 경우 그 변위량을 감지하여 연소실 압력을 측정한다.압력센서모듈(200)은 그 내부에 배치되는 탄성체의 변형을 검출하여 압력을 측정한다. 예를 들어, 압력센서모듈(200)은 가압되어 압축되면 압축되는 양에 비례하여 변형게이지의 전기 저항값이 변화하는 것을 검출하고 이를 통해 압력을 측정하는 변형게이지식 로드셀일 수 있다. 구체적으로, 압력센서모듈(200)은 센서헤드(210), 압력전달튜브(220), 센서부(230) 등을 포함한다. 센서헤드(210)는 금구(10)의 단부에 배치되며 금구(10)의 축방향으로 왕복 이동된다. 압력전달튜브(220)는 금속 재질 예를 들어, 스테인리스로 형성되며 원형파이프 형태를 갖는다. 압력전달튜브(220)는 글로우 플러그의 조립 시에 금구(10) 내에 삽입된다. 일 실시예에 따른 센서헤드(210)는 센서헤드(210)에서 연장되는 압력전달튜브(220)와 일체로 형성된다. 센서부(230)는 센서헤드(210)에 결합되어 발열체(110)의 변위량을 감지한다. 이를 위해, 센서부(230) 내에는 탄성편 등이 배치되어 있다. 또한, 압력전달튜브(220)를 통해 삽입되는 이너폴(130)과 외부 전원이 인가되는 단자(30)는 압력센서모듈(200) 내부에서 코일스프링(240)에 의해 연결된다.압력센서모듈(200)을 제작할 때, 센서헤드(210)와 압력전달튜브(220)를 일체로 형성하면 종래 센서헤드(210)에 이너튜브를 결합시키기 위한 용접 공정을 생략할 수 있다. 이는 용접 공정 중에 발생할 수 있는 동심 불량을 방지할 수 있다.동시에, 생산 업체는 부품 수의 감소로 부품 관리를 용이하게 할 수 있다. 또한, 센서헤드(210)와 압력전달튜브(220)를 일체로 형성하면 압력전달튜브(220)의 연장 방향을 압력센서모듈(200)의 축이동 방향과 일치하도록 형성하여 압력 측정의 정확성을 향상시킬 수 있다.한편, 압력전달튜브(220)의 일단에는 발열튜브(120)가 삽입되어 끼움 결합된다. 그리고, 압력전달튜브(220)와 발열튜브(120)의 결합 부위는 레이저 용접된다. 그 결과, 압력센서모듈(200)은 연소압에 따라 발열체(110)가 이동하면 그 변위량에 해당되는 거리만큼 금구(10)의 축방향으로 이동될 수 있다. 이는, 변형률이 낮은 금속 재질의 압력전달튜브(220)가 센서헤드(210)와 발열튜브(120) 사이에 배치되어 연소압을 그대로 전달하는 이유로 가능하다.이상에서는 본 발명의 바람직한 실시예를 예시적으로 설명하였으나, 본 발명의 범위는 이와 같은 특정 실시예에만 한정되는 것은 아니며, 특허청구범위에 기재된 범주 내에서 적절하게 변경 가능한 것이다.\\n[ 부호의 설명 ]\\n\\t10: 금구\\t\\t\\t\\t\\t100: 핀어셈블리200: 압력센서모듈\\t\\t\\t\\t110: 발열체120: 발열튜브\\t\\t\\t\\t130: 이너폴140: 발열조절코일\\t\\t\\t\\t210: 센서헤드220: 압력전달튜브\\t\\t\\t\\t230: 센서부240: 코일스프링</td>\n",
       "      <td>중공이 형성된 금구; 상기 금구 내에 삽입되며, 발열체와 인가된 전압을 상기 발열체로 전달하는 이너폴을 포함하는 핀어셈블리; 및 상기 금구의 단부에 배치되는 센서헤드와 상기 금구 내에 삽입되며 상기 센서헤드에서 연장되는 압력전달튜브와 상기 센서헤드에 결합되어 변위량을 감지하는 센서부를 포함하는 압력센서모듈;을 포함하는 개선된 구조의 압력센서를 갖는 글로우 플러그를 제공한다.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A220223620093</td>\n",
       "      <td>section</td>\n",
       "      <td>그는 또한 Teilhard de Chardin에 대한 연구를 통하여 인간 내외적 전일성의 추구를 통한 인간의 존재차원의 확장 가능성을 탐구하였다. 한명희(2000)도 한국교육사회학회 연차학술대회 발표에서 “변화하는 교육패러다임과 인간상의 문제”라는 제목으로 21세기 교육 패러다임은 홀리스틱 패러다임으로의 전환이 이루어질 것을 전망하였다. 홀리스틱 교육의 holistic이라는 단어는 생태(학)적이라는 의미의 ecological과 동의어로 쓰이는데, ecological이라는 단어의 시각에서 연구되는 논문이 기존에 발표되었다. 노상우는 생태주의에서 본 현대교육학의 세 가지 과제에서 노상우는 21세기 교육적 패러다임 전환의 사상적동인을 통해 과학주의에 대한 생태주의적 시각의 대안을 통해 21세기 교육에 대한 시각을 분석하고, 객관주의적 지식에 치우쳐서 인간중심 혹은 이성 중심 사고와 윤리에 바탕을 둔 현실 인식에서 보살핌과 상호 공동체에 대한 인식을 하는 성찰적 공동체를 지향할 것을 제안하였다(1995:103~120). 홀리스틱 교육에 대한 연구로는 송민영·김현재(1991)의 『홀리스틱 교육의 이해』, 박영만 외 공저(2002)인 『홀리스틱 교육의 원리와 방법』, 송민영(2006)의 『홀리스틱 교육사상』 등의 연구가 이어졌다. 21세기 교육 현실에 대한 대안적 패러다임으로 인식되고 있는 홀리스틱 교육에 대한 우리나라와 서구 학자들의 연구에서 홀리스틱 교육의 개념이라는 측면에서 논의를 이끌었던 학자들을 중심으로 주장하는 개념을 소개하고, 홀리스틱 교육의 어떠함에 대한 비교 검토하는 일은 홀리스틱 교육이 갈 길에 대한 비전을 제시하는 데 있어서 기초를 제공한다는 점에서 의미 있는 접근일 것이다.</td>\n",
       "      <td>그는 또한 Teilhard de Chardin에 대한 연구를 통하여 인간 내외적 전일성의 추구를 통한 인간의 존재차원의 확장 가능성을 탐구하였다. 한명희도 한국교육사회학회 연차학술대회 발표에서 “변화하는 교육패러다임과 인간상의 문제”라는 제목으로 21세기 교육 패러다임은 홀리스틱 패러다임으로의 전환이 이루어질 것을 전망하였다.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A201008222385</td>\n",
       "      <td>section</td>\n",
       "      <td>국내의 모든 상장기업들은 2011년도부터 회계투명성을 통한 재무제표의 국제적인 신뢰도 제고 등을 이유로 한국채택국제회계기준(이하 “K-IFRS”)을 도입하였다. 현재 세계적인 수용추세를 보이는 국제회계기준(IFRS : International Financial Reporting Standards)은 120여개 국가들이 도입 또는 도입예정에 있다. 국제회계기준은 기존의 한국회계기준(이하 “K-GAAP”)이나 미국회계기준처럼 개별사안에 대한 구체적인 회계처리방법과 절차를 세밀하게 규정하는 규정중심(rule-based)의 기준체계가 아닌 원칙중심(principle-based)의 기준체계이다. 원칙주의는 경제적 실질에 기초하여 회계처리를 할 수 있도록 단을 중요시 여기는 기준으로 볼 수 있다. 국제회계기준의 채택에 대해 다수의 선행연구들은 긍정적인 효과를 제시하고 있는데 Barth et al.(2008)과 Ding et al.(2006) 등은 국회계처리의 기본원칙과 방법론을 제시하는 기준으로 구체적인 실무지침보다는 전문가의 판제회계기준에 따라 보고된 재무제표의 회계이익이 개별국가의 회계기준에 따라 보고된 것보다 질적 수준이 높다고 보고하였다. Bellas et al(2007), Lin and Paananen(2007), Morais and Curto(2007) 등은 국제회계기준을 도입함으로써 회계정보와 기업가치간의 관련성이 높아졌고 또한 이익조정이 감소한다고 보고하였으며, Christensen et al.(2007), Covrig et al.(2007)과 Francis et al.(2004) 등은 국제회계기준이 기업의 자본비용을 줄여 기업가치를 높인다고 보고하였다. 결국은 재무보고의 비교가능성의 증가는 해외 투자자로 하여금 정보취득 비용의 감소를 가져와 해외기업 투자의 증가를 가져온다는 것이다(Kang and Stulz, 1997).</td>\n",
       "      <td>국내의 모든 상장기업들은 2011년도부터 회계투명성을 통한 재무제표의 국제적인 신뢰도 제고 등을 이유로 한국채택국제회계기준을 도입하였다. 현재 세계적인 수용추세를 보이는 국제회계기준은 120여개 국가들이 도입 또는 도입예정에 있다. 국제회계기준은 기존의 한국회계기준이나 미국회계기준처럼 개별사안에 대한 구체적인 회계처리방법과 절차를 세밀하게 규정하는 규정중심의 기준체계가 아닌 원칙중심의 기준체계이다.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A201008119257</td>\n",
       "      <td>entire</td>\n",
       "      <td>\\n[ 발명의 명칭 ]\\n방송 신호 송신 장치, 방송 신호 수신 방법, 방송 신호 송신 방법 및 방송 신호 수신 방법 APPARATUS FOR TRANSMITTING BROADCAST SIGNALS, APPARATUS FOR RECEIVING BROADCAST SIGNALS, METHOD FOR TRANSMITTING BROADCAST SIGNALS AND METHOD FOR RECEIVING BROADCAST SIGNALS\\n[ 기술분야 ]\\n본 발명은 방송 신호를 송신하는 방송 신호 송신 장치, 방송 신호를 수신하는 방송 신호 수신 장치 및 방송 신호를 송신하고 수신하는 방법에 관한 것이다.\\n[ 배경기술 ]\\n아날로그 방송 신호에 대한 송출의 중단 시점이 다가오면서, 디지털 방송 신호를 송수신하기 위한 다양한 기술들이 개발되고 있다. 디지털 방송 신호는 아날로그 방송 신호에 비해 대용량의 비디오/오디오 데이터를 포함할 수 있으며, 비디오/오디오 데이터 외에도 다양한 부가 데이터를 포함할 수 있다. 즉, 디지털 방송을 위한 디지털 방송 시스템은 HD(High Definition)급의 영상과 다채널의 음향 및 다양한 부가 서비스를 제공할 수 있다. 다만, 고용량의 데이터 전송을 위한 데이터 전송 효율, 송수신 네트워크의 강인성(robustness) 및 모바일 수신 장비를 고려한 네트워크의 유연성(flexibility)은 여전히 개선해야 하는 과제이다.본 발명의 일 실시예는 방송 신호의 송수신 시스템에서 타임 인터리빙 또는 타임 인터리버에 관한 것이다. 관련 선행기술 문헌으로 미국 특허출원공개공보 US 2011/0286535호가 있다.\\n[ 발명의 개요 ]\\n\\n[ 해결하려는 과제 ]\\n따라서 본 발명의 목적은 차세대 방송 서비스(future broadcast service)를 위한 방송 신호를 전송하고 수신할 수 있는 방송 신호 송신 장치, 방송 신호 수신 장치, 그리고 차세대 방송 서비스를 위한 방송 신호를 송신하고 수신하는 방법을 제공하는데 있다.본 발명의 목적은 서로 다른 두 개 이상의 방송 서비스를 제공하는 방송 송수신 시스템의 데이터를 시간 영역에서 멀티플렉싱하여 동일한 RF 신호 대역폭(signal bandwidth)을 통하여 전송할 수 있는 방송 신호 송신 장치 및 방송 신호 송신 방법 및 이에 대응하는 방송 신호 수신 장치 및 방송 신호 수신 방법을 제공하는 데에 있다.본 발명의 또 다른 목적은 서비스에 해당하는 데이터를 컴포넌트 별로 분류하여 각각의 컴포넌트에 해당하는 데이터를 별개의 데이터 파이프(data pipe, 이하 DP로 호칭)로 전송하고, 수신하여 처리할 수 있도록 하는 방송 신호 송신 장치, 방송 신호 수신 장치, 그리고 방송 신호를 송신하고 수신하는 방법을 제공하는데 있다.본 발명의 또 다른 목적은 방송 신호를 서비스하는데 필요한 시그널링 정보를 시그널링하는 방송 신호 송신 장치, 방송 신호 수신 장치, 그리고 방송 신호를 송신하고 수신하는 방법을 제공하는데 있다.\\n[ 과제의 해결 수단 ]\\n상술한 목적을 달성하기 위한 본 발명의 일 실시예에 따른 방송 신호 송신 방법은 적어도 하나 이상의 서비스 컴포넌트를 전송하는 복수의 DP(Data Pipe)들 각각에 대응하는 DP 데이터를 인코딩하는 단계, 상기 인코딩된 DP 데이터를 컨스텔레이션들에 매핑하는 단계, 상기 매핑된 DP 데이터의 FEC 블록들을 TI 메모리의 첫번째 열부터 마지막 열까지 쓰는 컬럼-와이즈 라이팅 오퍼레이션 (column-wise writing operation)을 수행하고, 다이아고날-와이즈 리딩 패턴(diagonal-wise reading pattern)에 따라 상기 TI 메모리의 첫번째 행부터 마지막 행까지 상기 쓰여진 FEC 블록들의 셀들을 읽는 다이아고날-와이즈 리딩 오퍼레이션 (diagonal-wise reading operation)을 수행하여 상기 매핑된 DP 데이터를 DP 레벨에서 타임 인터리빙하는 단계, 상기 타임 인터리빙된 DP 데이터를 포함하는 적어도 하나 이상의 신호 프레임을 생성하는 단계, 상기 생성된 적어도 하나 이상의 신호 프레임 내의 데이터를 OFDM (Orthogonal Frequency Division Multiplex) 방식으로 변조하는 단계 및 상기 변조된 데이터를 포함하는 방송 신호들을 송신하는 단계를 포함할 수 있다.\\n[ 발명의 효과 ]\\n본 발명은 다양한 방송 서비스를 제공하기 위하여 서비스의 특성에 따라 데이터를 처리하므로서, 서비스나 서비스 콤포넌트 별로 QoS를 조절할 수 있다.본 발명은 다양한 방송 서비스를 동일한 RF 신호 대역폭을 통해 전송하므로서 전송상의 유연성(flexibility)을 확보할 수 있다.본 발명은 MIMO 시스템을 사용함으로써 데이터 전송 효율을 높이고 방송 신호 송수신의 강건성(Robustness)을 증가시킬 수 있다.따라서 본 발명에 따르면 모바일 수신 장비 또는 인도어 환경에서도 디지털 방송 신호를 오류없이 수신할 수 있는 방송 신호의 송수신 방법 및 장치를 제공할 수 있다.\\n[ 도면의 간단한 설명 ]\\n도 1은 본 발명의 일 실시예에 따른 차세대 방송 서비스를 위한 송신 장치의 구조를 나타낸 도면이다.도 2는 본 발명의 일 실시예에 따른 인풋 포맷팅 모듈을 나타낸 도면이다. 도 3은 본 발명의 다른 실시예에 따른 인풋 포맷팅 모듈을 나타낸 도면이다.도 4는 본 발명의 또 다른 실시예에 따른 인풋 포맷팅 모듈을 나타낸 도면이다.도 5는 본 발명의 일 실시예에 따른 코딩 앤 모듈레이션 모듈을 나타낸 도면이다.도 6은 본 발명의 일 실시예에 따른 프레임 스트럭쳐 모듈을 나타낸 도면이다.도 7은 본 발명의 일 실시예에 따른 웨이브폼 제너레이션 모듈을 나타낸 도면이다.도 8은 본 발명의 일 실시예에 따른 차세대 방송 서비스를 위한 수신 장치의 구조를 나타낸 도면이다.도 9는 본 발명의 일 실시예에 따른 싱크로나이제이션 앤 디모듈레이션 모듈을 나타낸 도면이다.도 10은 본 발명의 일 실시예에 따른 프레임 파싱 모듈을 나타낸 도면이다.도 11은 본 발명의 일 실시예에 따른 디매핑 앤 디코딩 모듈을 나타낸 도면이다.도 12는 본 발명의 일 실시예에 따른 아웃풋 프로세서를 나타낸 도면이다.도 13은 본 발명의 다른 실시예에 따른 아웃풋 프로세서를 나타낸 도면이다.도 14는 본 발명의 다른 실시예에 따른 코딩 앤 모듈레이션 모듈을 나타낸 도면이다. 도 15는 본 발명의 다른 실시예에 따른 디매핑 앤 디코딩 모듈을 나타낸 도면이다.도 16은 본 발명의 일 실시예에 따른 타임 인터리빙 과정을 나타낸 도면이다.도 17은 본 발명에 따른 다이아고날 슬로프들의 실시예를 나타낸 도면이다.도 18은 본 발명의 다른 실시예에 따른 타임 인터리빙 과정을 나타낸 도면이다.도 19는 본 발명의 일 실시예에 따른 TI 아웃풋 메모리 인덱스를 생성하는 과정을 나타낸 도면이다.도 20은 본 발명의 일 실시예에 따른 타임 디인터리빙 과정을 나타낸 도면이다.도 21은 본 발명의 다른 실시예에 따른 타임 디인터리빙 과정을 나타낸 도면이다.도 22는 본 발명의 일 실시예에 따른 TDI 아웃풋 메모리 인덱스를 생성하는 과정을 나타낸 도면이다.도 23은 본 발명의 일 실시예에 따른 VDR (variable data-rate) 시스템을 나타낸 개념도이다.도 24는 본 발명의 또 다른 실시예에 따른 타임 인터리빙 과정을 나타낸 도면이다.도 25는 본 발명의 다른 실시예에 따른 TI 아웃풋 메모리 인덱스를 생성하는 과정을 나타낸 도면이다.도 26은 본 발명의 일 실시예에 따른 TI 메모리 인덱스 생성 과정을 나타낸 순서도 이다.도 27은 본 발명의 또 다른 실시예에 따른 타임 디인터리빙 과정을 나타낸 도면이다.도 28은 본 발명의 또 다른 실시예에 따른 타임 디인터리빙 과정을 나타낸 도면이다.도 29는 본 발명의 일 실시예에 따른 라이팅 오퍼레이션을 나타낸다. 도 30은 본 발명의 일 실시예에 따른 TDI 메모리 인덱스 생성 과정을 나타낸 순서도이다.도 31은 본 발명의 일 실시예에 따른 IF-by-IF TI 패턴 변화(Pattern Variation)을 나타낸다. 도 32는 본 발명의 일 실시예에 따른 랜덤 제너레이터의 구조를 나타낸 도면이다. 도 33은 본 발명의 다른 실시예에 따른 랜덤 제너레이터를 나타낸다.도 34는 본 발명의 또 다른 실시예에 따른 랜덤 제너레이터를 나타낸다.도 35는 본 발명의 일 실시예에 따른 프리퀀시 인터리빙 과정을 나타낸 도면이다.도 36은 본 발명의 일 실시예에 따른 프리퀀시 디인터리빙 과정을 나타낸 개념도이다.도 37은 본 발명의 일 실시예에 따른 프리퀀시 디인터리빙 과정을 나타낸 도면이다.도 38은 본 발명의 일 실시예에 따른 디인터리빙 메모리 인덱스 생성 과정을 나타낸 도면이다.도 39는 본 발명의 다른 실시예에 따른 프리퀀시 인터리빙 과정을 나타낸다.도 40은 본 발명의 일 실시예에 따른 방송 신호 송신 방법의 플로우 차트이다.도 41은 본 발명의 일 실시예에 따른 방송 신호 수신 방법의 플로우 차트이다.\\n[ 발명을 실시하기 위한 구체적인 내용 ]\\n이하 상기의 목적을 구체적으로 실현할 수 있는 본 발명의 바람직한 실시예를 첨부한 도면을 참조하여 설명한다. 이때 도면에 도시되고 또 이것에 의해서 설명되는 본 발명의 구성과 작용은 적어도 하나의 실시예로서 설명되는 것이며, 이것에 의해서 본 발명의 기술적 사상과 그 핵심 구성 및 작용이 제한되지는 않는다.본 발명에서 사용되는 용어는 본 발명에서의 기능을 고려하면서 가능한 현재 널리 사용되는 일반적인 용어를 선택하였으나, 이는 당분야에 종사하는 기술자의 의도 또는 관례 또는 새로운 기술의 출현 등에 따라 달라질 수 있다. 또한 특정한 경우는 출원인이 임의로 선정한 용어도 있으며, 이 경우 해당되는 발명의 설명 부분에서 상세히 그 의미를 기재할 것이다. 따라서 본 발명에서 사용되는 용어는 단순한 용어의 명칭이 아닌 그 용어가 가지는 의미와 본 발명의 전반에 걸친 내용을 토대로 정의되어야 함을 밝혀두고자 한다.본 발명은 차세대 방송 서비스를 위한 방송 신호를 송수신 할 수 있는 장치 및 방법을 제공하기 위한 것이다. 본 발명의 일 실시예에 따른 차세대 방송 서비스는 지상파 방송 서비스, 모바일 방송 서비스 및 UHDTV 서비스등을 포함하는 개념이다. 본 발명은 상술한 차세대 방송 서비스를 위한 방송 신호를 방송 서비스의 특성에 따라 비MIMO(non-MIMO, Multi Input Multi Output) 방식 또는 MIMO 방식으로 처리하는 것을 일 실시예로 할 수 있다. 본 발명의 일 실시예에 따른 비MIMO 방식은 MISO (Multi Input Single Output), SISO (Single Input Single Output) 방식 등을 포함할 수 있다.이하에서, MISO 또는 MIMO의 다중 안테나는 설명의 편의를 위해 2개의 안테나를 예로서 설명할 수 있으나, 이러한 본 발명의 설명은 2개 이상의 안테나를 사용하는 시스템에 적용될 수 있다.도 1은 본 발명의 일 실시예에 따른 차세대 방송 서비스를 위한 송신 장치의 구조를 나타낸 도면이다.본 발명의 일 실시예에 따른 차세대 방송 서비스를 위한 송신 장치는 인풋 포맷팅(Input formatting) 모듈(1000), 코딩 앤 모듈레이션 (coding 0026# modulation) 모듈(1100), 프레임 스트럭쳐 (frame structure) 모듈(1200), 웨이브폼 제너레이션(waveform generation) 모듈(1300) 및 시그널링 제너레이션 (signaling generation) 모듈(1400)을 포함할 수 있다. 이하 각 모듈의 동작을 중심으로 설명한다.도 1 에 도시된 바와 같이, 본 발명의 일 실시예에 따른 차세대 방송 서비스를 위한 송신 장치는 입력 신호로서 MPEG-TS 스트림, IP 스트림 (v4/v6) 그리고 GS (Generic stream)를 입력받을 수 있다. 또한 입력 신호를 구성하는 각 스트림의 구성에 관한 부가 정보(management information)를 입력받고, 입력받은 부가 정보를 참조하여 최종적인 피지컬 레이어 신호(physical layer signal)를 생성할 수 있다.본 발명의 일 실시예에 따른 인풋 포맷팅 모듈(1000)은 입력된 스트림들을 코딩 (coding) 및 모듈레이션(modulation)을 수행하기 위한 기준 또는 서비스 및 서비스 컴포넌트 기준에 따라 나누어 복수의 로지컬 (logical) DP들 (또는 DP들 또는 DP 데이터)를 생성할 수 있다. DP는 피지컬 레이어 단의 로지컬 채널로서, 서비스 데이터 또는 관련 메타 데이터를 운반할 수 있으며, 적어도 하나 이상의 서비스 또는 적어도 하나 이상의 서비스 콤포넌트를 운반할 수 있다. 또한 DP를 통해 전송되는 데이터를 DP 데이터 라 호칭할 수 있다.또한 본 발명의 일 실시예에 따른 인풋 포맷팅 모듈(1000)은 생성된 각각의 DP를 코딩 및 모듈레이션 을 수행하기 위해 필요한 블록 단위로 나누고, 전송효율을 높이거나 스케쥴링을 하기 위해 필요한 일련의 과정들을 수행할 수 있다. 구체적인 내용은 후술한다.본 발명의 일 실시예에 따른 코딩 앤 모듈레이션 모듈(1100)은 인풋 포맷팅 모듈(1000)으로부터 입력받은 각각의 DP에 대해서 FEC(forward error correction) 인코딩 을 수행하여 전송채널에서 발생할 수 있는 에러를 수신단에서 수정할 수 있도록 한다. 또한 본 발명의 일 실시예에 따른 코딩 앤 모듈레이션 모듈(1100)은 FEC 출력의 비트 데이터를 심볼 데이터로 전환하고, 인터리빙을 수행하여 채널에 의한 버스트 에러(burst error)를 수정 할 수 있다. 또한 도 1에 도시된 바와 같이 두 개 이상의 전송 안테나(Tx antenna)를 통해 전송하기 위하여 본 발명의 일 실시예에 따른 코딩 앤 모듈레이션 모듈(1100)은 처리한 데이터를 각 안테나로 출력하기 위한 데이터 통로 (또는 안테나 통로) 나누어 출력할 수 있다.본 발명의 일 실시예에 따른 프레임 스트럭쳐 모듈(1200)은 코딩 앤 모듈레이션 모듈(1100)에서 출력된 데이터를 신호 프레임(또는 프레임)에 매핑할 수 있다. 본 발명의 일 실시예에 따른 프레임 스트럭쳐 모듈(1200)은 인풋 포맷팅 모듈(1000)에서 출력된 스케쥴링 정보를 이용하여 매핑을 수행할 수 있으며, 추가적인 다이버시티 게인(diversity gain)을 얻기 위하여 신호 프레임 내의 데이터에 대하여 인터리빙 을 수행할 수 있다.본 발명의 일 실시예에 따른 웨이브폼 제너레이션 모듈(1300)은 프레임 스트럭쳐 모듈(1200)에서 출력된 신호 프레임들을 최종적으로 전송할 수 있는 형태의 신호로 변환시킬 수 있다. 이 경우, 본 발명의 일 실시예에 따른 웨이브폼 제너레이션 모듈(1300)은 수신기에서 전송 시스템의 신호 프레임을 획득할 수 있도록 하기 위하여 프리앰블 시그널(또는 프리앰블)을 삽입하고, 전송채널을 추정하여 왜곡을 보상할 수 있도록 레퍼런스 신호(reference signal)를 삽입할 수 있다. 또한 본 발명의 일 실시예에 따른 웨이브폼 제너레이션 모듈(1300)은 다중 경로 수신에 따른 채널 딜레이 스프레드(channel delay spread)에 의한 영향을 상쇄시키기 위해서 가드 인터벌(guard interval)을 두고 해당 구간에 특정 시퀀스를 삽입할 수 있다. 또한 본 발명의 일 실시예에 따른 웨이브폼 제너레이션 모듈(1300)은 부가적으로 출력 신호의 PAPR(Peak-to-Average Power Ratio)와 같은 신호특성을 고려하여 효율적인 전송에 필요한 과정을 수행할 수 있다.본 발명의 일 실시예에 따른 시그널링 제너레이션 모듈(1400)은 입력된 부가정보및 인풋 포맷팅 모듈(1000), 코딩 앤 모듈레이션 모듈(1100) 및 프레임 스트럭쳐 모듈(1200)에서 발생된 정보를 이용하여 최종적인 시그널링 정보(physical layer signaling 정보, 이하 PLS 정보라 호칭)을 생성한다. 따라서 본 발명의 일 실시예에 따른 수신 장치는 시그널링 정보를 복호화하여 수신된 신호를 디코딩할 수 있다.상술한 바와 같이 본 발명의 일 실시예에 따른 차세대 방송 서비스를 위한 송신 장치는 지상파 방송 서비스, 모바일 방송 서비스 및 UHDTV 서비스등을 제공할 수 있다. 따라서 본 발명의 일 실시예에 따른 차세대 방송 서비스를 위한 송신 장치는 서로 다른 서비스를 위한 신호들을 시간 영역에서 멀티플렉싱하여 전송할 수 있다.도 2 내지 도 4는 도 1에서 설명한 본 발명의 일 실시예에 따른 인풋 포맷팅 모듈(1000)의 실시예를 나타낸 도면이다. 이하 각 도면에 대해 설명한다.도 2는 본 발명의 일 실시예에 따른 인풋 포맷팅 모듈을 나타낸 도면이다. 도 2는 인풋 신호가 싱글 인풋 스트림인 경우의 인풋 포맷팅 모듈을 나타낸다.도 2에 도시된 바와 같이 본 발명의 일 실시예에 따른 인풋 포맷팅 모듈은 모드 어댑테이션 모듈(2000)과 스트림 어댑테이션 모듈(2100)을 포함할 수 있다.*도 2에 도시된 바와 같이 모드 어댑테이션 모듈(2000)은 인풋 인터페이스(input interface) 블록(2010), CRC-8 인코더(CRC-8 encoder) 블록(2020) 및 BB 헤더 인설션(header insertion) 블록(2030)을 포함할 수 있다. 이하 각 블록에 대해 간략히 설명한다. 인풋 인터페이스 블록(2010)은 입력된 싱글 인풋 스트림을 추후 FEC(BCH/LDPC)를 수행하기 위한 BB(baseband) 프레임 길이 단위로 나눠서 출력할 수 있다.CRC-8 인코더 블록(2020)은 각 BB 프레임의 데이터에 대해서 CRC 인코딩을 수행하여 리던던시(redundancy) 데이터를 추가할 수 있다.이후, BB 헤더 인설션 블록(2030)은 모드 어댑테이션 타입(Mode Adaptation Type (TS/GS/IP)), 유저 패킷 길이(User Packet Length), 데이터 필드 길이(Data Field Length), 유저 패킷 싱크 바이트(User Packet Sync Byte), 데이터 필드 내의 유저 패킷 싱크 바이트의 스타트 어드레스(Start Address), 하이 이피션시 모드 인디케이터(High Efficiency Mode Indicator), 인풋 스트림 싱크로나이제이션 필드(Input Stream Synchronization Field) 등 정보를 포함하는 헤더를 BB 프레임에 삽입할 수 있다.도 2에 도시된 바와 같이, 스트림 어댑테이션 모듈(2100)은 패딩 인설션(Padding insertion) 블록(2110) 및 BB 스크램블러(BB scrambler) 블록(2120)을 포함할 수 있다. 이하 각 블록에 대해 간략히 설명한다.패딩 인설션 블록(2110)은 모드 어댑테이션 모듈(2000)로부터 입력받은 데이터가 FEC 인코딩에 필요한 입력 데이터 길이보다 작은 경우, 패딩 비트를 삽입하여 필요한 입력 데이터 길이를 가지도록 출력할 수 있다.BB 스크램블러 블록(2120)은 입력된 비트 스트림에 대해 PRBS(Pseudo Random Binary Sequence)을 이용하여 XOR을 수행하여 랜더마이즈 할 수 있다.상술한 블록들은 설계자의 의도에 따라 생략되거나, 비슷하거나 동일한 기능을 가진 다른 블록에 의해서 대체될 수 있다.도 2에 도시된 바와 같이, 인풋 포맷팅 모듈은 최종적으로 DP를 코딩 앤 모듈레이션 모듈로 출력할 수 있다.도 3은 본 발명의 다른 실시예에 따른 인풋 포맷팅 모듈을 나타낸 도면이다. 도 3은 인풋 신호가 멀티플 인풋 스트림들인 경우의 인풋 포맷팅 모듈의 모드 어댑테이션 모듈을 나타낸 도면이다.멀티플 인풋 스트림들을 처리하기 위한 인풋 포맷팅 모듈의 모드 어댑테이션 모듈은 각 인풋 스트림을 독립적으로 처리할 수 있다. 도 3에 도시된 바와 같이, 멀티플 인풋 스트림들을 각각 처리 하기 위한 모드 어댑테이션 모듈(3000)은 인풋 인터페이스(input interface) 블록, 인풋 스트림 싱크로나이저(input stream synchronizer) 블록, 컴펀세이팅 딜레이(compensating delay) 블록, 널 패킷 딜리션(null packet deletion) 블록, CRC-8 인코더(CRC-8 encoder) 블록 및 BB 해더 인설션(BB header insertion) 블록을 포함할 수 있다. 이하 각 블록에 대해 간략히 설명한다. 인풋 인터페이스 블록, CRC-8 인코더 블록 및 BB 해더 인설션 블록의 동작들은 도 2에서 설명한 바와 같으므로 생략한다.인풋 스트림 싱크로나이저 블록(3100)은 ISCR(Input Stream Clock Reference) 정보를 전송하여, 수신단에서 TS 혹은 GS 스트림을 복원하는데 필요한 타이밍 정보를 삽입할 수 있다.컴펀세이팅 딜레이 블록(3200)은 인풋 스트림 싱크로나이저 블록에 의해 발생된 타이밍 정보와 함께 송신 장치의 데이터 프로세싱에 따른 DP들간 딜레이가 발생한 경우, 수신 장치에서 동기를 맞출 수 있도록 입력 데이터를 지연시켜서 출력할 수 있다.널 패킷 딜리션 블록(3300)은 불필요하게 전송될 입력 널 패킷을 제거하고, 제거된 위치에 따라 제거된 널 패킷의 개수를 삽입하여 전송할 수 있다.상술한 블록들은 설계자의 의도에 따라 생략되거나, 비슷하거나 동일한 기능을 가진 다른 블록에 의해서 대체될 수 있다.도 4는 본 발명의 또 다른 실시예에 따른 인풋 포맷팅 모듈을 나타낸 도면이다. 구체적으로 도 4는 인풋 신호가 멀티플 인풋 스트림인 경우의 인풋 포맷팅 모듈의 스트림 어댑테이션 모듈을 나타낸 도면이다.본 발명의 일 실시예에 따른 멀티플 인풋 스트림인 경우의 인풋 포맷팅 모듈의 스트림 어댑테이션 모듈은 스케쥴러(scheduler)(4000), 1-프레임 딜레이(1-frame delay) 블록(4100), 인밴드 시그널링 또는 패딩 인설션(In-band signaling or padding insertion) 블록(4200), PLS 생성(PLS, physical layer signaling, generation) 블록(4300) 및 BB 스크램블러(BB scrambler) 블록(4400)을 포함할 수 있다. 이하 각 블록의 동작에 대해 설명한다.스케쥴러 (4000)는 듀얼 극성(dual polarity)을 포함한 다중 안테나를 사용하는 MIMO 시스템을 위한 스케쥴링 을 수행할 수 있다. 또한 스케쥴러 (4000)는 도 1에서 설명한 코딩 앤 모듈레이션 모듈 내의 비트 투 셀 디먹스(bit to cell demux) 블록, 셀 인터리버(cell interleaver) 블록, 타임 인터리버(time interleaver) 블록등 각 안테나 경로를 위한 신호 처리 블록들에 사용될 파라미터들을 발생시킬 수 있다.1-프레임 딜레이 블록(4100)은 DP 내에 삽입될 인밴드 시그널링등을 위해서 다음 프레임 에 대한 스케쥴링 정보가 현재 프레임에 전송될 수 있도록 입력 데이터를 하나의 신호 프레임만큼 지연시킬 수 있다.인밴드 시그널링 또는 패딩 인설션 블록(4200)은 한 개의 신호 프레임만큼 지연된 데이터에 지연되지 않은 PLS-다이나믹 시그널링(dynamic signaling) 정보를 삽입할 수 있다. 이 경우, 인밴드 시그널링 또는 패딩 인설션 블록(4200)은 패딩을 위한 공간이 있는 경우에 패딩 비트를 삽입하거나, 인밴드 시그널링 정보를 패딩 공간에 삽입할 수 있다. 또한, 스케쥴러(4000)는 인밴드 시그널링과 별개로 현재 프레임에 대한 PLS-다이나믹 시그널링 정보를 출력할 수 있다. 따라서 후술할 셀 맵퍼는 스케쥴러 (4000)에서 출력한 스케쥴링 정보에 따라 입력 셀들을 매핑 할 수 있다.PLS 생성 블록(4300)은 인밴드 시그널링을 제외하고 신호 프레임의 프리앰블 심볼(preamble symbol)이나 스프레딩 되어 데이터 심볼 등에 전송될 PLS 데이터 (또는 PLS)를 생성할 수 있다. 이 경우, 본 발명의 일 실시예에 따른 PLS 데이터 는 시그널링 정보로 호칭할 수 있다. 또한 본 발명의 일 실시예에 따른 PLS 데이터는 PLS-프리 정보와 PLS-포스트 정보로 분리될 수 있다. PLS-프리 정보는 방송 신호 수신 장치가 PLS-포스트 정보를 디코딩하는데 필요한 파라미터들과 스태틱(static) PLS 시그널링 정보를 포함할 수 있으며, PLS-포스트 정보는 방송 신호 수신 장치가 DP 를 디코딩하는데 필요한 파라미터를 포함할 수 있다. 상술한 DP를 디코딩하는데 필요한 파라미터는 다시 스태틱 PLS 시그널링 정보 및 다이나믹 PLS 시그널링 정보로 분리될 수 있다. 스태틱 PLS 시그널링 정보 는 수퍼 프레임에 포함된 모든 프레임에 공통적으로 적용될 수 있는 파라미터로 수퍼 프레임 단위로 변경될 수 있다. 다이나믹 PLS 시그널링 정보는 수퍼 프레임에 포함된 프레임마다 다르게 적용될 수 있는 파라미터로, 프레임 단위로 변경될 수 있다. 따라서 수신 장치는 PLS-프리 정보를 디코딩하여 PLS-포스트 정보를 획득하고, PLS-포스트 정보를 디코딩하여 원하는 DP를 디코딩할 수 있다.BB 스크램블러 블록(4400)은 최종적으로 웨이브폼 제너레이션 블록 의 출력 신호의 PAPR 값이 낮아지도록 PRBS를 발생시켜서 입력 비트열과 XOR시켜서 출력할 수 있다. 도 4에 도시된 바와 같이 BB 스크램블러 블록(4400)의 스크램블링은 DP와 PLS 모두에 대해 적용될 수 있다.상술한 블록들은 설계자의 의도에 따라 생략되거나, 비슷하거나 동일한 기능을 가진 다른 블록에 의해서 대체될 수 있다.도 4에 도시된 바와 같이, 스트림 어댑테이션 모듈은 최종적으로 각 data pipe를 코딩 앤 모듈레이션 모듈로 출력할 수 있다.도 5는 본 발명의 일 실시예에 따른 코딩 앤 모듈레이션 모듈을 나타낸 도면이다. 도 5의 코딩 앤 모듈레이션 모듈은 도 1에서 설명한 코딩 앤 모듈레이션 모듈(1100)의 일 실시예에 해당한다.상술한 바와 같이 본 발명의 일 실시예에 따른 차세대 방송 서비스를 위한 송신 장치는 지상파 방송 서비스, 모바일 방송 서비스 및 UHDTV 서비스등을 제공할 수 있다.즉, 본 발명의 일 실시예에 따른 차세대 방송 서비스를 위한 송신 장치가 제공하고자 하는 서비스의 특성에 따라 QoS (quality of service)가 다르기 때문에 각 서비스에 대응하는 데이터가 처리되는 방식이 달라져야 한다. 따라서 본 발명의 일 실시예에 따른 코딩 앤 모듈레이션 모듈은 입력된 DP들에 대하여 각각의 경로별로 SISO, MISO와 MIMO 방식을 독립적으로 적용하여 처리할 수 있다. 결과적으로 본 발명의 일 실시예에 따른 차세대 방송 서비스를 위한 송신 장치는 각 DP를 통해 전송하는 서비스나 서비스 콤포넌트 별로 QoS를 조절할 수 있다.따라서 본 발명의 일 실시예에 따른 코딩 앤 모듈레이션 모듈은 SISO 방식을 위한 제 1 블록(5000), MISO 방식을 위한 제 2 블록(5100), MIMO 방식을 위한 제 3 블록(5200) 및 PLS-프리/포스트 정보를 처리하기 위한 제 4 블록(5300)을 포함할 수 있다. 도 5에 도시된 코딩 앤 모듈레이션 모듈은 일 실시예에 불과하며 설계자의 의도에 따라 코딩 앤 모듈레이션 모듈은 제 1 블록(5000) 및 제 4 블록(5300)만을 포함할 수도 있고, 제 2 블록(5100) 및 제 4 블록(5300)만을 포함할 수도 있고, 제 3 블록(5200) 및 제 4 블록(5300)만을 포함할 수도 있다. 즉 설계자의 의도에 따라 코딩 앤 모듈레이션 모듈은 각 DP를 동일하게 또는 다르게 처리하기 위한 블록들을 포함할 수 있다.이하 각 블록에 대해 설명한다.제 1 블록(5000)은 입력된 DP를 SISO 처리하기 위한 블록으로 FEC 인코더(FEC encoder) 블록(5010), 비트 인터리버(bit interleaver) 블록(5020), 비트 투 셀 디먹스(bit to cell demux) 블록(5030), 컨스텔레이션 맵퍼(constellation mapper) 블록(5040), 셀 인터리버(cell interleaver) 블록(5050) 및 타임 인터리버(time interleaver) 블록(5060)을 포함할 수 있다.FEC 인코더 블록(5010)은 입력된 DP에 대하여 BCH 인코딩과 LDPC 인코딩을 수행하여 리던던시를 추가하고, 전송채널상의 오류를 수신단에서 정정하여 FEC 블록을 출력할 수 있다.비트 인터리버 블록(5020)은 FEC 인코딩이 수행된 데이터의 비트열을 인터리빙 룰(rule)에 의해서 인터리빙하여 전송채널 중에 발생할 수 있는 버스트 에러 에 대해 강인성을 갖도록 처리할 수 있다. 따라서 QAM 심볼에 ?K 페이딩(deep fading) 혹은 이레이져(erasure)가 가해진 경우, 각 QAM 심볼에는 인터리빙된 비트들이 매핑되어 있으므로 전체 코드워드 비트들 중에서 연속된 비트들에 오류가 발생하는 것을 막을 수 있다.비트 투 셀 디먹스 블록(5030)은 입력된 비트열의 순서와 컨스텔레이션 매핑 룰을 모두 고려하여 FEC 블록내 각 비트들이 적절한 강인성(robustness)를 갖고 전송될 수 있도록 입력 비트열의 순서를 결정하여 출력할 수 있다.또한, 비트 인터리버 블 5020은 FEC 인코더 블 5010 과 컨스텔레이션 맵퍼 블록 5040 사이에 위치하며, 수신단의 LDPC 디코을 고려하여, FEC 인코더 블록 5010 에서 수행한 LDPC인코딩의 출력 비트를 컨스텔레이션 맵퍼 블록의 서로 다른 신뢰성(reliability) 및 최적의 값을 갖는 비트 포지션(bit position)과 연결시키는 역할을 수행할 수 있다. 따라서 비트 투 셀 디먹스 블록(5030)은 비슷하거나 동일한 기능을 가진 다른 블록에 의해 대체 될 수 있다.컨스텔레이션 맵퍼 블록(5040)은 입력된 비트 워드를 하나의 컨스텔레이션에 매핑할 수 있다. 이 경우 컨스텔레이션 맵퍼 블록은 추가적으로 로테이션 앤 Q-딜레이(rotation 0026# Q-delay)를 수행할 수 있다. 즉, 컨스텔레이션 맵퍼 블록은 입력된 컨스텔레이션들을 로테이션 각도(rotation angle)에 따라 로테이션 시킨 후에 I(In-phase) 성분과 Q(Quadrature-phase) 성분으로 나눈 후에 Q 성분만을 임의의 값으로 딜레이시킬 수 있다. 이후 페어로 된 I 성분과 Q 성분을 이용해서 새로운 컨스텔레이션으로 재매핑할 수 있다. 또한 컨스텔레이션 맵퍼 블록(5040)은 최적의 컨스텔레이션 포인트들을 찾기 위하여 2차원 평면상의 컨스텔레이션 포인트들을 움직이는 동작을 수행할 수 있다. 이 과정을 통해 코딩 앤 모듈레이션 모듈(1100)의 용량(capacity)은 최적화 될 수 있다. 또한, 컨스텔레이션 맵퍼 블록(5040)은 IQ 밸런스드 컨스텔레이션 포인트들(IQ-balanced constellation points)과 로테이션 방식을 이용하여 상술한 동작을 수행할 수 있다. 또한, 컨스텔레이션 맵퍼 블록(5040)은 비슷하거나 동일한 기능을 가진 다른 블록에 의해 대체될 수 있다.셀 인터리버 블록(5050)은 한 개의 FEC 블록에 해당하는 셀들을 랜덤 하게 섞어서 출력하여, 각 FEC 블록에 해당하는 셀들이 각 FEC 블록마다 서로 다른 순서로 출력할 수 있다.타임 인터리버 블록(5060)은 여러 개의 FEC 블록에 속하는 cell들을 서로 섞어서 출력할 수 있다. 따라서 각 FEC 블록의 셀들은 타임 인터리빙 뎁스(depth)만큼의 구간내에 분산되어 전송되므로 다이버시티 게인을 획득할 수 있다.제 2 블록(5100)은 입력된 DP를 MISO 처리하기 위한 블록으로 도 5에 도시된 바와 같이 제 1 블록(5000)과 동일하게 FEC 인코더 블록, 비트 인터리버 블록, 비트 투 셀 디먹스 블록, 컨스텔레이션 맵퍼 블록, 셀 인터리버 블록 및 타임 인터리버 블록을 포함할 수 있으나, MISO 프로세싱(processing) 블록(5110)을 더 포함한다는 점에서 차이가 있다. 제 2 블록(5100)은 제 1 블록(5000)과 마찬가지로 입력부터 타임 인터리버까지 동일한 역할의 과정을 수행하므로, 동일한 블록들에 대한 설명은 생략한다. MISO 프로세싱 블록(5110)은 입력된 일련의 셀들에 대해서 전송 다이버시티(transmit diversity)를 주는 MISO 인코딩 매트릭스 에 따라 인코딩을 수행하고, MISO 프로세싱된 데이터를 두 개의 경로들을 통해 출력할 수 있다. 본 발명의 일 실시예에 따른 MISO 프로세싱은 OSTBC(orthogonal space time block coding)/OSFBC (orthogonal space frequency block coding, 일명 Alamouti coding)을 포함할 수 있다.제 3 블록(5200)은 입력된 DP를 MIMO 처리하기 위한 블록으로 도 5에 도시된 바와 같이 제 2 블록(5100)과 동일하게 FEC 인코더 블록, 비트 인터리버 블록, 비트 투 셀 디먹스 블록, 컨스텔레이션 맵퍼 블록, 셀 인터리버 블록 및 타임 인터리버 블록을 포함할 수 있으나, MIMO 프로세싱 블록(5220)을 포함한다는 점에서 데이터 처리 과정의 차이가 있다.즉, 제 3 블록(5200)의 경우, FEC 인코더 블록 및 비트 인터리버 블록은 제 1 및 2 블록(5000, 5100)과 구체적인 기능은 다르지만 기본적인 역할은 동일하다.비트 투 셀 디먹스 블록(5210)은 MIMO 프로세싱의 입력 개수와 동일한 개수의 출력 비트열을 생성하여 MIMO 프로세싱을 위한 MIMO path를 통해 출력할 수 있다. 이 경우, 비트 투 셀 디먹스 블록(5210)은 LDPC와 MIMO 프로세싱의 특성을 고려하여 수신단의 디코딩 성능을 최적화하도록 설계될 수 있다.컨스텔레이션 맵퍼 블록, 셀 인터리버 블록, 타임 인터리버 블록 역시 구체적인 기능은 다를 수 있지만 기본적인 역할은 제 1 및 2 블록(5000, 5100)에서 설명한 바와 동일하다. 또한 도 5에 도시된 바와 같이, 컨스텔레이션 맵퍼 블록, 셀 인터리버 블록, 타임 인터리버 블록들은 비트 투 셀 디먹스블록에서 출력된 출력 비트열을 처리하기 위하여, MIMO 프로세싱을 위한 MIMO 경로들의 개수만큼 존재할 수 있다. 이 경우, 컨스텔레이션 맵퍼 블록, 셀 인터리버 블록, 타임 인터리버 블록들은 각 경로를 통해 입력되는 데이터들에 대하여 각각 동일하게 동작하거나 혹은 독립적으로 동작할 수 있다.MIMO 프로세싱 블록(5220)은 입력된 두 개의 입력 셀들에 대해서 MIMO 인코딩 매트릭스를 사용하여 MIMO 프로세싱을 수행하고 MIMO 프로세싱된 데이터를 두 개의 경로들을 통해 출력할 수 있다. 본 발명의 일 실시예에 따른 MIMO 인코딩 매트릭스는 SM 매트릭스(spatial multiplexing), 골든 코드(Golden code), 풀-레이트 풀 다이버시티 코드(Full-rate full diversity code), 리니어 디스펄션 코드(Linear dispersion code) 등을 포함할 수 있다.제 4 블록(5300)은 PLS-프리/포스트 정보를 처리하기 위한 블록으로, SISO 또는 MISO 프로세싱을 수행할 수 있다. 제 4 블록(5300)에 포함된 비트 인터리버 블록, 비트 투 셀 디먹스 블록, 컨스텔레이션 맵퍼 블록, 셀 인터리버 블록, 타임 인터리버 블록 및 MISO 프로세싱 블록 등은 상술한 제 2 블록(5100)에 포함된 블록들과 구체적인 기능은 다를 수 있지만 기본적인 역할은 동일하다.제 4 블록(5300)에 포함된 FEC 인코더(Shortened/punctured FEC encoder) 블록(5310)은 입력 데이터의 길이가 FEC 인코딩을 수행하는데 필요한 길이보다 짧은 경우를 대비한 PLS 경로를 위한 FEC 인코딩 방식을 사용하여 PLS 데이터를 처리할 수 있다. 구체적으로, FEC 인코더 블록(5310)은 입력 비트열에 대해서 BCH 인코딩을 수행하고, 이후 노멀 LDPC 인코딩에 필요한 입력 비트열의 길이만큼 제로 패딩(zero padding)을 수행 하고, LDPC 인코딩을 한 후에 패딩된 제로들을 제거하여 이펙티브 코드 레이트(effective code rate)가 DP와 같거나 DP보다 낮도록 패리티 비트(parity bit)를 펑처링(puncturing)할 수 있다.상술한 제 1 블록(5000) 내지 제 4 블록(5300)에 포함된 블록들은 설계자의 의도에 따라 생략되거나, 비슷하거나 동일한 기능을 가진 다른 블록에 의해서 대체될 수 있다.도 5에 도시된 바와 같이, 코딩 앤 모듈레이션 모듈은 최종적으로 각 경로별로 처리된 DP, PLS-프리 정보, PLS-포스트 정보를 프레임 스트럭쳐 모듈로 출력할 수 있다.도 6은 본 발명의 일 실시예에 따른 프레임 스트럭쳐 모듈을 나타낸 도면이다.도 6에 도시된 프레임 스트럭쳐 모듈은 도 1에서 설명한 프레임 스트럭쳐 모듈(1200)의 일 실시예에 해당한다.본 발명의 일 실시예에 따른 프레임 스트럭쳐 블록은 적어도 하나 이상의 셀 맵퍼(cell-mapper)(6000), 적어도 하나 이상의 딜레이 보상 (delay compensation) 모듈(6100) 및 적어도 하나 이상의 블록 인터리버(block interleaver)(6200)을 포함할 수 있다. 셀 맵퍼 (6000), 딜레이 보상 모듈(6100) 및 블록 인터리버 (6200)의 개수는 설계자의 의도에 따라 변경 가능하다. 이하 각 모듈의 동작을 중심으로 설명한다. 셀 맵퍼(6000)는 코딩 앤 모듈레이션 모듈로부터 출력된 SISO 또는 MISO 또는 MIMO 처리된 DP에 대응하는 셀들, DP간 공통으로 적용될 수 있는 커먼 데이터(common data)에 대응하는 셀들, PLS-프리/포스트 정보에 대응하는 셀들을 스케쥴링 정보에 따라 신호 프레임에 할당(또는 배치) 할 수 있다. 커먼 데이터는 전부 또는 일부의 DP들간에 공통으로 적용될 수 있는 시그널링 정보를 의미하며, 특정 DP를 통해 전송될 수 있다. 커먼 데이터를 전송하는 DP를 common DP라 호칭할 수 있으며 이는 설계자의 의도에 따라 변경가능하다.본 발명의 일 실시예에 따른 송신 장치가 2개의 출력 안테나를 사용하고, 상술한 MISO 프로세싱에서 알라모우티 코딩(Alamouti coding)을 사용하는 경우, 알라모우티 인코딩에 의한 오소고널리티(orthogonality)를 유지하기 위해서 셀 맵퍼(6000)는 페어 와이즈 셀 매핑(pair-wise cell mapping)을 수행할 수 있다. 즉, 셀 맵퍼(6000)는 입력 셀들에 대해서 연속된 두 개의 셀들을 하나의 단위로 처리하여 신호 프레임에 매핑할 수 있다. 따라서 각 안테나의 출력 경로에 해당하는 입력 경로 내의 페어로 된 셀들은 신호 프레임 내 서로 인접한 위치에 할당될 수 있다.딜레이 보상 블록(6100)은 다음 신호 프레임에 대한 입력 PLS 데이터 셀을 한 신호 프레임 만큼 딜레이하여 현재 신호 프레임에 해당하는 PLS 데이터를 획득할 수 있다. 이 경우, 현재 신호 프레임의 PLS 데이터 는 현재 신호 프레임내의 프리앰블 영역을 통해 전송될 수 있으며, 다음 신호 프레임에 대한 PLS 데이터는 현재 신호 프레임내의 프리앰블 영역 또는 현재 신호 프레임의 각 DP내의 인밴드 시그널링을 통해서 전송될 수 있다. 이는 설계자의 의도에 따라 변경 가능하다.블록 인터리버(6200)는 신호 프레임의 단위가 되는 전송 블록내의 셀들을 인터리빙함으로써 추가적인 다이버시티 게인을 획득할 수 있다. 또한 블록 인터리버(6200)는 상술한 페어 와이즈 셀 매핑이 수행된 경우, 입력 셀들에 대해서 연속된 두 개의 셀들을 하나의 단위로 처리하여 인터리빙을 수행할 수 있다. 따라서 블록 인터리버(6200)에서 출력 되는 셀들은 동일한 두 개의 연속된 cell들이 될 수 있다.페어 와이즈 매핑 및 페어 와이즈 인터리빙이 수행되는 경우, 적어도 하나 이상의 셀 맵퍼와 적어도 하나 이상의 블록 인터리버는 각각의 경로를 통해 입력되는 데이터에 대해서 동일하게 동작하거나 혹은 독립적으로 동작할 수 있다.상술한 블록들은 설계자의 의도에 따라 생략되거나, 비슷하거나 동일한 기능을 가진 다른 블록에 의해서 대체될 수 있다.도 6에 도시된 바와 같이, 프래임 스트럭쳐 모듈은 적어도 하나 이상의 신호 프레임을 웨이브폼 제너레이션 모듈로 출력할 수 있다.도 7은 본 발명의 일 실시예에 따른 웨이브폼 제너레이션 모듈을 나타낸 도면이다.도 7에 도시된 웨이브폼 제너레이션 모듈은 도 1에서 설명한 웨이브폼 제너레이션 모듈(1300)의 일 실시예에 해당한다.본 발명의 일 실시예에 따른 웨이브폼 제너레이션 모듈은 도 6에서 설명한 프레임 스트럭쳐 모듈에서 출력된 신호 프레임들을 입력받고 출력하기 위한 안테나의 개수만큼 신호 프레임들을 변조하여 전송할 수 있다.구체적으로 도 7에 도시된 웨이브폼 제너레이션 모듈은 m 개의 Tx 안테나를 사용하는 송신 장치의 웨이브폼 제너레이션 모듈의 실시예로서, m개의 경로만큼 입력된 프레임을 변조하여 출력하기 위한 m개의 처리 블록들을 포함할 수 있다. m개의 처리 블록들은 모두 동일한 처리 과정을 수행할 수 있다. 이하에서는 m개의 처리 블록 중 첫번째 처리 블록(7000)의 동작을 중심으로 설명한다. 첫번째 처리 블록(7000)은 레퍼런스 시그널 인설션 앤 PAPR 리덕션(reference signal insertion 0026# PAPR reduction) 블록(7100), 인버스 웨이브폼 트랜스폼 (Inverse waveform transform) 블록(7200), PAPR 리덕션 (PAPR reduction in time) 블록(7300), 가드 시퀀스 인설션 (Guard sequence insertion) 블록(7400), 프리앰블 인설션 (preamble insertion) 블록(7500), 웨이브폼 프로세싱 (waveform processing) 블록(7600), 타 시스템 인설션 (other system insertion) 블록(7700) 및 DAC (Digital Analog Conveter) 블록(7800)을 포함할 수 있다.레퍼런스 시그널 인설션 앤 PAPR 리덕션 블록(7100)은 각 신호 블록마다 정해진 위치에 레퍼런스 신호들을 삽입하고, 타임 도메인에서의 PAPR 값을 낮추기 위해서 PAPR 리덕션 스킴(reduction scheme)을 적용할 수 있다다. 본 발명의 일 실시예에 따른 방송 송수신 시스템이 OFDM 시스템인 경우, 레퍼런스 시그널 인설션 앤 PAPR 리덕션 블록(7100)은 액티브 서브 케리어들의 일부를 사용하지 않고 보존(reserve)하는 방법을 사용할 수 있다. 또한 레퍼런스 시그널 인설션 앤 PAPR 리덕션 블록(7100)은 방송 송수신 시스템에 따라 PAPR 리덕션 스킴을 추가 특징으로서 사용하지 않을 수도 있다.인버스 웨이브폼 트랜스폼 블록(7200)은 전송채널의 특성과 시스템 구조를 고려하여 전송효율 및 유연성(flexibility)이 향상되는 방식으로 입력 신호를 트팬스폼하여 출력할 수 있다. 본 발명의 일 실시예에 따른 방송 송수신 시스템이 OFDM 시스템의 경우 인버스 웨이브폼 트랜스폼 블록(7200)은 인버스 FFT 오퍼레이션(Inverse FFT operation)을 사용하여 주파수 영역의 신호를 시간 영역으로 변환하는 방식을 사용할 수 있다. 또한 본 발명의 일 실시예에 따른 방송 송수신 시스템이 싱글 캐리어 시스템인 경우, 인버스 웨이브폼 트랜스폼 블록은 웨이브폼 제너레이션 모듈 내에서 사용되지 않을 수도 있다.PAPR 리덕션 블록(7300)은 입력된 신호에 대해서 시간영역에서 PAPR를 낮추기 위한 방법을 적용할 수 있다. 본 발명의 일 실시예에 따른 방송 송수신 시스템이 OFDM 시스템의 경우, PAPR 리덕션 블록(7300)은 간단하게 피크 앰플리튜드(peak amplitude)를 클리핑(clipping)하는 방법을 사용할 수도 있다. 또한 PAPR 리덕션 블록(7300)은 추가 특징으로 본 발명의 일 실시예에 따른 방송 송수신 시스템에 따라 사용되지 않을 수도 있다.가드 시퀀스 인설션 블록(7400)은 전송채널의 딜레이 스프레드(delay spread)에 의한 영향을 최소화하기 위해서 인접한 신호 블록간에 가드 인터벌을 두고, 필요한 경우 특정 시퀀스를 삽입할 수 있다. 따라서 수신 장치는 동기화나 채널추정을 용이하게 수행할 수 있다. 본 발명의 일 실시예에 따른 방송 송수신 시스템이 OFDM 시스템의 경우, 가드 시퀀스 인설션블록(7400)은 OFDM 심볼의 가드 인터벌 구간에 사이클릭 프레픽스(cyclic prefix)를 삽입할 수도 있다.프리앰블 인설션 블록(7500)은 수신 장치가 타겟팅하는 시스템 신호를 빠르고 효율적으로 디텍팅할 수 있도록 송수신 장치간 약속된 노운 타입(known type)의 신호(프리앰블 또는 프리앰블 심볼)을 전송 신호에 삽입할 수 있다. 본 발명의 일 실시예에 따른 방송 송수신 시스템이 OFDM 시스템의 경우, 프리앰블 인설션 블록(7500)은 여러 개의 OFDM 심볼들로 구성된 신호 프레임을 정의하고, 매 신호 프레임의 시작 부분에 프리앰블을 삽입할 수 있다. 따라서, 프리앰블은 기본 PSL 데이터를 운반할 수 있으며, 각 신호 프레임의 시작 부분에 위치할 수 있다.웨이브폼 프로세싱 블록(7600)은 입력 베이스밴드 신호에 대해서 채널의 전송특성에 맞도록 웨이브폼 프로세싱 을 수행할 수 있다. 웨이브폼 프로세싱 블록(7600)은 일 실시예로서 전송신호의 아웃 오브 밴드 에미션(out-of-band emission)의 기준을 얻기 위해 SRRC 필터링(square-root-raised cosine filtering)을 수행하는 방식을 사용할 수도 있다. 또한 본 발명의 일 실시예에 따른 방송 송수신 시스템이 멀티 캐리어 시스템인 경우, 웨이브폼 프로세싱 블록(7600)은 사용되지 않을 수도 있다.타 시스템 인설션 블록(7700)은 동일한 RF 신호 대역폭 내에 서로 다른 두 개 이상의 방송 서비스를 제공하는 방송 송수신 시스템의 데이터를 함께 전송할 수 있도록 복수의 방송 송수신 시스템의 신호들을 시간 영역에서 멀티플렉싱할 수 있다. 이 경우 서로 다른 두 개 이상의 시스템이란 서로 다른 방송 서비스를 전송하는 시스템을 의미한다. 서로 다른 방송 서비스는 지상파 방송 서비스, 모바일 방송 서비스 등을 의미할 수 있다. 또한 각 방송 서비스와 관련된 데이터는 서로 다른 프레임을 통해 전송될 수 있다.DAC 블록(7800)은 입력 디지털 신호를 아날로그 신호로 변환하여 출력할 수 있다. DAC 블록(7800)에서 출력된 신호는 m 개의 출력 안테나를 통해 전송될 수 있다. 본 발명의 일 실시예에 따른 전송 안테나는 수직 (vertical) 또는 수평(horizontal) 극성(polarity)을 가질 수 있다.또한 상술한 블록들은 설계자의 의도에 따라 생략되거나, 비슷하거나 동일한 기능을 가진 다른 블록에 의해서 대체될 수 있다.도 8은 본 발명의 일 실시예에 따른 차세대 방송 서비스를 위한 수신 장치의 구조를 나타낸 도면이다.본 발명의 일 실시예에 따른 차세대 방송 서비스를 위한 수신 장치는 도 1에서 설명한 차세대 방송 서비스를 위한 송신 장치에 대응될 수 있다. 본 발명의 일 실시예에 따른 차세대 방송 서비스를 위한 수신 장치는 싱크로나이제이션 앤 디모듈레이션 (synchronization 0026# demodulation) 모듈(8000), 프레임 파싱 (frame parsing) 모듈(8100), 디매핑 앤 디코딩 (demapping 0026# decoding) 모듈(8200), 아웃풋 프로세서 (output processor) (8300) 및 시그널링 디코딩 (signaling decoding) 모듈(8400)을 포함할 수 있다. 이하 각 모듈의 동작을 중심으로 설명한다.싱크로나이제이션 앤 디모듈레이션 모듈(8000)은 블록은 m개의 수신 안테나를 통해 입력 신호를 수신하고, 수신 장치에 대응하는 시스템에 대한 신호의 디텍팅 과 싱크로나이제이션(synchronization)을 수행하고, 송신단에서 수행한 방식의 역과정에 해당하는 디모듈레이션(demodulation)을 수행할 수 있다.프레임 파싱 모듈(8100)은 입력된 신호 프레임을 파싱 하고 사용자가 선택한 서비스를 전송하는 데이터를 추출 할 수 있다. 프레임 파싱 모듈(8100)은 송신 장치에서 인터리빙을 수행한 경우, 이에 대한 역과정으로서 디인터리빙을 수행할 수 있다. 이 경우, 추출해야 할 신호 및 데이터의 위치는 시그널링 디코딩 모듈(8400)에서 출력된 데이터를 디코딩하여 송신 장치에서 수행한 스케쥴링 정보 등을 복원하여 획득할수 있다.디매핑 앤 디코딩 모듈(8200)은 입력 신호를 비트 도메인의 데이터로 변환한 이후에 필요한 경우에 디인터리빙 과정을 수행할 수 있다. 디매핑 앤 디코딩 모듈(8200)은 전송 효율을 위해 적용된 매핑에 대해 디매핑을 수행하고, 전송채널 중에 발생된 에러에 대해서 디코딩을 통해 에러 정정을 수행할 수 있다.  이 경우, 디매핑 앤 디코딩 모듈(8200)은 시그널링 디코딩모듈(8400)에서 출력된 데이터를 디코딩하여 디매핑과 디코딩에 필요한 전송 파라미터들을 획득할 수 있다.아웃풋 프로세서 (8300)는 송신 장치에서 전송효율을 높이기 위해 적용한 다양한 압축/신호처리 과정의 역과정을 수행할 수 있다. 이 경우, 아웃풋 프로세서 (8300)는 시그널링 디코딩 모듈(8400)에서 출력된 데이터로부터 필요한 제어 정보를 획득할 수 있다. 아웃풋 프로세서 (8300)의 최종 출력은 송신 장치에 입력된 신호에 해당하며, MPEG-TS, IP 스트림 (v4 or v6) 및 GS(generic stream)가 될 수 있다.시그널링 디코딩 모듈(8400)은 디모듈레이팅된 신호로부터 PLS 정보을 획득할 수 있다. 상술한 바와 같이, 프레임 파싱 모듈(8100), 디매핑 앤 디코딩 모듈(8200) 및 아웃풋 프로세서 (8300)는 시그널링 디코딩 모듈(8400)에서 출력된 데이터를 이용하여 해당 모듈의 기능을 수행할 수 있다. 도 9는 본 발명의 일 실시예에 따른 싱크로나이제이션 앤 디모듈레이션 모듈을 나타낸 도면이다.도 9에 도시된 싱크로나이제이션 앤 디모듈레이션 모듈은 도 8에서 설명한 싱크로나이제이션 앤 디모듈레이션 모듈의 일 실시예에 해당한다. 또한 도 9에 도시된 싱크로나이제이션 앤 디모듈레이션 모듈은 도 7에서 설명한 웨이브폼 제너레이션 모듈의 역동작을 수행할 수 있다.도 9에 도시된 바와 같이 본 발명의 일 실시예에 따른 싱크로나이제이션 앤 디모듈레이션 모듈은 m 개의 Rx 안테나를 사용하는 수신 장치의 싱크로나이제이션 앤 디모듈레이션 모듈의 실시예로서, m개의 경로만큼 입력된 신호를 복조하여 출력하기 위한 m개의 처리 블록들을 포함할 수 있다. m개의 처리 블록들은 모두 동일한 처리 과정을 수행할 수 있다. 이하에서는 m개의 처리 블록 중 첫번째 처리 블록(9000)의 동작을 중심으로 설명한다. 첫번째 처리 블록(9000)은 튜너 (tuner) (9100), ADC 블록(9200), 프리앰블 디텍터 (preamble dectector) (9300), 가드 시퀀스 디텍터 (guard sequence detector) (9400), 웨이브폼 트랜스폼 (waveform transmform) 블록(9500), 타임/프리퀀시 싱크 (Time/freq sync) 블록(9600), 레퍼런스 신호 디텍터 (Reference signal detector) (9700), 채널 이퀄라이저 (Channel equalizer) (9800) 및 인버스 웨이브폼 트랜스폼 (Inverse waveform transform) 블록(9900)을 포함할 수 있다.튜너(9100)는 원하는 주파수 대역을 선택하고 수신한 신호의 크기를 보상하여 AD C 블록(9200)으로 출력할 수 있다.ADC 블록(9200)은 튜너(9100)에서 출력된 신호를 디지털 신호로 변환할 수 있다.프리앰블 디텍터 (9300)는 디지털 신호에 대해 수신 장치에 대응하는 시스템 의 신호인지 여부를 확인하기 위하여 프리앰블(또는 프리앰블 신호 또는 프리앰블 심볼)을 디텍팅 할 수 있다. 이 경우, 프리앰블 디텍터 (9300)는 프리엠블을 통해 수신되는 기본적인 전송 파라미터들을 복호할 수 있다.가드 시퀀스 디텍터 (9400)는 디지털 신호 내의 가드 시퀀스를 디텍팅할 수 있다. 타임/프리퀀시 싱크 블록(9600)은 디텍팅된 가드 시쿼스를 이용하여 타임/프리퀀시 싱크로나이제이션(synchronization)을 수행할 수 있으며, 채널 이퀄라이저 (9800)는 디텍팅된 가드 시퀀스를 이용하여 수신/복원된 시퀀스를 통해서 채널을 추정할 수 있다.웨이브폼 트랜스폼 블록(9500)은 송신측에서 인버스 웨이브폼 트랜스폼이 수행되었을 경우 이에 대한 역변환 과정을 수행할 수 있다. 본 발명의 일 실시예에 따른 방송 송수신 시스템이 멀티 캐리어 시스템인 경우, 웨이브폼 트랜스폼 블록(9500)은 FFT 변환과정을 수행할 수 있다. 또한 본 발명의 일 실시예에 따른 방송 송수신 시스템이 싱글 캐리어 시스템인 경우, 수신된 시간영역의 신호가 주파수 영역에서 처리하기 위해서 사용되거나, 시간영역에서 모두 처리되는 경우, 웨이브폼 트랜스폼 블록(9500)은 사용되지 않을 수 있다.타임/프리퀀시 싱크 블록(9600)은 프리앰블 디텍터 (9300), 가드 시퀀스 디텍터 (9400), 레퍼런스 신호 디텍터 (9700)의 출력 데이터를 수신하고, 검출된 신호에 대해서 가드 시퀀스 디텍션 (guard sequence detection), 블록 윈도우 포지셔닝 (block window positioning)을 포함하는 시간 동기화 및 캐리어 주파수 동기화를 수행할 수 있다. 이때, 주파수 동기화를 위해서 타임/프리퀀시 싱크 블록(9600)은 웨이브폼 트랜스폼 블록(9500)의 출력 신호를 피드백하여 사용할 수 있다.레퍼런스 신호 디텍터 (9700)는 수신된 레퍼런스 신호를 검출할 수 있다. 따라서 본 발명의 일 실시예에 따른 수신 장치는 싱크로나이제이션을 수행하거나 채널 추정(channel estimation)을 수행할 수 있다.채널 이퀄라이저 (9800)는 가드 시퀀스나 레퍼런스 신호로부터 각 전송 안테나로부터 각 수신 안테나까지의 전송채널을 추정하고, 추정된 채널을 이용하여 각 수신 데이터에 대한 채널 보상(equalization)을 수행할 수 있다.인버스 웨이브폼 트랜스폼 블록(9900)은 동기 및 채널추정/보상을 효율적으로 수행하기 위해서 웨이브폼 트랜스폼 블록(9500)이 웨이브폼 트랜스폼을 수행한 경우, 다시 원래의 수신 데이터 도메인으로 복원해주는 역할을 수행할 수 있다. 본 발명의 일 실시예에 따른 방송 송수신 시스템이싱글 캐리어 시스템인 경우, 웨이브폼 트랜스폼 블록(9500)은 동기/채널추정/보상을 주파수 영역에서 수행하기 위해서 FFT를 수행할 수 있으며, 인버스 웨이브폼 트랜스폼 블록(9900)은 채널보상이 완료된 신호에 대해 IFFT를 수행함으로서 전송된 데이터 심볼들을 복원할 수 있다. 본 발명의 일 실시예에 따른 방송 송수신 시스템이 멀티 캐리어 시스템인 경우, 인버스 웨이브폼 트랜스폼 블록(9900)은 사용되지 않을 수도 있다.또한 상술한 블록들은 설계자의 의도에 따라 생략되거나, 비슷하거나 동일한 기능을 가진 다른 블록에 의해서 대체될 수 있다.도 10은 본 발명의 일 실시예에 따른 프레임 파싱 모듈을 나타낸 도면이다. 도 10에 도시된 프레임 파싱 모듈은 도 8에서 설명한 프레임 파싱 모듈의 일 실시예에 해당한다. 또한 도 10에 도시된 프레임 파싱 모듈은 도 6에서 설명한 프레임 스트럭쳐 모듈의 역동작을 수행할 수 있다.도 10에 도시된 바와 같이, 본 발명의 일 실시예에 따른 프레임 파싱 모듈은 적어도 하나 이상의 블록 인터리버(10000) 및 적어도 하나 이상의 셀 디맵퍼 (10100)를 포함할 수 있다. block interleaver(10000)는 m 개 수신안테나의 각 data 경로로 입력되어 싱크로나이제이션 앤 디모듈레이션 모듈에서 처리된 데이터에 대하여, 각 신호 블록 단위로 데이터에 대한 디인터리빙을 수행할 수 있다. 이 경우, 도 8에서 설명한 바와 같이, 송신측에서 페어 와이즈 인터리빙이 수행된 경우, 블록 인터리버 (10000)는 각 입력 경로에 대해서 연속된 두 개의 데이터를 하나의 pair로 처리할 수 있다. 따라서 블록 인터리버 (10000)는 디인터리빙을 수행한 경우에도 연속된 두개의 출력 데이터를 출력할 수 있다. 또한 블록 인터리버(10000)는 송신단에서 수행한 인터리빙 과정의 역과정을 수행하여 원래의 데이터 순서대로 출력할 수 있다.셀 디맵퍼 (10100)는 수신된 신호 프레임으로부터 커먼 데이터에 대응하는 셀들과 DP에 대응하는 셀들 및 PLS 정보에 대응하는 셀들을 추출할 수 있다. 필요한 경우, 셀 디맵퍼 (10100)는 여러 개의 부분으로 분산되어 전송된 데이터들을 머징(merging)하여 하나의 스트림으로 출력할 수 있다. 또한 도 6에서 설명한 바와 같이 송신단에서 두 개의 연속된 셀들의 입력 데이터가 하나의 페어로 처리되어 매핑된 경우, 셀 디맵퍼 (10100)는 이에 해당하는 역과정으로 연속된 두개의 입력 셀들을 하나의 단위로 처리하는 페어 와이즈 셀 디매핑을 수행할 수 있다.또한, 셀 디맵퍼 (10100)는 현재 프레임을 통해 수신한 PLS 시그널링 정보에 대해서, 각각 PLS-프리 정보 및 PLS-포스트 정보로서 모두 추출하여 출력할 수 있다.상술한 블록들은 설계자의 의도에 따라 생략되거나, 비슷하거나 동일한 기능을 가진 다른 블록에 의해서 대체될 수 있다.도 11은 본 발명의 일 실시예에 따른 디매핑 앤 디코딩 모듈을 나타낸 도면이다.도 11에 도시된 디매핑 앤 디코딩 모듈은 도 8에서 설명한 디매핑 앤 디코딩 모듈의 일 실시예에 해당한다. 또한 도 11에 도시된 디매핑 앤 디코딩 모듈은 도 5에서 설명한 코딩 앤 모듈레이션 모듈의 역동작을 수행할 수 있다.상술한 바와 같이 본 발명의 일 실시예에 따른 송신 장치의 코딩 앤 모듈레이션 모듈은 입력된 data pipe들에 대하여 각각의 경로별로 SISO, MISO와 MIMO 방식을 독립적으로 적용하여 처리할 수 있다. 따라서 도 11에 도시된 디매핑 앤 디코딩 모듈 역시 송신 장치에 대응하여 프레임 파서에서 출력된 데이터를 각각 SISO, MISO, MIMO 처리하기 위한 블록들을 포함할 수 잇다.도 11에 도시된 바와 같이, 본 발명의 일 실시예에 따른 디매핑 앤 디코딩 모듈은 SISO 방식을 위한 제 1 블록(11000), MISO 방식을 위한 제 2 블록(11100), MIMO 방식을 위한 제 3 블록(11200) 및 PLS pre/post 정보를 처리하기 위한 제 4 블록(11300)을 포함할 수 있다. 도 11에 도시된 디매핑 앤 디코딩 모듈은 일 실시예에 불과하며 설계자의 의도에 따라 디매핑 앤 디코딩 모듈은 제 1 블록(11000)및 제 4 블록(11300)만을 포함할 수도 있고, 제 2 블록(11100) 및 제 4 블록(11300)만을 포함할 수도 있고, 제 3 블록(11200) 및 제 4 블록(11300)만을 포함할 수도 있다. 즉 설계자의 의도에 따라 디매핑 앤 디코딩 모듈은 각 DP를 동일하게 또는 다르게 처리하기 위한 블록들을 포함할 수 있다.이하 각 블록에 대해 설명한다. 제 1 블록(11000)은 입력된 DP를 SISO 처리하기 위한 블록으로 타임 디인터리버 (time de-ineterleaver) 블록(11010), 셀 디인터리버 (cell de-interleaver) 블록(11020), 컨스텔레이션 디맵퍼 (constellation demapper) 블록(11030), 셀 투 비트 먹스 (cell to bit mux) 블록(11040), 비트 디인터리버 (bit de-interleaver) 블록(11050) 및 FEC 디코더 (FEC decoder) 블록(11060)을 포함할 수 있다. 타임 인터리버 블록(11010)은 도 5에서 설명한 타임 인터리버 블록(5060)의 역과정을 수행할 수 있다. 즉, 타임 인터리버 블록(11010)은 시간 영역에서 인터리빙된 입력 심볼을 원래의 위치로 디인터리빙 할 수 있다.셀 디인터리버 블록(11020)은 도 5에서 설명한 셀 디인터리버 블록(5050)의 역과정을 수행할 수 있다. 즉, 셀 디인터리버 블록(11020)은 하나의 FEC 블록내에서 스프레딩된 셀들의 위치를 원래의 위치로 디인터리빙 할 수 있다.컨스텔레이션 디맵퍼 블록(11030)은 도 5에서 설명한 컨스텔레이션 디맵퍼 블록(5040)의 역과정을 수행할 수 있다. 즉, 컨스텔레이션 디맵퍼 블록(11030)은 심볼 도메인의 입력 신호를 비트 도메인의 데이터로 디매핑할 수 있다. 또한, 컨스텔레이션 디맵퍼 블록(11030)은 하드 디시젼(hard decision)을 수행하여 하드 디시젼 결과에 따라 비트 데이터를 출력할 수도 있고, 소프트 디시젼 (soft decision) 값이나 혹은 확률적인 값에 해당하는 각 비트의 LLR (Log-likelihood ratio) 값을 출력할 수 있다. 만약 송신단에서 추가적인 다이버시티 게인 얻기 위해 로테이트된 컨스텔레이션을 적용한 경우, 컨스텔레이션 디맵퍼 블록(11030)은 이에 상응하는 2-D(2-Dimensional) LLR 디매핑을 수행할 수 있다. 이때 컨스텔레이션 디맵퍼 블록(11030)은 LLR을 계산할 때 송신 장치에서 I 또는 Q 성분에 대해서 수행된 딜레이 값을 보상할 수 있도록 계산을 수행할 수 있다.셀 투 비트 먹스 블록(11040)은 도 5에서 설명한 비트 투 셀 디먹스 블록(5030)의 역과정을 수행할 수 있다. 즉, 셀 투 비트 먹스 블록(11040)은 비트 투 셀 디먹스블록(5030)에서 매핑된 비트 데이터들을 원래의 비트 스트림 형태로 복원할 수 있다.비트 디인터리버 블록(11050)은 도 5에서 설명한 비트 인터리버 블록(5020)의 역과정을 수행할 수 있다. 즉, 비트 디인터리버 블록(11050)은 셀 투 비트 먹스 블록(11040)에서 출력된 비트 스트림을 원래의 순서대로 디인터리빙할 수 있다.FEC 디코더 블록(11060)은 도 5에서 설명한 FEC 인코더 블록(5010)의 역과정을 수행할 수 있다. 즉, FEC 디코더 블록(11060)은 LDPC 디코딩과 BCH 디코딩을 수행하여 전송채널상 발생된 에러를 정정할 수 있다.제 2 블록(11100)은 입력된 DP를 MISO 처리하기 위한 블록으로, 도 11에 도시된 바와 같이 제 1 블록(11000)과 동일하게 타임 디인터리버 블록, 셀 디인터리버 블록, 컨스텔레이션 디맵퍼 블록, 셀 투 비트 먹스 블록, 비트 디인터리버 블록 및 FEC 디코더 블록을 포함할 수 있으나, MISO 디코딩 블록(11110)을 더 포함한다는 점에서 차이가 있다. 제 2 블록(11100)은 제 1 블록(11000)과 마찬가지로 타임 디인터리버부터 출력까지 동일한 역할의 과정을 수행하므로, 동일한 블록들에 대한 설명은 생략한다.MISO 디코딩 블록(11110)은 도 5에서 설명한 MISO 프로세싱 블록(5110)의 역과정을 수행할 수 있다. 본 발명의 일 실시예에 따른 방송 송수신 시스템이 STBC를 사용한 시스템인 경우, MISO 디코딩 블록(11110)은 알라모우티 디코딩을 수행할 수 있다.제 3 블록(11200)은 입력된 DP를 MIMO 처리하기 위한 블록으로, 도 11에 도시된 바와 같이 제 2 블록(11100) 과 동일하게 타임 디인터리버 블록, 셀 디인터리버 블록, 컨스텔레이션 디맵퍼 블록, 셀 투 비트 먹스 블록, 비트 디인터리버 블록 및 FEC 디코더 블록을 포함할 수 있으나, MIMO 디코딩 블록(11210)을 포함한다는 점에서 데이터 처리 과정의 차이가 있다. 제 3 블록(11200)에 포함된 타임 디인터리버, 셀 디인터리버, 컨스텔레이션 디맵퍼, 셀 투 비트 먹스, 비트 디인터리버 블록들의 동작은 제 1 내지 제 2 블록(11000-11100)에 포함된 해당 블록들의 동작과 구체적인 기능은 다를 수 있으나 기본적인 역할은 동일하다.MIMO 디코딩 블록(11210)은 m개의 수신 안테나 입력 신호에 대해서 셀 디인터리버의 출력 데이터를 입력으로 받고, 도 5에서 설명한 MIMO 프로세싱 블록(5220)의 역과정으로서 MIMO 디코딩을 수행할 수 있다. MIMO 디코딩 블록(11210)은 최고의 복호화 성능을 얻기 위해서 맥시멈 라이클리후드 (Maximum likelihood) 디코딩을 수행하거나, 복잡도를 감소시킨 스피어 디코딩(Sphere decoding)을 수행할 수 있다. 또는 MIMO 디코딩 블록(11210)은 MMSE 디텍션을 수행하거나 이터러티브 디코딩(iterative decoding)을 함께 결합 수행하여 향상된 디코딩 성능을 확보할 수 있다.제 4 블록(11300)은 PLS-프리/포스트 정보를 처리하기 위한 블록으로, SISO 또는 MISO 디코딩을 수행할 수 있다. 제 4 블록(11300)은 도 5에서 설명한 제 4 블록(5300)의 역과정을 수행할 수 있다. 제 4 블록(11300)에 포함된 타임 디인터리버, 셀 디인터리버, 컨스텔레이션 디맵퍼, 셀 투 비트 먹스, 비트 디인터리버 블록들의 동작은 제 1 내지 제 3 블록(11000-11200)에 포함된 해당 블록들의 동작과 구체적인 기능은 다를 수 있으나 기본적인 역할은 동일하다.제 4 블록(11300)에 포함된 FEC 디코더 (Shortened/Punctured FEC decoder) (11310)은 도 5에서 설명한 FEC 인코더 (Shortened/punctured FEC encoder) 블록(5310)의 역과정을 수행할 수 있다. 즉, FEC 디코더 (11310)는 PLS 데이터의 길이에 따라 쇼트닝/펑쳐링 (shortening/puncturing)되어 수신된 데이터에 대해서 디쇼트닝 (de-shortening) 및 디펑쳐링 (de-puncturing)을 수행한 후에 FEC 디코딩을 수행할 수 있다. 이 경우, DP에 사용된 FEC 디코더를 동일하게 PLS 데이터에도 사용할 수 있으므로, PLS 데이터만을 위한 별도의 FEC 디코딩 하드웨어가 필요하지 않으므로 시스템 설계가 용이하고 효율적인 코딩이 가능하다는 장점이 있다.상술한 블록들은 설계자의 의도에 따라 생략되거나, 비슷하거나 동일한 기능을 가진 다른 블록에 의해서 대체될 수 있다.결과적으로 도 11에 도시된 바와 같이, 본 발명의 일 실시예에 따른 디매핑 앤 디코딩 모듈은 각 경로 별로 처리된 DP 및 PLS 정보를 아웃풋 프로세서로 출력할 수 있다.도 12내지 도 13은 본 발명의 일 실시예에 따른 아웃풋 프로세서를 나타낸 도면이다.도 12는 본 발명의 일 실시예에 따른 아웃풋 프로세서를 나타낸 도면이다.도 12에 도시된 아웃풋 프로세서는 도 8에서 설명한 아웃풋 프로세서의 일 실시예에 해당한다. 또한 도 12에 도시된 아웃풋 프로세서는 디매핑 앤 디코딩 모듈로부터 출력된 DP를 수신하여 싱글 아웃풋 스트림(single output stream)을 출력하기 위한 것으로, 도 2에서 설명한 인풋 포맷팅 모듈의 역동작을 수행할 수 있다.도 12에 도시된 아웃풋 프로세서는 BB 스크램블러 (BB scrambler) 블록(12000), 패딩 리무벌 (Padding removal) 블록(12100), CRC-8 디코더 (CRC-8 decoder) 블록(12200) 및 BB 프레임 프로세서 (BB frame processor) 블록(12300)을 포함할 수 있다. BB 스크램블러 블록(12000)은 입력된 비트 스트림에 대해서 송신단에서 사용한 것과 동일한 PRBS를 발생시켜서 비트열과 XOR하여 디스클램블링을 수행할 수 있다.패딩 리무벌 블록(12100)은 송신단에서 필요에 따라 삽입된 패딩 비트들을 제거할 수 있다.CRC-8 디코더 블록(12200)은 패딩 리무벌 블록(12100)으로부터 입력받은 비트 스트림에 대해서 CRC 디코딩을 수행하여 블록 에러를 체크할 수 있다.BB 프레임 프로세서 블록(12300)은 BB 프레임 헤더에 전송된 정보를 디코딩하고 디코딩된 정보를 이용하여 MPEG-TS, IP 스트림(v4 or v6) 또는 GS(Generic Stream)를 복원할 수 있다.상술한 블록들은 설계자의 의도에 따라 생략되거나, 비슷하거나 동일한 기능을 가진 다른 블록에 의해서 대체될 수 있다.도 13은 본 발명의 다른 실시예에 따른 아웃풋 프로세서를 나타낸 도면이다.도 13에 도시된 아웃풋 프로세서는 도 8에서 설명한 아웃풋 프로세서의 일 실시예에 해당한다. 또한 도 13에 도시된 아웃풋 프로세서는 디매핑 앤 디코딩  모듈로부터 출력된 복수의 DP들을 수신하는 경우에 해당한다. 복수의 DP들에 대한 디코딩은 복수의 DP들에 공통으로 적용될 수 있는 커먼 데이터및 이와 연관된 DP를 머징(merging)하여 디코딩 하는 경우 또는 수신 장치가 여러 개의 서비스 혹은 서비스 컴포넌트 (SVC, scalable video service를 포함)를 동시에 디코딩하는 경우를 포함할 수 있다.도 13에 도시된 아웃풋 프로세서는 도 12에서 설명한 아웃풋 프로세서의 경우와 마찬가지로 BB 디스크램블러 블록, 패딩 리무벌 블록, CRC-8 디코더 블록 및 BB 프레임 프로세서 블록을 포함할 수 있다, 각 블록들은 도 12에서 설명한 블록들의 동작과 구체적인 동작은 다를 수 있으나 기본적인 역할은 동일하다. 도 13에 도시된 아웃풋 프로세서에 포함된 디-지터 버퍼(De-jitter buffer) 블록(13000)은 복수의 DP들간의 싱크를 위해서 송신단에서 임의로 삽입된 딜레이를 복원된 TTO (time to output) 파라미터에 따라 보상할 수 있다.또한 널 패킷 인설션 (Null packet insertion) 블록(13100)은 복원된 DNP (deleted null packet) 정보를 참고하여 스트림내 제거된 널 패킷 을 복원할 수 있으며, 커먼 데이터를 출력할 수 있다.TS 클럭 리제너레이션 (TS clock regeneration) 블록(13200)은 ISCR(Input Stream Time Reference) 정보를 기준으로 출력 패킷의 상세한 시간동기를 복원할 수 있다.TS 리콤바이닝 (TS recombining) 블록(13300)은 널 패킷 인설션 블록(13100)에서 출력된 커먼 데이터 및 이와 관련된 DP들을 재결합하여 원래의 MPEG-TS, IP 스트림 (v4 or v6) 혹은 GS (Generic Stream)로 복원하여 출력할 수 있다. TTO, DNP, ISCR 정보는 모두 BB 프레임 헤더를 통해 획득될 수 있다.인밴드 시그널링 디코더 (In-band signaling decoder) 블록(13400)은 DP의 각 FEC 프레임내 패딩 비트 필드를 통해서 전송되는 인밴드 피지컬 시그널링 (in-band physical layer signaling) 정보를 복원하여 출력할 수 있다.도 13에 도시된 아웃풋 프로세서는 PLS-프리 경로와 PLS-포스트 경로에 따라 입력되는 PLS-프리 정보 및 PLS-포스트 정보를 각각 BB 디스크램블링을 하고 디스크램블링된 데이터에 대해 디코딩을 수행하여 원래의 PLS 데이터를 복원할 수 있다. 복원된 PLS 데이터는 수신 장치 내의 시스템 콘트롤러에 전달되며, 시스템 콘트롤러 는 수신 장치의 싱크로나이제이션 앤 디모듈레이션 모듈, 프레임 파싱 모듈, 디매핑 앤 디코딩 모듈 및 아웃풋 프로세서 모듈에 필요한 파라미터들을 공급할 수 있다.상술한 블록들은 설계자의 의도에 따라 생략되거나, 비슷하거나 동일한 기능을 가진 다른 블록에 의해서 대체될 수 있다.도 14는 본 발명의 다른 실시예에 따른 코딩 앤 모듈레이션 모듈을 나타낸 도면이다. 도 14에 도시된 코딩 앤 모듈레이션 모듈은 도 1 및 5에서 설명한 코딩 앤 모듈레이션 모듈의 다른 실시예에 해당한다.도 14에 도시된 코딩 앤 모듈레이션 모듈은 도 5에서 설명한 바와 같이, 각 DP를 통해 전송하는 서비스나 서비스 콤포넌트별로 QoS를 조절하기 위하여, SISO 방식을 위한 제 1 블록(14000), MISO 방식을 위한 제 2 블록(14100), MIMO 방식을 위한 제 3 블록(14200) 및 PLS-프리/포스트 정보를 처리하기 위한 제 4 블록(14300)을 포함할 수 있다. 또한 본 발명의 일 실시예에 따른 코딩 앤 모듈레이션 모듈은 상술한 바와 같이 설계자의 의도에 따라 각 DP를 동일하게 또는 다르게 처리하기 위한 블록들을 포함할 수 있다. 도 14에 도시된 제 1 블록 내지 제 4 블록(14000-14300)은 도 5에서 설명한 제 1 블록 내지 제 4 블록(5000-5300)과 거의 동일한 블록들을 포함하고 있다.하지만, 제 1 블록 내지 제 3 블록(14000-14200)에 포함된 컨스텔레이션 맵퍼 블록(14010)의 기능이 도 5의 제 1 블록 내지 제 3 블록(5000-5200)에 포함된 컨스텔레이션 맵퍼 블록(5040)의 기능과 다르다는 점, 제 1 블록 내지 제 4 블록(14000-14300)의 셀 인터리버 및 타임 인터리버 사이에 로테이션 앤 I/Q 인터리버 (rotation 0026# I/Q interleaver) 블록(14020)이 포함되어 있다는 점 및 MIMO 방식을 위한 제 3 블록(14200)의 구성이 도 5에 도시된 MIMO 방식을 위한 제 3 블록(5200)의 구성이 다르다는 점에 있어서 차이가 있다. 이하에서는 도 5와 동일한 블록들에 대한 설명은 생략하고 상술한 차이점을 중심으로 설명한다.도 14에 도시된 컨스텔레이션 맵퍼 블록(14010)은 입력된 비트 워드를 콤플렉스 심볼 (complex symbol)로 매핑할 수 있다. 다만, 도 5에 도시된 컨스텔레이션 맵퍼블록(5040)과는 달리 컨스텔레이션 로테이션을 수행하지 않을 수 있다. 도 14에 도시된 컨스텔레이션 맵퍼블록(14010)은 상술한 바와 같이 제 1 블록 내지 제 3 블록(14000-14200)에 공통적으로 적용될 수 있다.로테이션 앤 I/Q 인터리버 블록(14020)은 셀 인터리버에서 출력된 셀 인터리빙이 된 데이터의 각 컴플렉스 심볼의 I (In-phase) 성분과 Q(Quadrature-phase) 성분을 독립적으로 인터리빙 하여 심볼 단위로 출력할 수 있다. 로테이션 앤 I/Q 인터리버 블록(14020)의 입력 데이 터 및 출력 심볼의 개수는 2개 이상이며 이는 설계자의 의도에 따라 변경 가능하다. 또한 로테이션 앤 I/Q 인터리버 블록(14020)은 I 성분에 대해서는 인터리빙을 수행하지 않을 수도 있다.로테이션 앤 I/Q 인터리버 블록(14020)은 상술한 바와 같이 제 1 블록 내지 제 4 블록(14000-14300)에 공통적으로 적용될 수 있다. 이 경우, 로테이션 앤 I/Q 인터리버 블록(14020)이 PLS-프리/포스트 정보를 처리하기 위한 제 4 블록(14300)에 적용되는지 여부는 상술한 프리앰블을 통해 시그널링 될 수 있다.MIMO 방식을 위한 제 3 블록(14200)은 도 14에 도시된 바와 같이, Q-블록 인터리버 (Q-block interleaver) 블록(14210) 및 콤플렉스 심볼 제너레이터  (complex symbol generator) 블록(14220)을 포함할 수 있다.Q-블록 인터리버 블록(14210)은 FEC 인코더로부터 입력받은 FEC 인코딩이 수행된 FEC 블록의 패리티 파트 에 대해 치환(permutation)을 수행할 수 있다. 이를 통해 LDPC H 매트릭스의 패리티 파트를 인포메이션 파트(information part)와 동일하게 순환 구조(cyclic structure)로 만들수 있다 Q-블록 인터리버 블록(14210)은 LDPC H 매트릭스의 Q 크기를 갖는 출력 비트 블록들의 순서를 치환(permutation)한 뒤, 행-열 블록 인터리빙 (row-column block interleaving)을 수행하여 최종 비트열을 생성하여 출력할 수 있다.콤플렉스 심볼 제너레이터 블록(14220)은 Q-블록 인터리버 블록(14210)에서 출력된 비트 열들을 입력받고, 콤플렉스 심볼 로 매핑하여 출력할 수 있다. 이 경우, 콤플렉스 심볼 제너레이터 블록(14220)은 적어도 두개의 경로를 통해 심볼들을 출력할 수 있다. 이는 설계자의 의도에 따라 변경 가능하다.상술한 블록들은 설계자의 의도에 따라 생략되거나, 비슷하거나 동일한 기능을 가진 다른 블록에 의해서 대체될 수 있다.결과적으로 도 14에 도시된 바와 같이 본 발명의 다른 실시예에 따른 코딩 앤 모듈레이션 모듈은 각 경로별로 처리된 DP, PLS-프리 정보, PLS-포스트 정보를 프레임 스트럭쳐 모듈로 출력할 수 있다.도 15는 본 발명의 다른 실시예에 따른 디매핑 앤 디코딩 모듈을 나타낸 도면이다.도 15에 도시된 디매핑 앤 디코딩모듈은 도 8 및 도 11에서 설명한 디매핑 앤 디코딩모듈의 다른 실시예에 해당한다. 또한 도 15에 도시된 디매핑 앤 디코딩모듈은 도 14에서 설명한 코딩 앤 모듈레이션 모듈의 역동작을 수행할 수 있다.도 15에 도시된 바와 같이, 본 발명의 다른 실시예에 따른 디매핑 앤 디코딩 모듈은 SISO 방식을 위한 제 1 블록(15000), MISO 방식을 위한 제 2 블록(15100), MIMO 방식을 위한 제 3 블록(15200) 및 PLS pre/post 정보를 처리하기 위한 제 4 블록(15300)을 포함할 수 있다. 또한 본 발명의 일 실시예에 따른 디매핑 앤 디코딩모듈은 상술한 바와 같이 설계자의 의도에 따라 각  DP를 동일하게 또는 다르게 처리하기 위한 블록들을 포함할 수 있다. 도 15에 도시된 제 1 블록 내지 제 4 블록(15000-15300)은 도 11에서 설명한 제 1 블록 내지 제 4 블록(11000-11300)과 거의 동일한 블록들을 포함하고 있다.하지만, 제 1 블록 내지 제 4 블록(15000-15300)의 타임 디인터리버 및 셀 디인터리버 사이에 I/Q 디인터리버 앤 디로테이션 (I/Q deinterleaver0026# derotation) 블록 (15010)이 포함되어 있다는 점, 제 1 블록 내지 제 3 블록(15000-15200)에 포함된 컨스텔레이션 디맵퍼 블록(15020)의 기능이 도 11의 제 1 블록 내지 제 3 블록(11000-11200)에 포함된 컨스텔레이션 디맵퍼 블록(11030)의 기능과 다르다는 점 및 MIMO 방식을 위한 제 3 블록(15200)의 구성이 도 11에 도시된 MIMO 방식을 위한 제 3 블록(11200)의 구성이 다르다는 점에 있어서 차이가 있다. 이하에서는 도 11과 동일한 블록들에 대한 설명은 생략하고 상술한 차이점을 중심으로 설명한다.I/Q 디인터리버 앤 디로테이션 블록(15010)은 도 14에서 설명한 로테이션 앤 I/Q 인터리버 블록(14020)의 역과정을 수행할 수 있다. 즉, I/Q 디인터리버 앤 디로테이션 블록(15010)은 송신단에서 I/Q 인터리빙되어 전송된 I 및 Q 성분들에 대해 각각 디인터리빙을 수행할 수 있으며, 복원된 I/Q 성분들을 갖는 콤플렉스 심볼을 다시 디로테이션하여 출력할 수 있다. I/Q 디인터리버 앤 디로테이션 블록(15010)은 상술한 바와 같이 제 1 블록 내지 제 4 블록(15000-15300)에 공통적으로 적용될 수 있다. 이 경우, I/Q 디인터리버 앤 디로테이션 블록(15010)이 PLS-프리/포스트 정보를 처리하기 위한 제 4 블록(15300)에 적용되는지 여부는 상술한 프리앰블을 통해 시그널링 될 수 있다.컨스텔레이션 디맵퍼 블록(15020)은 도 14에서 설명한 컨스텔레이션 맵퍼 블록(14010)의 역과정을 수행할 수 있다. 즉, 컨스텔레이션 디맵퍼 블록(15020)은 디로테이션을 수행하지 않고, 셀 디인터리빙된 데이터들에 대하여 디매핑을 수행할 수 있다.MIMO 방식을 위한 제 3 블록(15200)은 도 15에 도시된 바와 같이, 콤플렉스 심볼 파싱 (complex symbol parsing) 블록(15210) 및 Q-블록 디인터리버 (Q-block deinterleaver) 블록(15220)을 포함할 수 있다.콤플렉스 심볼 파싱 블록(15210)은 도 14에서 설명한 콤플렉스 심볼 제너레이터 블록(14220)의 역과정을 수행할 수 있다. 즉, 콤플렉스 데이터 심볼을 파싱하고, 비트 데이터로 디매핑하여 출력할 수 있다. 이 경우, 콤플렉스 심볼 파싱 블록(15210)은 적어도 두개의 경로를 통해 콤플렉스 데이터 심볼들을 입력받을 수 있다.Q-블록 디인터리버 블록(15220)은 도 14에서 설명한 Q-블록 인터리버 블록(14210)의 역과정을 수행할 수 있다. 즉, Q-블록 디인터리버 블록(15220)은 행-열 디인터리빙 (row-column deinterleaving)에 의해서 Q 사이즈 블록들을 복원한 뒤, 치환(permutation)된 각 블럭들의 순서를 원래의 순서대로 복원한 후, 패리티 디인터리빙을 통해서 패리티 비트들의 위치를 원래대로 복원하여 출력할 수 있다.상술한 블록들은 설계자의 의도에 따라 생략되거나, 비슷하거나 동일한 기능을 가진 다른 블록에 의해서 대체될 수 있다.결과적으로 도 15에 도시된 바와 같이, 본 발명의 다른 실시예에 따른 디매핑 앤 디코딩 모듈은 각 경로 별로 처리된 DP 및 PLS 정보를 아웃풋 프로세서로 출력할 수 있다.상술한 바와 같이 본 발명의 일 실시예에 따른 방송 신호 송신 방법 및 장치는 하나의 RF 채널 내에서 서로 다른 방송 송/수신 시스템의 신호들을 멀티플렉싱하고, 멀티플렉싱된 신호들을 전송할 수 있다. 또한 본 발명의 일 실시예에 따른 방송 신호 수신 방법 및 장치는 상술한 방송 신호 송신 동작에 대응한 신호들을 처리할 수 있다. 따라서 유연한 방송 송신 및 수신 시스템의 제공을 할 수 있다.도 16은 본 발명의 일 실시예에 따른 타임 인터리빙 과정을 나타낸 도면이다.상술한 바와 같이 본 발명의 일 실시예에 따른 방송 신호 송신 장치에 포함된 타임 인터리버(또는 타임 인터리버 블록)는 여러 개의 FEC 블록들에 속하는 셀들을 서로 시간 축에 따라 인터리빙하여 출력하는 과정을 수행한다. TI 그룹은 특정 DP를 위한 가변적인 용량 할당이 수행될 수 있는 유닛으로, 가변적인 정수 개수들로 이루어질 수 있으며, 가변적인 개수의 FEC 블록들로 이루어질 수 있다. 타임 인터리빙 블록 (이하 TI 블록)은 타임 인터리빙이 수행되고, 타임 인터리버 메모리의 사용에 대응하는 셀들의 집합에 해당한다. FEC 블록은 DP 데이터의 인코딩된 비트들의 집합 또는 인코딩된 비트들을 전송하는 복수개의 셀들의 집합이 될 수 있다.각 TI 그룹은 하나의 프레임에 매핑되거나 복수의 프레임에 걸쳐 매핑될 수 있다. 각 TI 그룹은 하나 또는 그 이상의 TI 블록으로 나누어질 수 있으며, 각 TI 블록은 타임 인터리버 메모리의 하나 사용에 대응할 수 있다. TI 그룹 내의 TI 블록들은 서로 같거나 다른 개수의 FEC 블록들을 포함할 수 있다.타임 인터리빙을 통해 각 FEC 블록의 셀들은 타임 인터리빙 뎁스 또는 깊이 (time interleaving depth)만큼의 특정 구간 내에 분산되어 전송됨으로써 다이버시티 게인 (diversity gain)을 얻을 수 있다. 본 발명의 일 실시예에 따른 타임 인터리버는 DP 레벨에서 동작할 수 있다.또한, 본 발명의 일 실시예에 따른 타임 인터리버는 서로 다른 입력 FEC 블록들을 주어진 메모리에 순차적으로 배열 (writing operation)한 후 다이아고날 (diagonal) 방향으로 인터리빙하는 과정(diagonal reading operation)을 포함하는 타임 인터리빙을 수행할 수 있다. 특히 본 발명의 일 실시예에 따른 타임 인터리버는 서로 다른 FEC 블록들을 다이아고날 방향으로 읽힐(reading) 때, 읽는 방향의 다이아고날 슬로프 (diagonal slope)의 크기를 변경하여 타임 인터리빙을 수행할 수 있다. 즉, 본 발명의 일 실시예에 따른 타임 인터리버는 TI 리딩 패턴 (reading pattern) 또는 다이아고날-와이즈 리딩 패턴 (diagonal-wise reading pattern)을 변경할 수 있다. 본 발명의 일 실시예에 따른 타임 인터리빙은 다이아고날-타입 타임 인터리빙 (diagonal-type Time interleaving) 또는 다이아고날-타입 TI (diagonal-type TI), 또는 플렉서블 다이아고날-타입 인터리빙 (flexible diagonal-type time interleaving) 또는 플렉서블 다이아고날-타입 TI (flexible diagonal-type TI)라고 호칭될 수 있다. 일반적으로, 타임 인터리버는 프레임 빌딩 전, DP 데이터를 위한 버퍼로서 동작할 수 있다. 이는 각 예를 위한 두 개의 메모리 뱅크들에 의해 수행이 가능하다.  첫번째 TI 블록은 첫번째 메모리 뱅크에 쓰여질 수 있으며, 두번째 TI 블록은 첫번째 TI 블록이 첫번째 메모리 뱅크로부터 읽혀 나오는 동안 두번째 메모리 뱅크에 쓰여질 수 있다.구체적인 수행 장치의 명칭이나 수행 장치의 위치 또는 수행 장치의 기능 등은 설계자의 의도에 따라 변경 가능하다.본 발명의 일 실시예에 따른 하나의 TI 블록은 Nc 개의 FEC 블록들로 구성될 수 있으며, FEC 블록의 길이는 Nr x 1로 가정할 수 있다. 따라서 본 발명의 일 실시예에 따른 TI 메모리는 Nr x Nc 행렬의 크기와 동일한 크기를 가질 수 있다. 또한, 본 발명의 일 실시예에 따른 타임 인터리빙의 깊이는 FEC 블록 길이와 동일하다. 도면에 도시된 (a)는 본 발명의 일 실시예에 따른 타임 인터리빙의 쓰기 방향(writing direction)을 도시한 도면이며, (b)는 본 발명의 일 실시예에 따른 타임 인터리빙의 읽기 방향 (reading direction)을 도시한 도면이다.구체적으로, (a)에 도시된 바와 같이, 본 발명의 일 실시예에 따른 방송 신호 송신 장치는 입력된 FEC 블록들을 Nr x Nc의 크기를 갖는 TI 메모리에 컬럼 또는 열  (column) 방향으로 순차적으로 쓰여질 수 있다(컬럼-와이즈 라이팅, Column-wise writing). 첫번째 FEC블록 0은 TI 메모리의 첫번째 컬럼에 열 방향으로 쓰여질 수 있으며, 두번째 FEC블록 1은 그 다음 컬럼에 열 방향으로 쓰여질 수 있다. 그 다음 FEC블록들도 마찬가지이다.이후, (b)에 도시된 바와 같이, 본 발명의 일 실시예에 따른 방송 신호 송신 장치는 컬럼 방향으로 쓰여진 FEC 블록들을 다이아고날 방향으로 읽을 수 있다. 이 경우, 본 발명의 일 실시예에 따른 방송 신호 송신 장치는 한 주기 (one period) 동안, 다이아고날 라이팅 (diagonal reading)을 수행 할 수 있다. 특히 이 경우, 도 (b)에 도시된 바와 같이 TI 읽는 방향 (reading direction)의 다이아고날 슬로프는 각 TI 블록마다 또는 수퍼 프레임 단위마다 다르게 설정될 수 있다.즉, 가장 왼쪽 열에서 시작하는 첫번째 행부터 오른쪽 방향으로 행을 따라 마지막 열까지 읽어내려오는 다이아고날-와이즈 리딩이 수행되는 동안,   셀들은 (b)에 도시된 바와 같이 읽힐 수 있다.특히 (b)에 도시된 바와 같이 TI 읽는 방향의 다이아고날 슬로프는 각 TI 블록마다 또는 수퍼 프레임 단위마다 다르게 설정될 수 있다. 도 16은 TDI 쓰는 방향 (writing direction)의 다이아고날 슬로프가 다이아고날 슬로프-1 또는 다이아고날 슬로프-2인 경우를 나타내고 있다.TI 읽는 방향의 다이아고날 슬로프가 다이아고날 슬로프-1인 경우, 첫 번째 주기 다이아고날 리딩 과정은 메모리 행렬의 (0,0)에서 시작하여 열의 맨 하단의 셀을 읽을 때까지 수행되므로, 서로 다른 FEC 블록들 내의 셀들을 고르게 인터리빙될 수 있다. 다음 주기의 다이아고날 리딩은 그림에서 ① ② ③ … 순서대로 진행될 수 있다.또한, 또한, TI 읽는 방향의 다이아고날 슬로프가 다이아고날 슬로프-2인 경우, TI 다이아고날 리딩은 TI 읽는 방향의 다이아고날 슬로프에 따라 첫번째 주기 동안 메모리 행렬의 (0,0)에서 시작하여 특정 시프팅 값에 따른 특정 FEC 블록에 포함된 셀들을 읽을 때까지 수행될 수 있다. 이는 설계자의 의도에 따라 변경 가능한 사항이다.도 17은 본 발명에 따른 다이아고날 슬로프들의 실시예를 나타낸 도면이다.도 17은 TI 블록의 Nc의 크기가 7, Nr의 크기가 11인 경우의 다이아고날 슬로프-1부터 다이아고날 슬로프-6 까지의 실시예를 나타낸다. 본 발명의 일 실시예에 따른 다이아고날 슬로프의 크기는 설계자의 의도에 따라 변경 가능하다.본 발명의 일 실시예에 따른 타임 인터리버는 맥시멈 TI 메모리 크기에 따라 TI  읽는 방향의 다이아고날 슬로프의 크기를 변경하여 TI 리딩 패턴 (reading pattern)을 변경할 수 있다. TI 리딩 패턴은 시간축상에서 연속적으로 전송되는 신호 프레임들의 집합인 수퍼 프레임 단위로 변경될 수 있으며. TI 리딩 패턴에 관한 정보는 상술한 스태틱 PLS 시그널링 데이터 (static PLS signaling data)를 통해 전송될 수 있다.도 18은 본 발명의 다른 실시예에 따른 타임 인터리빙 과정을 나타낸 도면이다.도 18은 상술한 다이아고날-타입 TI의 라이팅 오퍼레이션 및 리딩 오퍼레이션의 다른 실시예를 나타낸다.본 발명의 일 실시예에 따른 하나의 TI 블록은 4개의 FEC 블록들로 구성이 되며, 각 FEC 블록길이는 8개의 셀로 구성될 수 있다. 따라서 TI 메모리 크기는 8 x 4 행렬 배열 (또는 32x1)의 크기와 동일하며, 컬럼의 길이와 로우(row, 행)의 길이는 각각 FEC 블록 길이 (또는 타임 인터리빙 뎁스)와 FEC 개수와 같음을 알 수 있다.도 18의 좌측에 도시된 TI 인풋 FEC 블록들에 대응하는 블록은 타임 인터리버에 순차적으로 입력되는 FEC 블록들을 나타낸다.도 18의 가운데에 도시된 TI FEC 블록들에 대응하는 블록은 TI 메모리에 저장된 i번째 FEC 블록의 n번째 셀 값들을 나타내며, TI 메모리 인데스들 (memory indexes)에 대응하는 도면은 TI 메모리에 저장된 FEC 블록의 셀들의 순서를 지시하는 메모리 인덱스들을 나타낸다.(a)는 TI 쓰기 동작 (또는 라이팅 오퍼레이션, writing operation)을 나타낸다. 상술한 바와 같이, 순차적으로 입력된 FEC 블록들은 TI 메모리에 열 방향으로 순차적으로 쓰일 수 있다. 따라서 각 FEC 블록의 셀들은 순차적으로 저장되어 TI 메모리 인덱스에 쓰일 수 있다.(b)는 TI 읽기 동작 (리딩 오퍼레이션, reading operation)을 나타낸다. 도면에 도시된 바와 같이, TI 메모리에 저장된 셀 값들은 메모리 인덱스 0, 9, 18, 27… 에 따라 다이아고날 방향으로 읽혀져서 출력될 수 있다. 또한 다이아고날-와이즈 리딩 (diagonal-wise reading)이 시작되는 셀의 위치 또는 다이아고날-와이즈 리딩 패턴 (diagonal-wise reading pattern)은 설계자의 의도에 따라 변경 가능하다.도 18의 우측에 도시된 TI 아웃풋 FEC 블록들에 대응하는 블록은, 본 발명의 일 실시예에 따른 다이아고날-타입 TI (diagonal-type TI)를 통해 출력된 셀 값들을 순차적으로 나타낸다. TI 아웃풋 메모리 인데스들 (output memory indexes)에 대응하는 블록은 다이아고날-타입 TI를 통해 출력된 셀 값들에 대응하는 메모리 인덱스들을 나타낸다.결과적으로, 본 발명의 일 실시예에 따른 타임 인터리버는 순차적으로 입력되는 FEC 블록들에 대하여 TI 아웃풋 메모리 인덱스 값들을 순차적으로 발생시켜 다이아고날-타입 TI 수행을 할 수 있다.도 18은 도 17에서 설명한 TI 읽기 동작의 다이아고날 슬로프-1의 타임 인터리버의 동작을 나타내며, 도 17에 도시된 나머지 TI 읽기 동작의 다이아고날 슬로프들의 경우에도, 상술한 동작이 동일하게 적용될 수 있다.도 19는 본 발명의 일 실시예에 따른 TI 아웃풋 메모리 인덱스를 생성하는 과정을 나타낸 도면이다.도 18에서 상술한 바와 같이 본 발명의 일 실시예에 따른 타임 인터리버는 순차적으로 입력되는 FEC 블록들에 대하여 TI 아웃풋 메모리 인덱스 값들을 순차적으로 발생시켜 다이아고날-타입 TI를 수행할 수 있다.도 19에 도시된 (a)는 상술한 순차적으로 입력되는 FEC 블록들에 대해 다이아고날-타입 TI를 위한 메모리 인덱스를 생성시키는 메모리 인덱스 생성과정을 나타내며, (b)는 메모리 인덱스 발생 과정을 나타낸 수학식이다.이하의 수학식은 도 17에서 설명한 다양한 TI 읽기 동작의 다이아고날 슬로프 값들이 설정된 경우, 다이아고날-타입 TI를 수행하기 위한 메모리 인덱스 발생 과정을 나타낸다.본 발명의 일 실시예에 따른 방송 신호 수신 장치에 포함된 타임 디인터리버(또는 타임 디인터리버 블록)는 상술한 다이아고날-타입 TI의 역과정을 수행할 수 있다. 즉, 본 발명의 일 실시예에 따른 타임 디인터리버는 다이아고날-타입 TI이 수행되어 전송된 FEC 블록들을 입력받고 TI 메모리에 다이아고날 방향으로 쓰기 동작을 수행 후 순차적으로 읽기 동작을 수행하여 타임 디인터리빙을 수행할 수 있다. 본 발명의 일 실시예에 따른 타임 디인터리빙은 다이아고날-타입 TDI 또는 다이아고날-타입 타임 디인터리빙 또는 또는 플렉서블 다이아고날-타입 타임 디인터리빙 (flexible diagonal-type time deinterleaving) 또는 플렉서블 다이아고날-타입 TDI (flexible diagonal-type TDI)라고 호칭될 수 있다. 구체적인 수행 장치의 명칭이나 수행 장치의 위치 또는 수행 장치의 기능 등은 설계자의 의도에 따라 변경 가능하다. 도 20은 본 발명의 일 실시예에 따른 타임 디인터리빙 과정을 나타낸 도면이다.도 20에 도시된 타임 디인터리빙 과정은 도 16에서 설명한 타임 인터리빙 과정의 역과정에 해당한다.(a)는 본 발명의 일 실시예에 따른 타임 디인터리빙의 쓰기 방향을 도시한 도면이며, (b)는 본 발명의 일 실시예에 따른 타임 디인터리빙의 읽기 방향을 도시한 도면이다.구체적으로 (a)에 도시된 바와 같이, 본 발명의 일 실시예에 따른 타임 디인터리버는 송신측에서 다이아고날- 타입 TI이 수행된 FEC 블록들을 입력받고, TDI(time deinterleaver) 메모리에 다이아고날 방향으로 쓸 수 있다(다이아고날-와이즈 라이팅, Diagonal-wise writing).이 경우, 본 발명의 일 실시예에 따른 타임 인터리버는 한 주기 (one period) 동안, 다이아고날 쓰기를 수행할 수 있다. 특히 도 (a)에 도시된 바와 같이 TDI 쓰기 방향의 다이아고날 슬로프의 값은 각 TDI 블록마다 또는 수퍼 프레임 단위마다 다르게 설정될 수 있다. 도 20은 TDI 쓰기 방향의 다이아고날 슬로프가 다이아고날 슬로프가-1 또는 다이아고날 슬로프가-2인 경우를 나타내고 있다.TDI 쓰기 방향의 다이아고날 슬로프가 다이아고날 슬로프-1인 경우,첫 번째 주기 다이아고날 쓰기 (diagonal writing)는 메모리 행렬의 (0,0)에서 시작하여 행의 맨 하단의 셀을 읽을 때까지 수행된다. 각 주기의 다이아고날 쓰기는 그림에서 ① ② ③ … 순서대로 진행될 수 있다. 또한, TDI 쓰기 방향의 다이아고날 슬로프가 다이아고날 슬로프-2인 경우, TDI 다이아고날 쓰기 (diagonal writing)는 첫번째 주기 동안 메모리 행렬의 (0,0)에서 시작하여 특정 시프팅 값에 따른 특정 FEC 블록에 포함된 셀들을 읽을 때까지 수행될 수 있다. 이는 설계자의 의도에 따라 변경 가능한 사항이다. 또한 (b)에 도시된 바와 같이, 본 발명의 일 실시예에 따른 타임 디인터리버는 다이아고날 방향으로 쓰여진 FEC 블록들을 컬럼 (column) 방향으로 순차적으로 읽을  수 있다(컬럼-와이즈 리딩, Column-wise reading).도 21은 본 발명의 다른 실시예에 따른 타임 디인터리빙 과정을 나타낸 도면이다.도 21에 도시된 타임 디인터리빙 과정은 도 18에서 설명한 타임 인터리빙 과정의 역과정에 해당한다.본 발명의 일 실시예에 따른 하나의 TI 블록은 4개의 FEC 블록들로 구성이 되며, 각 FEC 블록길이는 8개의 셀로 구성될 수 있다. 따라서 TI 메모리 크기는 8 x 4 행렬 배열 (또는 32x1)의 크기와 동일하며, 열 (column)의 길이와 행 (row)의 길이는 각각 FEC 블록 길이 (또는 타임 인터리빙 뎁스)와 FEC 개수와 같음을 알 수 있다.도 21의 좌측에 도시된 TDI 인풋 FEC 블록들에 대응하는 블록은 타임 디인터리버에 순차적으로 입력되는 FEC 블록들의 셀들을 나타내며, TDI 인풋 메모리 인덱스들 (input memory indexes)에 대응하는 블록은 순차적으로 입력되는 FEC 블록들의 셀들에 대응하는 메모리 인덱스들을 나타낸다.도 21의 가운데에 도시된 TDI FEC 블록들에 대응하는 블록은 TDI 메모리에 저장된 i번째 FEC 블록의 n번째 셀 값들을 나타내며, TDI 메모리 인덱스들 (memory indexes)에 대응하는 도면은 TDI 메모리에 저장된 FEC 블록의 셀들의 순서를 지시하는 메모리 인덱스들을 나타낸다.(a)는 TDI 쓰기 동작 (writing operation)을 나타낸다. 상술한 바와 같이, 순차적으로 입력된 FEC 블록들은 TDI 메모리에 다이아고날 방향으로 순차적으로 쓰여질 수 있다. 따라서 입력된 FEC 블록들의 셀들은 순차적으로 저장되어 TDI 메모리 인덱스에 쓰여질 수 있다.(b)는 TDI 읽기 동작 (리딩 오퍼레이션, reading operation)을 나타낸다. 도면에 도시된 바와 같이, TDI 메모리에 저장된 셀 값들은 메모리 인덱스 0, 1, 2, 3… 에 따라 열 (column) 방향으로 읽혀 출력될 수 있다.도 21의 우측에 도시된 TDI 아웃풋 FEC 블록들에 대응하는 블록은, 본 발명의 일 실시예에 따른 타임 디인터리빙을 통해 출력된 셀 값들을 순차적으로 나타낸다. TDI 아웃풋 메모리 인덱스들 (output memory indexes)에 대응하는 블록은 본 발명의 일 실시예에 따른 타임 디인터리빙을 통해 출력된 셀 값들에 대응하는 메모리 인덱스들을 나타낸다.결과적으로, 본 발명의 일 실시예에 따른 타임 다인터리버는 순차적으로 입력되는 FEC 블록들에 대하여 TDI 아웃풋 메모리 인덱스 값들을 순차적으로 발생시켜 다이아고날-타입 TDI를 수행할 수 있다.도 21은 도 17에서 설명한 다이아고날 슬로프-1에 대응하는 타임 디인터리버의 동작을 나타내며, 도 17에 도시된 나머지 다이아고날 슬로프들의 경우에도, 상술한 타임 디인터리빙 동작이 적용될 수 있다.도 22는 본 발명의 일 실시예에 따른 TDI 아웃풋 메모리 인덱스를 생성하는 과정을 나타낸 도면이다.상술한 바와 같이 본 발명의 일 실시예에 따른 타임 디인터리버는 순차적으로 입력되는 FEC 블록들에 대하여 TDI 아웃풋 메모리 인덱스 값들을 순차적으로 발생시켜 다이아고날-타입 TDI를 수행할 수 있다.도 22에 도시된 (a)는 상술한 순차적으로 입력되는 FEC 블록들에 대해 다이아고날-타입 TDI를 위한 메모리 인덱스를 생성시키는 메모리 인덱스 생성 과정을 나타내며, (b)는 메모리 인덱스 발생 과정을 나타낸 수학식이다.이하의 수학식은 도 17에서 설명한 다양한 TI 읽기 동작의 다이아고날 슬로프 값들이 설정된 경우, 다이아고날-타입 TDI를 수행하기 위한 TDI 아웃풋 메모리 인덱스 발생 과정을 나타낸다.또한, 본 발명의 일 실시예에 따른 방송 신호 송신 장치는 복수 개의 FEC 블록들이 복수 개의 TI 블록으로 패킹 (packing) 및 구성되어 전송되는 VDR (variable data-rate) 시스템일 수 있다. 이 경우 하나의 TI 블록에 포함된 FEC 블록 개수는 각 TI 블록 별로 다를 수 있다. 도 23은 본 발명의 일 실시예에 따른 VDR (variable data-rate) 시스템을 나타낸 개념도이다.구체적으로, 도 23은 하나의 전송 슈퍼 프레임은 NIF_NUM개의 인터리빙 프레임(Interleaving Frame, IF)들로 구성되며 각 IF는 NFEC_NUM 개의 FEC 블록들을 포함할 수 있다. 이 경우, 각 IF에 포함된 FEC 블록의 개수는 서로 다를 수 있다. 본 발명의 일 실시예에 따른 IF는 타임 인터리빙을 수행하기 위한 블록으로 정의될 수 있으며, 상술한 TI 블록으로 호칭될 수 있다.상술한 바와 같이 본 발명의 일 실시예에 따른 VDR (variable data-rate) 시스템의 경우, 방송 신호 송신 장치는 복수 개의 FEC 블록들을 복수 개의 IF로 패킹하여 전송할 수 있다. 이 경우 하나의 IF에 포함되는 FEC 블록 개수는 각 IF마다 다를 수 있다.이하에서는 상술한 VDR (variable data-rate) 시스템에서 수행될 수 있는 타임 인터리빙에 대해 설명한다. 이는 상술한 타임 인터리빙의 또 다른 실시예로서, 방송 신호 수신 장치가 싱글 메모리를 갖는 경우에도 적용될 수 있다는 장점을 가진다.본 발명의 다른 실시예에 따른 타임 인터리빙은 상술한 다이아고날-타입 TI와 동일하게 호칭될 수 있으며, 본 발명의 일 실시예에 따른 방송 신호 송신 장치 내의 타임 인터리버에서 수행될 수 있다. 또한 이에 대한 역과정으로서, 타임 디인터리빙은 다이아고날-타입 TDI라고 호칭될 수 있으며 본 발명의 일 실시예에 따른 방송 신호 수신 장치 내의 타임 디인터리버에서 수행될 수 있다. 구체적인 수행 장치의 명칭이나 수행 장치의 위치 또는 수행 장치의 기능 등은 설계자의 의도에 따라 변경 가능하다. 이하 구체적인 동작을 설명한다.상술한 바와 같이, IF 내에 포함된 FEC 블록들의 개수가 서로 다른 경우, 각 IF마다 서로 다른 다이아고날-타입 TI 방식을 적용해야 한다. 하지만 이러한 방식은 방송 신호 수신 장치가 싱글 메모리를 사용하는 경우, 이에 대응하는 디인터리빙이 수행될 수 없다는 문제점이 있다.따라서 본 발명의 방송 신호 송신 장치는 하나의 다이아고날-타입 TI 방식을 결정하고 모든 IF에 대해 동일하게 적용하도록 하는 것을 일 실시예로 할 수 있다. 또한 본 발명의 일 실시예에 따른 방송 신호 송신 장치는 이에 대응하여 싱글 메모리를 사용하여 복수 개의 IF들을 순차적으로 디인터리빙할 수 있다.이 경우, 본 발명의 일 실시예에 따른 방송 신호 송신 장치는 모든 TI 블록들에 대해 적용되는 하나의 다이아고날-타입 TI 방법을 하나의 신호 프레임 내에서 FEC 블록 개수를 가장 많이 포함하고 있는 IF를 기준으로 하여 결정할 수 있다. 또한 본 발명의 일 실시예에 따른 방송 신호 송신 장치는 하나의 신호 프레임 내에서 가장 많은 FEC 블록의 개수와 가장 적은 FEC 블록의 개수의 중간 값에 해당하는 IF 또는 임의의 IF를 기준으로 하여 하나의 다이아고날-타입 TI 방법을 결정할 수 있다. 이는 설계자의 의도에 따라 변경 가능하다.이 경우, FEC 블록 개수를 가장 많이 포함하고 있는 IF와 대비하여 FEC 블록의 개수가 적은 TI 블록에 대해 상술한 다이아고날-타입 TI를 어떻게 적용할 것인지 여부가 문제될 수 있다.따라서 본 발명의 방송 신호 송신 장치는 발생하는 메모리 인덱스를 모니터링하여 적용 여부를 결정하는 것을 일 실시예로 할 수 있다.구체적으로, 본 발명의 방송 신호 송신 장치는 발생된 TI 메모리 인덱스들이 임의의 IF 내의 전체 셀 개수를 초과하는 경우, 해당 IF에 대하여 가상의 FEC 블록들을 추가하여 (zero padding) 다이아고날-타입 TI를 수행할 수 있다. 이 경우, 제로 패딩된 가상의 FEC 블록들은 데이터를 포함하지 않으므로 다이아고날-타입 TI의 리딩 오퍼레이션에서 스킵된다. 이를 스킵 오퍼레이션 (skip operation)이라 호칭할 수 있다. 따라서 TI 아웃풋 메모리 인덱스는 실제 데이터를 포함하는 셀들에 대응하는 값들만을 포함할 수 있다. skip operation에 대해서는 후술한다.또한 본 발명의 방송 신호 송신 장치는 상술한 다이아고날-타입 TI 방법을 서로 다른 IF에 대해 적용함에 있어서, FEC 블록 개수가 적은 IF부터 순차적으로 FEC 블록 개수의 개수에 따라 적용하는 것을 일 실시예로 할 수 있다. 따라서 본 발명의 일 실시예에 따른 방송 신호 수신 장치는 싱글-메모리를 간단히 운영할 수 있다. 구체적인 내용은 후술한다.이하의 수학식은 상술한 모든 IF에 대해 적용되는 하나의 다이아고날-타입 TI 방법을 결정하는 과정을 나타낸다.도 24는 본 발명의 또 다른 실시예에 따른 타임 인터리빙 과정을 나타낸 도면이다.구체적으로 도 24는 VDR (variable data-rate) 시스템에서 다이아고날-타입 TI가 적용된 일 실시예를 나타낸다.(a)는 4개의 FEC 블록들을 포함하는 IF-0에 대해 다이아고날-타입 TI가 적용되는 과정을 나타내며, (b)는 5개의 FEC 블록들을 포함하는 IF-1에 대해 다이아고날-타입 TI가 적용되는 과정을 나타낸다.TI FEC 블록들에 대응하는 블록들은 각 IF에 포함된 FEC 블록들 및 각 FEC 블록들에 포함된 셀 값들을 나타낸다. TI 메모리 인덱스들 (memory indexes)에 대응하는 블록들은 IF들에 포함된 셀 값들에 대응하는 메모리 인덱스를 나타낸다.각 IF는 하나의 수퍼 프레임에 포함되며, 각 FEC 블록은 8개의 셀들을 포함할 수 있다.본 발명의 일 실시예에 따른 방송 신호 송신 장치는 두 개의 IF들에 대해 동일하게 적용하기 위한 다이아고날-타입 TI 방식을 결정할 수 있다. 상술한 바와 같이, 본 발명의 일 실시예에 따른 다이아고날-타입 TI 방식은 하나의 신호 프레임 내에서 FEC 블록 개수를 가장 많이 포함하고 있는 IF를 기준으로 하여 결정되므로, 도 24의 경우, IF-1을 기준으로 다이아고날-타입 TI 방식이 결정된다. 따라서 TI 메모리의 크기는 8 x 5 행렬 배열 (또는 40x1)의 크기와 동일할 수 있다.(a)의 상단에 도시된 바와 같이, IF-0에 포함된 FEC 블록들은 4개로서, IF-1에 포함된 FEC 블록들의 개수보다 적다. 따라서, 본 발명의 일 실시예에 따른 방송 신호 송신 장치는 IF-0에 대하여, IF의 가장 마지막에 제로 값을 갖는 버츄얼 (virtual) FEC 블록(24000)을 부가(padding)하고 해당 셀들을 TI 메모리에 컬럼-와이즈 라이팅 (column-wise writing)을 수행 할 수 있다. 버츄얼 FEC 블록이 추가되는 위치는 설계자의 의도에 따라 변경 가능하다. 따라서 상술한 바와 같이 제로 패딩에 따른 다이아고날-타입 TI 방식을 이용하여, 본 발명의 일 실시예에 따른 방송 신호 수신 장치는 싱글-메모리인 경우에도 이에 대응하는 디인터리빙을 수행할 수 있다.이후 (a)의 하단에 도시된 바와 같이, 본 발명의 일 실시예에 따른 방송 신호 송신 장치는 TI 메모리에 쓰여진 셀들을 다이아고날 방향으로 읽어 낼 수 있다. 이 경우, 마지막 열은 버츄얼 FEC 블록에 해당하므로 해당 셀들은 무시하고 일기 동작을 수행할 수 있다.IF-1에 대하여, 본 발명의 일 실시예에 따른 방송 신호 송신 장치는 (b)의 상단 및 하단에 도시된 바와 같이 상술한 방법에 따라 컬럼-와이즈 쓰기 동작을 수행하고, 다이아고날 읽기 동작을 수행할 수 있다.상술한 바와 같이, 본 발명의 일 실시예에 따른 다이아고날-타입TI는 적은 FEC 블록들을 포함하는 IF에 대해 먼저 적용되므로, 도 24의 경우, IF-0에 대해서 먼저 적용될 수 있다.도 25는 본 발명의 다른 실시예에 따른 TI 아웃풋 메모리 인덱스를 생성하는 과정을 나타낸 도면이다.도 25는 상술한 두 개의 IF (IF-0 및 IF-1)에 대해 TI 아웃풋 메모리 인덱스를 생성하는 과정 및 TI 아웃풋 메모리 인덱스에 대응하는 TI 아웃풋 FEC 블록들을 나타낸다.TI 아웃풋 메모리 인덱스들 (output memory indexes)에 대응하는 블록들은 TI 아웃풋 메모리 인덱스를 생성하는 과정을 나타내며, TI 아웃풋 FEC 블록들에 대응하는 블록들은 생성된 TI 아웃풋 메모리 인덱스에 대응하는 FEC 블록들의 셀 값들을 나타낸다.(a)는 IF-0의 TI 아웃풋 메모리 인덱스 발생 과정을 나타낸다. (a)의 상단에 도시된 바와 같이, TI 메모리 인덱스들이 IF-0 내의 전체 셀 개수를 초과하는 경우, 본 발명의 일 실시예에 따른 방송 신호 송신 장치는 버추얼 FEC 블록내의 셀들에 해당하는 32 내지 39번에 대응하는 TI 메모리 인덱스를 무시할 수 있다. 이를 스킵 오퍼레이션 (skip operation) 이라고 호칭할 수 있다. 그 결과, (a)의 가운데에 도시된 바와 같이, 스킵된 TI 메모리 인덱스들을 제외하고 읽기 동작을 수행할 수 있는 최종 아웃풋 메모리 인덱스가 발생된다. (a)의 하단에는 최종 아웃풋 메모리 인덱스에 대응하는 출력 FEC 블록들의 셀 값들이 도시되어 있다.(b)는 IF-1의 TI 아웃풋 메모리 인덱스 발생 과정을 나타낸다. IF-1의 경우 스킵 오퍼레이션이 적용되지 않음을 확인할 수 있다. 구체적인 과정은 상술한 바와 동일하다.이하의 수학식은 상술한 VDR (variable data-rate) 시스템에서 적용될 수 있는 다이아고날-타입 TI을 수행하기 위한 아웃풋 메모리 인덱스 발생 과정을 나타낸다.상술한 수학식에서 if 조건부는 상술한 스킵 오퍼레이션을 나타낸다. 또한 본 수학식은 상술한 다이아고날 슬로프에 따른 다이아고날-타입 TI를 수행하기 위한 아웃풋 메모리 인덱스 발생 과정을 나타낸다. 따라서 다이아고날 슬로프 값을 하나의 변수로 규정하고 있다.도 26은 본 발명의 일 실시예에 따른 TI 메모리 인덱스 생성 과정을 나타낸 순서도 이다.상술한 바와 같이 본 발명의 일 실시예에 따른 타임 인터리버는 순차적으로 입력되는 FEC 블록들에 대하여 TI 아웃풋 메모리 인덱스 값들을 순차적으로 발생시켜 다이아고날-타입 TI를 수행할 수 있다.도 26에 도시된 바와 같이, 본 발명의 일 실시예에 따른 방송 신호 송신 장치는 초기값들 (initial values)을 설정할 수 있다(S26000). 즉, 본 발명의 일 실시예에 따른 방송 신호 송신 장치는 모든 IF에 대해 적용되는 하나의 다이아고날-타입 TI 방법을 하나의 신호 프레임 내에서 FEC 블록 개수를 가장 많이 포함하고 있는 IF를 기준으로 하여 결정할 수 있다.이후, 본 발명의 일 실시예에 따른 방송 신호 송신 장치는 일시적인 TI 메모리-인덱스 (Temporal TI memory-index)를 생성할 수 있다(S26100). 즉, 본 발명의 일 실시예에 따른 방송 신호 송신 장치는 FEC 블록의 개수가 설정된 TI 메모리 인덱스보다 작은 IF들에 대하여 버츄얼 FEC 블록을 부가(padding)하여 TI 메모리에 쓸 수 있다.이후, 본 발명의 일 실시예에 따른 방송 신호 송신 장치는 생성된 TI 메모리-인덱스의 가용성(availability)을 평가(evaluate)할 수 있다(S26200). 즉, 본 발명의 일 실시예에 따른 방송 신호 송신 장치는 TI 메모리에 쓰여진 셀들을 다이아고날 방향으로 읽을 수 있다. 이 경우, 버츄얼 FEC 블록에 해당하는 셀들은 무시하고 읽기 동작을 수행할 수 있다.이후, 본 발명의 일 실시예에 따른 방송 신호 송신 장치는 최종 TI 메모리-인덱스를 생성할 수 있다(S26300).도 26에 도시된 순서도는 도 23 내지 25에서 설명한 TI 아웃풋 메모리 인덱스를 생성하는 과정에 대응되며, 설계자의 의도에 따라 변경 가능하다.도 27은 본 발명의 또 다른 실시예에 따른 타임 디인터리빙 과정을 나타낸 도면이다.도 27에 도시된 타임 디인터리빙 과정은 도 24 내지 도 26에서 설명한 타임 인터리빙 과정의 역과정에 해당하며, 상술한 IF-0 및 IF-1을 실시예로 설명한다.특히 본 발명의 또 다른 실시예에 따른 타임 디인터리빙은 방송 신호 수신 장치가 싱글 메모리를 사용하는 경우에 적용될 수 있다. 이러한 싱글-메모리 어프로치를 획득하기 위해서, 인터리빙된 IF들을 위한 읽기 및 쓰기 동작들은 연속적으로 수행될 수 있다. 타임 디인터리빙 과정은 효과적인 타임 디인터리빙 실행을 이끌어 낼 수 있는 클로즈드 폼(closed form)으로 표현될 수 있다.본 발명의 또 다른 실시예에 따른 타임 디인터리빙은 4개의 단계의 과정들을 통해 진행 될 수 있다. 도 27의 (a)는 타임 디인터리빙의 첫번째 단계(step 1)를 나타낸다. IF-0을 타임 디인터리빙 과정 전에 타임 인터리빙 과정동안 타임 인터리빙 룰에 따라 스킵되었던 메모리 인덱스에 대응하는 셀들은 제로 또는 초기 값으로 설정될 수 있다. 즉, (a)의 상단에 도시된 블록은 IF-0의 최종 아웃풋 메모리 인덱스에 대응하는 출력 FEC 블록들의 셀 값들을 나타내며, (a)의 하단에 도시된 블록은 스킵 오퍼레이션에서 스킵된 메모리 인덱스에 대응하는 셀 값들을 제로로 세팅하여 생성한 FEC 블록들의 셀 값들을 나타낸다.두번째 단계 (step 2)로서, 첫번째 단계 이후, 해당 아웃풋은 실제 8 x 5  사이즈의 싱글 메모리에 쓰여질 수 있으며, 쓰기 방향은 타임 인터리빙 과정의 읽기 동작의 방향과 동일할 수 있다. 본 발명의 일 실시예에 따른 방송 신호 수신 장치는 입력되는 첫 번째 IF에 대해 송신단의 TI의 첫번째 역 과정으로서 다이아고날 쓰기 (다이아고날 라이팅, diagonal writing) 동작을 수행할 수 있다. 즉, 다이아고날 라이팅의 방향은 송신단에서 이루어진 다이아고날 읽기 (다이아고날 리딩, diagonal reading)의 방향과는 반대 방향으로 이루어질 수 있다.도 27의 (b)는 타임 디인터리빙의 세번째 단계(step 3)를 나타낸다.TDI FEC 블록들에 대응하는 블록들은 입력되는 FEC 블록들의 셀 값들을 나타낸다. TDI 메모리 인덱스들에 대응하는 블록들은 FEC 블록들의 셀 값들에 대응하는 TDI 메모리 인덱스를 나타낸다.두번째 스텝 이후, 타임 인터리빙의 라이팅 오퍼레이션의 방향과 동일한 방향으로 컬럼-와이즈 리딩 오퍼레이션(column-wise reading operation)이 수행될 수 있다. 이 경우, 읽혀진 셀들의 값이 제로 또는 초기값이라면 해당 셀들은 스킵오퍼레이션(skip operation)을 통해 스킵될 수 있다. 이러한 스킵 오퍼레이션은 상술한 방송 신호 송신 장치에서 수행된 스킵 오퍼레이션과 상응한다.이하의 수학식은 상술한 TDI 메모리 인덱스를 발생하는 과정을 나타낸다.상술한 수학식에서 if 조건부는 상술한 스킵 오퍼레이션, 즉, TDI 출력 메모리 인덱스에 저장되어 있는 셀 값이 0 (또는 강제로 삽입한 내용임을 확인 수 있는 임의의 값) 일 경우 인덱스를 무시하는 과정을 나타낸다.또한 본 수학식은 상술한 다이아고날 슬로프에 따른 다이아고날-타입 TI에 대응하는 타임 디인터리빙을 수행하기 위한 TDI 메모리 인덱스를 발생하는 과정을 나타낸다.도 28은 본 발명의 또 다른 실시예에 따른 타임 디인터리빙 과정을 나타낸 도면이다.상술한 바와 같이 본 발명의 일 실시예에 따른 방송 신호 수신 장치는 싱글 메모리를 이용하여 타임 디인터리빙을 수행할 수 있다. 따라서 상술한 타임 디인터리빙의 네번째 단계(step 4)로서, 본 발명의 일 실시예에 따른 방송 신호 수신 장치는 IF-0을 읽는 동시에 IF-1을 쓸 수 있다. (a)는 IF-0을읽는 동시에 쓰여지는 IF-1의 TDI FEC 블록들과 TDI 메모리 인덱스들을 나타낸다. 상술한 바와 같이 라이팅 오퍼레이션은 방송 신호 수신 장치에서 수행된 다이아고날 리딩 오퍼레이션의 방향과는 반대 방향으로 이루어질 수 있다.(b)는 IF-1의 쓰기에 따른 아웃풋TDI 메모리 인덱스들을 나타낸다. 이 경우, 저장된 IF-1내의 FEC 블록들의 배열은 방송 신호 송신 장치의 TI 메모리에 저장된 FEC 블록들의 배열과 다를 수 있다. 즉, 방송 신호 송신 장치에서 수행한 라이팅 오퍼레이션 및 리딩 오퍼레이션의 역 과정은 싱글 메모리의 경우 동일하게 적용할 수 없는 경우가 발생할 수 있다.도 29는 본 발명의 일 실시예에 따른 라이팅 오퍼레이션을 나타낸다. 상술한 바와 같이, 방송 신호 송신 장치에서 수행한 라이팅 오퍼레이션 및 리딩오퍼레이션의 역 과정은 싱글 메모리의 경우 동일하게 적용할 수 없는 경우를 방지하기 위해서 본 발명에서는 TI 메모리에 매트릭스 형태로 FEC 블록을 쓰는 방법을 제안한다.도 29에 도시된 라이팅 오퍼레이션은 상술한 본 발명의 일 실시예에 따른 타임 인터리빙 및 타임 디인터리빙 모두에 동일하게 적용될 수 있다.(a)는 벡터 형태로 FEC 블록들의 셀들을 메모리에 쓰는 경우를 나타낸다. 이는 상술한 라이팅 오퍼레이션과 동일하다.(b)는 매트릭스 형태로 FEC 블록들의 셀들을 메모리에 쓰는 경우를 나타낸다. 즉, 각 FEC 블록들은 m by n의 형태의 매트릭스 형태로 쓰여질 수 있다. 이 경우 매트릭스의 크기는 설계자의 의도에 따라 변경 가능하며, 방송 신호 송신 장치에서 수행한 라이팅 오퍼레이션 및 리딩 오퍼레이션의 역 과정을 방송 신호 수신 장치가 싱글 메모리의 경우에도 동일하게 적용할 수 있다는 장점이 있다.도 30은 본 발명의 일 실시예에 따른 TDI 메모리 인덱스 생성 과정을 나타낸 순서도 이다.상술한 바와 같이 본 발명의 일 실시예에 따른 타임 디인터리버는 순차적으로 입력되는 FEC 블록들에 대하여 TI 아웃풋 메모리 인덱스 값들을 순차적으로 발생시켜 다이아고날-타입 TI를 수행할 수 있다.도 30에 도시된 바와 같이, 본 발명의 일 실시예에 따른 방송 신호 수신 장치는 초기값들(initial values)을 설정할 수 있다(S30000). 즉, 본 발명의 일 실시예에 따른 방송 신호 수신 장치는 첫번째 IF를 위한 타임 디인터리빙 전에, TI 룰을 이용하여 타임 인터리빙 과정 동안에 스킵되었던 메모리 인덱스에 대응하는 셀 값들을 제로 또는 초기값으로 설정할 수 있다.이후, 본 발명의 일 실시예에 따른 방송 신호 수신 장치는 타임 디인터리빙 과정에 사용될 다이아고날 슬로프를 계산할 수 있다(S30100).이후, 본 발명의 일 실시예에 따른 방송 신호 수신 장치는 일시적인 TI 메모리-인덱스(Temporal TI memory-index)를 생성할 수 있다(S30200). 본 발명의 일 실시예에 따른 방송 신호 수신 장치는 입력되는 첫 번째 IF에 대해 송신단의 TI의 첫번째 역 과정으로서 다이아고날 라이팅 오퍼레이션을 수행할 수 있다. 이후, 본 발명의 일 실시예에 따른 방송 신호 송신 장치는 생성된 TI 메모리-인덱스의 가용성(availability)을 평가(evaluate)할 수 있다(S30300). 이후, 본 발명의 일 실시예에 따른 방송 신호 송신 장치는 최종 TI 메모리-인덱스를 생성할 수 있다(S30400).도 30에 도시된 순서도는 도 27 내지 29에서 설명한 TDI 아웃풋 메모리 인덱스를 생성하는 과정에 대응되며, 설계자의 의도에 따라 변경 가능하다.도 31은 본 발명의 일 실시예에 따른 IF-by-IF TI 패턴 변화(Pattern Variation)을 나타낸다. 상술한 바와 같이 본 발명의 일 실시예에 따른 방송 신호 송신 장치 (또는 타임 인터리버)는 수퍼 프레임 단위 또는 IF 단위로 다이아고날 슬로프를 다르게 적용할 수 있다.도 31은 각 IF에 다이아고날 슬로프를 다르게 적용하여 TI 패턴을 변화시키는 실시예로서, IF에 포함된 FEC 블록들의 개수가 짝수인 경우 및 홀수인 경우에 따라 각 IF에 다이아고날 슬로프를 다르게 적용하기 위한 실시예를 나타낸다. FEC 블록들의 개수가 짝수인 경우, 인터리빙 뎁스 를 감소시키는 다이아고날 슬로프가 존재할 수 있기 때문이다.도 31에 도시된 실시예는 하나의 수퍼 프레임 내에 포함된 IF의 개수가 6이고, 각 IF에 포함된 FEC 블록의 길이인 Nr 값이 11인 경우로서, 다이아고날 슬로프는 FEC 블록들의 개수가 7일 때 적용되도록 결정된 경우의 실시예를 나타낸다.(a)는 IF에 포함된 FEC 블록들의 개수가 홀수 즉, 7인 경우의 실시예로서, 본 발명의 일 실시예에 따른 타임 인터리버는 6개의 IF들에 대하여 도 17에서 설명한 다이아고날 슬로프들을 중복되지 않도록 랜덤하게 선택하여 적용할 수 있다.(b)는 각 IF에 포함된 FEC 블록들의 개수가 짝수 즉, 6인 경우의 실시예로서, 도 17에서 설명한 다이아고날 슬로프의 값들은 FEC 블록의 개수가 7일 때 적용되도록 설정된 경우의 실시예를 나타낸다. 이 경우, 본 발명의 일 실시예에 따른 타임 인터리버는 각 IF가 7개의 FEC 블록들을 포함하고 있다고 가정하고, 즉, 상술한 버츄얼 FEC 블록을 추가하고, 임의의 다이아고날 슬로프를적용하여 다이아고날 리딩 오퍼레이션을 수행할 수 있다. 이 경우, 상술한 바와 같이, 버츄얼 FEC 블록의 셀들은 스킵 오퍼레이션을 통해 무시 또는 스킵될 수 있다.본 발명의 일 실시예에 따른 방송 신호 송신 장치는 하나의 수퍼 프레임 내에 가장 많은 FEC 블록을 가지고 있는 IF를 선택하여, Nc값을 결정할 수 있다. Nc를 결정하는 과정은 상술한 수학식 3과 같다.이후, 본 발명의 일 실시예에 따른 방송 신호 송신 장치는 결정된 Nc 값이 짝수 인지 홀수인지 판단하고 짝수인 경우, 상술한 바와 같이 버츄얼 FEC 블록을 추가할 수 있다. 이하의 수학식은 Nc 값이 짝수인 경우, 버츄얼 FEC 블록을 추가하여 홀수로 만드는 과정을 나타낸다.이후 본 발명의 일 실시예에 따른 방송 신호 송신 장치는 다이아고날 슬로프들을 다양한 방법에 따라 순차 또는 랜덤하게 발생시킬 수 있다. 이하의 수학식은 QP (Quadratic Polynomial)방식을 사용하여 각 IF에 사용될 다이아고날 슬로프를 생성하는 과정을 나타낸다.QP 방식은 본 발명의 일 실시예에 해당하며, PP (Primitive Polynomial) 방식으로 대체될 수 있다. 이는 설계자의 의도에 따라 변경가능하다.다음의 수학식은 다이아고날 슬로프를 순차적으로 발생시키는 과정을 나타낸다.이후 본 발명의 일 실시예에 따른 방송 신호 송신 장치는 수학식 3 및 수학식 6 내지 8의 과정으로 생성된 변수들을 고려하여 타임 인터리빙을 수행할 수 있다. 이 경우, 본 발명의 일 실시예에 따른 방송 신호 송신 장치의 TI 아웃풋 메모리 인덱스를 생성하는 과정은 상술한 수학식 4로 표현될 수 있다. 상술한 수학식 4는 수학식 7 내지 8에 의해 생성된 다이아고날 슬로프를 주요 변수로 포함할 수 있다. 또한 수학식 4에서 설명한 스킵 오퍼레이션은 Nc의 길이가 짝수이던지 홀수이던지 관계없이 적용될 수 있다.본 발명의 일 실시예에 따른 방송 신호 수신 장치는 상술한 방송 신호 송신 장치에 대응하여 타임 디인터리빙을 수행할 수 있다. 이 경우, 본 발명의 일 실시예에 따른 방송 신호 수신 장치의 TDI 아웃풋 메모리 인덱스를 생성하는 과정은 상술한 수학식 5로 표현될 수 있다. 수학식 5는 수학식 7 내지 8로 표현된 생성 과정에 의해 생성된 다이아고날 슬로프를 주요 변수로서 포함할 수 있다. 또한 수학식 5에서 설명한 스킵 오퍼레이션은 Nc의 길이가 짝수이던지 홀수이던지 관계없이 적용될 수 있다.또한 상술한 바와 같이 TI 패턴과 관련된 정보는 상술한 스태틱 PLS 시그널링데이터를 통해 전송될 수 있다. TI 패턴 변경여부에 대한 정보는 TI_Var로 표현될 수 있으며 1비트의 크기를 가질 수 있다. TI_Var의 값이 0인 경우, TI 패턴의 변화가 없음을 의미한다. 따라서 본 발명의 일 실시예에 따른 방송 신호 수신기는 디폴트 (default) 값으로서 변수 ST 값을 1로 결정할 수 있다. TI_Var의 값이 1인 경우, TI 패턴의 변화가 있음을 의미한다. 이 경우, 본 발명의 일 실시예에 따른 방송 신호 수신기는 변수 ST 값을 ST,j로 결정할 수 있다.이하에서는 본 발명의 일 실시예에 따른 프리퀀시 인터리빙 과정에 대해 설명한다.상술한 블록 인터리버(6200)는 신호 프레임의 단위가 되는 전송 블록내의 셀들을 인터리빙하여 추가적인 다이버시티 게인을 획득할 수 있다. 본 발명의 일 실시예에 따른 블록 인터리버(6200)는 프리퀀시 인터리버라고 호칭할 수 있으며 이는 설계자의 의도에 따라 변경 가능하다. 또한 블록 인터리버 (6200)는 상술한 페어 와이즈 샐 매핑이 수행된 경우, 입력 셀들에 대해서 연속된 두 개의 셀들을 하나의 단위로 처리하여 인터리빙을 수행할 수 있다. 이를 페어 와이즈 인터리빙이라 호칭할 수 있다. 따라서 블록 인터리버 (6200)는 두 개의 연속된 셀들의 단위로 출력할 수 있다. 이 경우 블록 인터리버(6200)은 두 개의 안테나 경로에 대해서 동일하게 동작하거나 혹은 독립적으로 동작할 수 있다.본 발명에서는 블록 인터리버(6200)에서 수행되는 프리퀀시 인터리빙의 다른 실시예로서, 심볼 바이 심볼 프리퀀시 인터리빙 (symbol by symbol frequency interleaving)을 제안한다. 본 발명에서는 심볼 바이 심볼 프리퀀시 인터리빙 을 인터리빙 또는 콰지-랜덤 프리퀀서 인터리빙 (quasi-random frequency interleaving)이라고 호칭할 수 있다. 상술한 페어 와이즈 인터리빙과 달리, 본 발명의 일 실시예에 따른 프리퀀시 인터리빙은 하나의 OFDM 심볼을 서로 다른 인터리빙 방식을 적용하기 위한 시드(seed) 또는 모 인터리빙 시드 (mother interleaving seed)를 설정할 수 있다. 모 인터리빙 시드 (mother interleaving seed)는 매 OFDM 심볼에 적용될 수 있는 인터리빙  패턴 (또는 시퀀스)을 생성하기 위한 것으로, 매 OFDM 심볼에 다르게 적용되는 인터리빙 패턴은 모 인터리빙 시드의 패턴을 사이클릭-시프팅 (cyclic-shifting)함으로써 생성될 수 있다. 모 인터리빙 시드 (mother interleaving seed)는 후술할 발명에서는 RPI (Relative Prime Interleaving) 방식을 통해 결정될 수 있다. 이를 통해 페어 와이즈 인터리빙보다 증가된 프리퀀시 다이버시티를 획득할 수 있다.상술한 프리퀀시 인터리빙을 구현하기 위하여 본 발명에서는 RPI 방식을 이용하여 모 인터리빙 시드 (mother interleaving seed)를 결정하는 방법을 일 실시예로 할 수 있다. 이 경우, 상술한 프리퀀시 인터리빙을 수행하기 위하여, 본 발명의 일 실시예에 따른 모 (mother) RPI의 이니셜-오프셋 밸류 (initial-offset value)는 랜덤하게 결정될 수 있다. 또한, 이니셜-오프셋 밸류는 QP (Quadratic Polynomial) 또는 PP (Primitive Polynomial) 방식을 통해 생성될 수 있다. 본 발명의 일 실시예에 따른 프리퀀시 인터리빙은 타임 도메인 쪽의 랜덤 특성 (random property)을 보존하면서 주파수 도메인 쪽의 주기적인 특성을 보존할 수 있다. 또한, 본 발명의 일 실시예에 따른 프리퀀시 인터리빙은 수신기 측면에서 싱글 메모리를 사용한 (페어-와이즈) 디인터리빙을 제공할 수 있다.본 발명의 일 실시예에 따른 프리퀀시 인터리빙은 싱글 메모리를 갖는 방송 신호 수신 장치를 위해 다음과 같은 특징을 가질 수 있다.본 발명의 일 실시예에 따른 모 인터리빙 (mother interleaving)은 이니셜-오프셋 밸류를 이용한 RPI를 실행하여 설계될 수 있다.이하의 수학식은 본 발명의 일 실시예에 따른 RPI를 나타낸다.또한 본 발명의 일 실시예에 따른 이니셜-오프셋 밸류는 랜덤 제너레이터(random generator)를 실행함으로서 랜덤하게 생성될 수 있다. 또한, 이니셜-오프셋 밸류는 QP 또는 PP 방식을 통해 생성될 수 있다. PP 방식의 경우 본 발명의 일 실시예에 따른 랜덤 제너레이터는 PRBS (primitive random binary sequence) 제너레이터, PN (pseudo-random noise) 제너레이터 중 어느 하나가 될 수 있다.이하의 수학식은 QP 방식을 사용하는 경우의 이니셜-오프셋밸류를 생성하는 과정을 나타낸다.도 32는 본 발명의 일 실시예에 따른 랜덤 제너레이터의 구조를 나타낸 도면이다. 도 32에 도시된 랜덤 제너레이터는 PP 방식을 이용하여 이니셜-오프셋 밸류를 생성하는 경우를 나타낸다.본 발명의 일 실시예에 따른 랜덤 제너레이터는 레지스터(register)(32000) 및 XOR 연산기(32100)을 포함할 수 있다. 일반적으로 PP 방식은 랜덤하게 1,…, 2n-1 값을 출력할 수 있다. 따라서 본 발명의 일 실시예에 따른 랜덤 제너레이터는 0을 포함한 2n 심볼 인덱스 출력을 하기 위하여 레지스터 리셋 (register reset) 과정을 수행하고, 레지스터 시프팅 (register shifting) 과정을 위한 레지스터 초기값을 설정할 수 있다.또한 본 발명의 일 실시예에 따른 렌덤 제너레이터는 PP 방식을 위한 PP (primitive polynomial)들마다 다른 레지스터 및 XOR 구성을 포함할 수 있다.이하의 표는 상술한 PP 방식을 위한 PP (primitive polynomial)들 및 레지스터 리셋 (register reset) 과정 및 레지스터 시프팅 (register shifting) 과정을 위한 레지스터 밸류및 이니셜 밸류 (initial value, 초기값)를 나타낸다.Order (n)Primitive polynomialk=0 (reset value)k=1 (initial value) 9f(x)=1+x5+x9[0 0 0 0 0 0 0 0 0][0 0 0 0 1 0 0 0 1]10f(x)=1+x7+x10[0 0 0 0 0 0 0 0 0 0][0 0 0 0 0 0 1 0 0 1]11f(x)=1+x9+x11[0 0 0 0 0 0 0 0 0 0  0][0 0 0 0 0 0 0 0 1 0 1]12f(x)=1+x6+x8+x11+x12[0 0 0 0 0 0 0 0 0 0 0 0][0 0 0 0 0 1 0 1 0 0  1 1]13f(x)=1+x2+x4+x8+x9+x12+x13[0 0 0 0 0 0 0 0 0 0  0 0 0][0 1 0 1 0 0 0 1 1 0  0 1 1]14f(x)=1+x2+x12+x13+x14[0 0 0 0 0 0 0 0 0 0  0 0 0 0][0 0 1 0 0 0 0 0 0 0  0 1 1 1]15f(x)=1+x14+x15[0 0 0 0 0 0 0 0 0 0  0 0 0 0 0][0 0 0 0 0 0 0 0 0 0  0 0 0 1 1]표 1은 n 번째 PP (n=9,…,15) 각각에 해당하는 레지스터 리셋 밸류 및 레지스터 이니셜 밸류를 나타내고 있다. 표에 도시된 바와 같이 k=0인 경우, 레지스터 리셋 밸류 를 의미하며, k=1인 경우, 레지스터 이니셜 밸류를 의미한다. 또한, 2≤k≤2n-1 인 경우, 시프팅된 레지스터 밸류들을 의미한다.도 33은 본 발명의 다른 실시예에 따른 랜덤 제너레이터를 나타낸다.도 33은 상술한 표의 n 번째 PP의 n이 9부터 12인 경우의 랜덤 제너레이터의 구성을 나타낸다.도 34는 본 발명의 또 다른 실시예에 따른 랜덤 제너레이터를 나타낸다.도 34는 상술한 표의 n 번째 PP의 n이 13부터 15인 경우의 랜덤 제너레이터의 구성을 나타낸다.도 35는 본 발명의 일 실시예에 따른 프리퀀시 인터리빙 과정을 나타낸 도면이다.도 35는 본 발명의 일 실시예에 따른 방송 신호 수신 장치에 싱글 메모리가 적용된 경우, 전체 심볼 개수가 10이고, 한 개의 심볼을 구성하는 셀들의 개수가 10이며, p는 3인 경우의 프리퀀시 인터리빙 과정을 나타낸다.(a)는 RPI 방식을 적용하여 출력되는 매심볼들의 출력값을 나타낸다. 특히 매 OFDM 심볼의 첫 메모리 인덱스 값, 즉, 0, 7, 4, 1, 8…등은 상술한 RPI의 랜덤 제너레이터에서 생성된 이니셜-오프셋 밸류로 설정될 수 있다. 인터리빙 메모리 인덱스에 표시된 숫자는 각 심볼에 포함된 셀들이 인터리빙되어 출력되는 순서를 나타낸다. (b)는 생성된 인터리빙 메모리 인덱스를 이용하여 입력된 OFDM 심볼의 셀들을 심볼 단위로 인터리빙한 결과를 나타낸다.도 36은 본 발명의 일 실시예에 따른 프리퀀시 디인터리빙 과정을 나타낸 개념도이다.도 36은 방송 신호 수신 장치에 싱글 메모리가 적용된 경우의 프리퀀시 디인터리빙 과정을 도시한 도면으로, 한 개의 심볼을 구성하는 셀들의 개수가 10인 경우의 실시예를 나타낸다.본 발명의 일 실시예에 따른 방송 신호 수신 장치 (또는 프레임 파싱 모듈 또는 블록 디인터리버)는 상술한 프리퀀시 인터리빙 방식에 따라 인터리빙된 심볼들을 입력 순서대로 쓰는 과정(writing)을 통해 디인터리빙 메모리 인덱스에 생성하고, 다시 읽는 과정(reading)을 통해 디인터리빙된 심볼들을 출력할 수 있다. 이 경우, 본 발명의 일 실시예에 따른 방송 신호 수신 장치는 읽는 과정을 수행한 디인터리빙 메모리 인덱스에 쓰는 과정을 수행할 수 있다.도 37은 본 발명의 일 실시예에 따른 프리퀀시 디인터리빙 과정을 나타낸 도면이다.도 37은 전체 심볼 개수가 10이고, 한 개의 심볼을 구성하는 셀들의 개수가 10이며, p는 3인 경우의 디인터리빙 과정을 나타낸다.도 37의 (a)는 본 발명의 일 실시예에 따라 싱글 메모리에 인풋 되는 심볼들을 도시한 도면이다. 즉, 도면에 도시된 싱글-메모리 인풋 심볼들은 매 입력 심볼에 따라 싱글-메모리에 저장된 값들을 나타낸다. 이 경우, 매 입력 심볼 마다 싱글 메모리에 저장된 값들은 이전 심볼에 대해 디인터리빙(reading)을 수행하면서 현재 입력되는 심볼의 셀들을 순차적으로 쓴 값들의 결과를 나타낸다.도 37의 (b)는 디인터리빙 메모리 인덱스를 생성하는 과정을 나타낸 도면이다.디인터리빙 메모리 인덱스는 싱글 메모리에 저장된 값들을 디인터리빙하기 위해 사용되는 인덱스로서, 디인터리빙 메모리 인덱스에 표시된 숫자는 각 심볼에 포함된 셀들이 디인터리빙되어 출력되는 순서를 나타낸다.이하에서는 상술한 프리퀀시 디인터리빙 과정을 도면에 도시된 심볼들 중 #0 및 1 입력 심볼들을 중심으로 설명한다.본 발명의 일 실시예에 따른 방송 신호 수신 장치는 #0 입력 심볼을 싱글 메모리에 순차적으로 쓰는 과정을 수행한다. 이후 본 발명의 일 실시예에 따른 방송 신호 수신 장치는 #0 입력 심볼을 디인터리빙하기 위하여 상술한 디인터리빙 메모리 인덱스를 0, 3, 6, 9..의 순으로 순차적으로 생성할 수 있다. 이후, 본 발명의 일 실시예에 따른 방송 신호 수신 장치는 생성된 디인터리빙 메모리 인덱스에 따라 싱글 메모리에 쓰여진 (또는 저장된) #0 입력 심볼을 읽는 과정을 수행한다. 이미 읽힌 값들은 저장할 필요가 없기 때문에 새롭게 입력되는 #1 심볼을 다시 순차적으로 쓸 수 있다.이후 #0 입력 심볼에 대해 읽는 과정과 #1 입력 심볼에 대해 쓰는 과정이 모두 완료되면, 쓰여진 #1 입력 심볼을 디인터리빙하기 위하여 디인터리빙 메모리 인덱스를 생성할 수 있다. 이 경우, 본 발명의 일 실시예에 따른 방송 신호 수신 장치는 싱글 메모리를 사용하므로 방송 신호 송신 장치에서 적용한 매 심볼에 적용된 인터리빙 패턴을 사용하여 인터리빙을 수행할 수는 없다. 이후 입력되는 심볼들은 동일한 방식으로 디인터리빙될 수 있다.도 38은 본 발명의 일 실시예에 따른 디인터리빙 메모리 인덱스 생성 과정을 나타낸 도면이다.특히 도 38은 본 발명의 일 실시예에 따른 방송 신호 수신 장치가 싱글 메모리를 사용하므로 방송 신호 송신 장치에서 적용한 매 심볼에 적용된 인터리빙 패턴 (interleaving pattern)을 사용하여 인터리빙을 수행할 수는 없는 경우의 새로운 인터리빙 패턴을 생성하는 방법을 도시하고 있다.(a)는 j 번째 입력 심볼의 디인터리빙 메모리 인덱스를 나타내며, (b)는 상술한 디인터리빙 메모리 인덱스의 생성과정을 수학식과 함께 나타낸 도면이다.(b)에 도시된 바와 같이 각 입력 심볼의 RPI 변수를 사용하는 것을 일 실시예로 할 수 있다.#0 입력 심볼의 디인터리빙 메모리 인덱스 생성과정은 방송 신호 송신 장치에서와 동일하게 RPI의 변수로서 p=3, I0=0을 사용하는 것을 일 실시예로 할 수 있다. #1 입력 심볼의 경우, RPI의 변수로서 p2=3x3, I0=1을 사용할 수 있으며, #2 입력 심볼의 경우, RPI의 변수로서 p3=3x3x3, I0=7을 사용하는 것을 일 실시예로 할 수 있다. 또한 #3 입력 심볼의 경우, RPI의 변수로서 p4=3x3x3x3, I0=4을 사용하는 것을 일 실시예로 할 수 있다.즉, 본 발명의 일 실시예에 따른 방송 신호 수신 장치는 매 싱글 메모리에 저장되는 심볼들을 디인터리빙 하기 위하여, RPI의 p 값과 이니셜 오프셋 값을 매 심볼마다 변경하여 효과적으로 디인터리빙을 수행할 수 있다. 또한, 매 심볼에 사용되는 p 값은 p의 지수승으로 쉽게 도출할 수 있으며, 이니셜 오프셋 값들은 모 인터리빙 시드를 이용하여 순차적으로 획득할 수 있다. 이하 이니셜 오프셋 값을 도출하는 방법을 설명한다.#0 입력 심볼에서 사용되는 이니셜 오프셋 값은 I0=0으로 정의하는 것을 일 실시예로 할 수 있다. #1 입력 심볼에서 사용되는 이니셜 오프셋 값은 I0=1이며, 이 값은 #0 입력 심볼에 대한 디인터리빙 메모리 인덱스 생성 과정에서 일곱 번째에 발생된 값과 동일하다. 즉, 본 발명의 일 실시예에 따른 방송 신호 수신 장치는 #0 입력 심볼에 대한 디인터리빙 메모리 인덱스를 생성하는 과정에서 상기 값을 저장하여 사용할 수 있다. #2 입력 심볼에서 사용되는 이니셜 오프셋 값은 I0=7이고, 이 값은 #1 입력 심볼에 대한 디인터리빙 메모리 인덱스 생성과정에서 네 번째에 발생된 값과 동일하며, #3 입력 심볼에서 사용되는 이니셜 오프셋 값은 I0=4이고, 이 값은 #2 입력 심볼에 대한 디인터리빙 메모리 인덱스 생성 과정에서 첫 번째에 발생된 값과 동일하다.따라서 본 발명의 일 실시예에 따른 방송 신호 수신 장치는 각 심볼에 사용될 이니셜 오프셋 값에 해당하는 값을 이전 심볼의 디인터리빙 메모리 인덱스를 생성하는 과정에서 저장하고 사용할 수 있다.결과적으로, 상술한 방법은 다음의 수학식으로 표현될 수 있다. 이 경우, 각 이니셜 오프셋 값에 해당 하는 값의 위치는 상술한 수학식으로부터 쉽게 유도될 수 있다.본 발명의 일 실시예에 따른 방송 신호 송신 장치는 인접한 두개의 셀들을 하나의 셀로 인식하여 프리퀀시 인터리빙을 수행하는 것을 일 실시예로 할 수 있다. 이를 pair-wise interleaing이라 호칭할 수 있다. 이 경우, 2개의 인접한 셀들을 하나의 셀로 간주하여 인터리빙을 수행하므로 메모리 인덱스를 발생시키는 횟수가 반으로 줄어든다는 장점이 있다.이하의 수학식은 pair-wise RPI를 나타낸다.이하의 수학식은 페어-와이즈 디인터리빙 (pair-wise deinterleaving) 방법을 나타낸다.도 39는 본 발명의 다른 실시예에 따른 프리퀀시 인터리빙 과정을 나타낸다.도 39는 상술한 프리퀀시 인터리버가 다수의 OFDM 심볼로 구성된 수퍼 프레임 간에 서로 다른 관련된 프라임 값 (relative prime value)을 이용하여 프리퀀시 다이버시티 (frequency diversity) 성능을 보다 향상 시키기 위한 인터리빙 방법을 나타낸다.즉, 도 39에 도시된 바와 같이, 각 프레임 또는 수퍼 프레임의 관련된 프라임 값은 반복된 인터리빙 패턴을 피하고, 프리퀀시 다이버시티 퍼포먼스를 향상시키기 위하여 변화될 수 있다.도 40은 본 발명의 일 실시예에 따른 방송 신호 송신 방법의 플로우 차트이다.본 발명의 일 실시예에 따른 방송 신호 송신 장치는 복수의 DP들을 통해 전송되는 DP 데이터를 FEC 인코딩할 수 있다(S40000). 상술한 바와 같이 각 DP는 적어도 하나 이상의 서비스 또는 적어도 하나 이상의 서비스 컴포넌트를 전송할 수 있다. 구체적인 인코딩 방법은 상술한 바와 같다.이후, 본 발명의 일 실시예에 따른 방송 신호 송신 장치는 인코딩된 DP 데이터를 성상도에 매핑할 수 있다 (S40100). 구체적인 맵핑 방법은 상술한 바와 같다.이후, 본 발명의 일 실시예에 따른 방송 신호 송신 장치는 인코딩된 DP 데이터를DP 레벨에서 타임 인터리빙 할 수 있다(S40200). 이 경우, 본 발명의 일 실시예에 따른 타임 인터리버는 서로 다른 입력 FEC 블록들을 주어진 메모리에 순차적으로 배열 (writing operation)한 후 다이아고날 방향으로 인터리빙하는 과정(diagonal reading operation)을 포함하는 타임 인터리빙을 수행할 수 있다. 특히 본 발명의 일 실시예에 따른 타임 인터리버는 서로 다른 FEC 블록들을 다이아고날 방향으로 읽을 때, 읽는 방향의 다이아고날 슬로프의 크기를 변경하여 타임 인터리빙을 수행할 수 있다. 즉, 본 발명의 일 실시예에 따른 타임 인터리버는 TI 리딩 패턴 또는 다이아고날 리딩 패턴 (reading pattern, diagonal reading pattern)을 변경할 수 있다. 이 경우, 본 발명의 일 실시예에 따른 다이아고날 리딩 패턴은 FEC 블록의 최대 크기에 따라 결정될 수 있으며, 수퍼 프레임 단위로 변경 될 수 있다.  구체적인 타임 인터리빙 방법은 도 16 내지 도 26 및 도 31에서 설명한 바와 같다. 이후, 본 발명의 일 실시예에 따른 방송 신호 송신 장치는 인터리빙된 DP 데이터를 포함하는 적어도 하나 이상의 신호 프레임을 생성할 수 있다(S40300). 구체적인 내용은 상술한 바와 같다.이후 본 발명의 일 실시예에 따른 방송 신호 송신 장치는 생성된 적어도 하나 이상의 신호 프레임을 OFDM 방식으로 변조할 수 있으며(S40400), 변조된 적어도 하나 이상의 신호 프레임을 포함하는 방송 신호를 전송할 수 있다(S40500).도 41은 본 발명의 일 실시예에 따른 방송 신호 수신 방법의 플로우 차트이다.도 41은 도 40에서 설명한 방송 신호 송신 방법의 역과정에 해당한다.본 발명의 일 실시예에 따른 방송 신호 수신 장치는 적어도 하나 이상의 방송 신호를 수신할 수 있다(S41000).이후 본 발명의 일 실시예에 따른 방송 신호 수신 장치는 수신한 적어도 하나 이상의 방송 신호를 OFDM (Othogonal Frequency Division Multiplexing) 방식으로 복조할 수 있다(S41100). 구체적인 과정은 상술한 바와 같다.이후 본 발명의 일 실시예에 따른 방송 신호 수신 장치는 복조된 적어도 하나 이상의 방송 신호로부터 적어도 하나 이상의 신호 프레임을 획득할 수 있다(S41200). 구체적인 내용은 상술한 바와 같다. 이후, 본 발명의 일 실시예에 따른 방송 신호 수신 장치는 파싱된 적어도 하나 이상의 신호 프레임에 포함된 DP데이터를 타임 디인터리빙할 수 있다(S41300). 이 경우, 본 발명의 일 실시예에 따른 방송 신호 수신 장치에 포함된 타임 디인터리버(또는 타임 디인터리버 블록)는 상술한 다이아고날-타입 TI의 역과정을 수행할 수 있다. 즉, 본 발명의 일 실시예에 따른 타임 디인터리버는 다이아고날-타입 TI이 수행되어 전송된 FEC 블록들을 입력받고 TI 메모리에 다이아고날 방향으로 라이팅 오퍼레이션을 수행 후 순차적으로 리딩 오퍼레이션을 수행하여 타임 디인터리빙을 수행할 수 있다. 구체적인 타임 인터리빙 방법은 도 27 내지 도 28 및 도 30에서 설명한 바와 같다. 이후 본 발명의 일 실시예에 따른 방송 신호 수신 장치는 DP 데이터를 디맵핑하고(S41400), 디매핑된 DP 데이터를 디코딩하여 원하는 서비스 또는 서비스 컴포넌트를 획득할 수 있다(S41500). 상술한 바와 같이 각 DP 데이터는 해당 DP 경로를 통해 각각 처리될 수 있으며 구체적인 처리 과정은 상술한 바와 같다.본 발명의 범위를 벗어나지 않는 한도 내에서 본 발명에 대한 다양한 변경 및 변형이 이루어질 수 있음은 당업자에게 자명하다. 따라서, 본 발명은 첨부된 청구항들 및 그와 동등한 것들의 범위 내에서 발생된 본 발명의 변경 및 변형들을 전부 포함할 수 있다.</td>\n",
       "      <td>본 발명의 일 실시예에 따른 방송 신호 송신 장치는 적어도 하나 이상의 서비스 컴포넌트를 전송하는 복수의 DP(Data Pipe)들 각각에 대응하는 DP 데이터를 인코딩하는 인코더, 인코딩된 DP 데이터를 컨스텔레이션들에 매핑하는 매퍼, 매핑된 DP 데이터를 DP 레벨에서 타임 인터리빙하는 타임 인터리버, 타임 인터리빙된 DP 데이터를 포함하는 적어도 하나 이상의 신호 프레임을 생성하는 프레임 빌더, 생성된 적어도 하나 이상의 신호 프레임 내의 데이터를 OFDM (Orthogonal Frequency Division Multiplex) 방식으로 변조하는 변조부 및 변조된 데이터를 포함하는 방송 신호들을 송신하는 송신부를 포함할 수 있다.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_random_elements(raw_datasets[\"train\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lnjDIuQ3IrI-"
   },
   "source": [
    "The metric is an instance of [`datasets.Metric`](https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasets.Metric):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "5o4rUteaIrI_",
    "outputId": "18038ef5-554c-45c5-e00a-133b02ec10f1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Metric(name: \"rouge\", features: {'predictions': Value(dtype='string', id='sequence'), 'references': Value(dtype='string', id='sequence')}, usage: \"\"\"\n",
       "Calculates average rouge scores for a list of hypotheses and references\n",
       "Args:\n",
       "    predictions: list of predictions to score. Each prediction\n",
       "        should be a string with tokens separated by spaces.\n",
       "    references: list of reference for each prediction. Each\n",
       "        reference should be a string with tokens separated by spaces.\n",
       "    rouge_types: A list of rouge types to calculate.\n",
       "        Valid names:\n",
       "        `\"rouge{n}\"` (e.g. `\"rouge1\"`, `\"rouge2\"`) where: {n} is the n-gram based scoring,\n",
       "        `\"rougeL\"`: Longest common subsequence based scoring.\n",
       "        `\"rougeLSum\"`: rougeLsum splits text using `\"\n",
       "\"`.\n",
       "        See details in https://github.com/huggingface/datasets/issues/617\n",
       "    use_stemmer: Bool indicating whether Porter stemmer should be used to strip word suffixes.\n",
       "    use_aggregator: Return aggregates if this is set to True\n",
       "Returns:\n",
       "    rouge1: rouge_1 (precision, recall, f1),\n",
       "    rouge2: rouge_2 (precision, recall, f1),\n",
       "    rougeL: rouge_l (precision, recall, f1),\n",
       "    rougeLsum: rouge_lsum (precision, recall, f1)\n",
       "Examples:\n",
       "\n",
       "    >>> rouge = datasets.load_metric('rouge')\n",
       "    >>> predictions = [\"hello there\", \"general kenobi\"]\n",
       "    >>> references = [\"hello there\", \"general kenobi\"]\n",
       "    >>> results = rouge.compute(predictions=predictions, references=references)\n",
       "    >>> print(list(results.keys()))\n",
       "    ['rouge1', 'rouge2', 'rougeL', 'rougeLsum']\n",
       "    >>> print(results[\"rouge1\"])\n",
       "    AggregateScore(low=Score(precision=1.0, recall=1.0, fmeasure=1.0), mid=Score(precision=1.0, recall=1.0, fmeasure=1.0), high=Score(precision=1.0, recall=1.0, fmeasure=1.0))\n",
       "    >>> print(results[\"rouge1\"].mid.fmeasure)\n",
       "    1.0\n",
       "\"\"\", stored examples: 0)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jAWdqcUBIrJC"
   },
   "source": [
    "You can call its `compute` method with your predictions and labels, which need to be list of decoded strings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "6XN1Rq0aIrJC",
    "outputId": "a4405435-a8a9-41ff-9f79-a13077b587c7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge1': AggregateScore(low=Score(precision=1.0, recall=1.0, fmeasure=1.0), mid=Score(precision=1.0, recall=1.0, fmeasure=1.0), high=Score(precision=1.0, recall=1.0, fmeasure=1.0)),\n",
       " 'rouge2': AggregateScore(low=Score(precision=1.0, recall=1.0, fmeasure=1.0), mid=Score(precision=1.0, recall=1.0, fmeasure=1.0), high=Score(precision=1.0, recall=1.0, fmeasure=1.0)),\n",
       " 'rougeL': AggregateScore(low=Score(precision=1.0, recall=1.0, fmeasure=1.0), mid=Score(precision=1.0, recall=1.0, fmeasure=1.0), high=Score(precision=1.0, recall=1.0, fmeasure=1.0)),\n",
       " 'rougeLsum': AggregateScore(low=Score(precision=1.0, recall=1.0, fmeasure=1.0), mid=Score(precision=1.0, recall=1.0, fmeasure=1.0), high=Score(precision=1.0, recall=1.0, fmeasure=1.0))}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake_preds = [\"hello there\", \"general kenobi\"]\n",
    "fake_labels = [\"hello there\", \"general kenobi\"]\n",
    "metric.compute(predictions=fake_preds, references=fake_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n9qywopnIrJH"
   },
   "source": [
    "## Preprocessing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YVx71GdAIrJH"
   },
   "source": [
    "Before we can feed those texts to our model, we need to preprocess them. This is done by a 🤗 Transformers `Tokenizer` which will (as the name indicates) tokenize the inputs (including converting the tokens to their corresponding IDs in the pretrained vocabulary) and put it in a format the model expects, as well as generate the other inputs that the model requires.\n",
    "\n",
    "To do all of this, we instantiate our tokenizer with the `AutoTokenizer.from_pretrained` method, which will ensure:\n",
    "\n",
    "- we get a tokenizer that corresponds to the model architecture we want to use,\n",
    "- we download the vocabulary used when pretraining this specific checkpoint.\n",
    "\n",
    "That vocabulary will be cached, so it's not downloaded again the next time we run the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "eXNLu_-nIrJI"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/robust/anaconda3/lib/python3.9/site-packages/transformers/convert_slow_tokenizer.py:446: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "    \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vl6IidfdIrJK"
   },
   "source": [
    "By default, the call above will use one of the fast tokenizers (backed by Rust) from the 🤗 Tokenizers library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rowT4iCLIrJK"
   },
   "source": [
    "You can directly call this tokenizer on one sentence or a pair of sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "a5hBlsrHIrJL",
    "outputId": "acdaa98a-a8cd-4a20-89b8-cc26437bbe90"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [10701, 17375, 11507, 9188, 9186, 12715, 12385, 9186, 18454, 9186, 9573, 16575, 21940, 9269, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"Hello, this one sentence!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qo_0B1M2IrJM"
   },
   "source": [
    "Depending on the model you selected, you will see different keys in the dictionary returned by the cell above. They don't matter much for what we're doing here (just know they are required by the model we will instantiate later), you can learn more about them in [this tutorial](https://huggingface.co/transformers/preprocessing.html) if you're interested.\n",
    "\n",
    "Instead of one sentence, we can pass along a list of sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[10701, 17375, 11507, 9188, 9186, 12715, 12385, 9186, 18454, 9186, 9573, 16575, 21940, 9269, 1], [21902, 12385, 30124, 9186, 11295, 40323, 9186, 9573, 16575, 21940, 9187, 1]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer([\"Hello, this one sentence!\", \"This is another sentence.\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To prepare the targets for our model, we need to tokenize them inside the `as_target_tokenizer` context manager. This will make sure the tokenizer uses the special tokens corresponding to the targets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[10701, 17375, 11507, 9188, 9186, 12715, 12385, 9186, 18454, 9186, 9573, 16575, 21940, 9269, 1], [21902, 12385, 30124, 9186, 11295, 40323, 9186, 9573, 16575, 21940, 9187, 1]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/robust/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:3578: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "with tokenizer.as_target_tokenizer():\n",
    "    print(tokenizer([\"Hello, this one sentence!\", \"This is another sentence.\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2C0hcmp9IrJQ"
   },
   "source": [
    "If you are using one of the five T5 checkpoints we have to prefix the inputs with \"summarize:\" (the model can also translate and it needs the prefix to know which task it has to perform)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t5\n"
     ]
    }
   ],
   "source": [
    "if 't5' in model_checkpoint:\n",
    "    prefix = \"summarize: \"\n",
    "    print('t5')\n",
    "else:\n",
    "    prefix = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2C0hcmp9IrJQ"
   },
   "source": [
    "We can then write the function that will preprocess our samples. We just feed them to the `tokenizer` with the argument `truncation=True`. This will ensure that an input longer that what the model selected can handle will be truncated to the maximum length accepted by the model. The padding will be dealt with later on (in a data collator) so we pad examples to the longest length in the batch and not the whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "vc0BSBLIIrJQ"
   },
   "outputs": [],
   "source": [
    "max_input_length = 1024\n",
    "max_target_length = 512\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [prefix + doc for doc in examples[\"original_text\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n",
    "\n",
    "    # Setup the tokenizer for targets\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(examples[\"summary_text\"], max_length=max_target_length, truncation=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0lm8ozrJIrJR"
   },
   "source": [
    "This function works with one or several examples. In the case of several examples, the tokenizer will return a list of lists for each key:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "-b70jh26IrJS",
    "outputId": "acd3a42d-985b-44ee-9daa-af5d944ce1d9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[9186, 29375, 9495, 37419, 29012, 9270, 9720, 10049, 8609, 43291, 9200, 28981, 41936, 27497, 14747, 17057, 10725, 11509, 17170, 40259, 25747, 38052, 42430, 12494, 16411, 35778, 17253, 17881, 9188, 21980, 9194, 9193, 17778, 10582, 9301, 26047, 10075, 11087, 9200, 10390, 10502, 25995, 11304, 12494, 10675, 24975, 31069, 9186, 33066, 9893, 13033, 21733, 10215, 10088, 9188, 10451, 22858, 17309, 9194, 9290, 11770, 9940, 11203, 9191, 9285, 11492, 13810, 9191, 10625, 25197, 10309, 9190, 13001, 13973, 9630, 8413, 9187, 10922, 9421, 15273, 25380, 16034, 9210, 18878, 17005, 11492, 9190, 9234, 24322, 9193, 25380, 9464, 9470, 13810, 9213, 8253, 9805, 8861, 30241, 11576, 10855, 10803, 9420, 8410, 9187, 11770, 9940, 16346, 13810, 9236, 11492, 18510, 9850, 12518, 9536, 9290, 9796, 9206, 9873, 14350, 9206, 9186, 8280, 10600, 11492, 9190, 11420, 11123, 9217, 12542, 8348, 27256, 9279, 12109, 9420, 8418, 24160, 18680, 9285, 36897, 18580, 9284, 8720, 8410, 9187, 9210, 10049, 8609, 25380, 11653, 9236, 13810, 9190, 11980, 9196, 10126, 18189, 15237, 9199, 9992, 10754, 12890, 18136, 9236, 27932, 10199, 10667, 34549, 9192, 9625, 9223, 8253, 20254, 13188, 9191, 12314, 9459, 10369, 16668, 24600, 16002, 17173, 9192, 33784, 32582, 9463, 12532, 16639, 9463, 12109, 9190, 12380, 27481, 8416, 9187, 12750, 9200, 9259, 9445, 9194, 9217, 13810, 9188, 10726, 9200, 17170, 40034, 9194, 9208, 13810, 9188, 13810, 9190, 12492, 7925, 9240, 11308, 28878, 37914, 9228, 8308, 9188, 11492, 13810, 28929, 9992, 10754, 14920, 10344, 10356, 9214, 9223, 8427, 33784, 9644, 40430, 13567, 9640, 9195, 13099, 19566, 9228, 8416, 9187, 9483, 9188, 11492, 13810, 9197, 11770, 9940, 11203, 23090, 9191, 10625, 10309, 9188, 11770, 14458, 16859, 9186, 6834, 10625, 9850, 9188, 11770, 9940, 22125, 12914, 24895, 10168, 19380, 9188, 13810, 9201, 11661, 15373, 9493, 11492, 9190, 11770, 9940, 11033, 13041, 9644, 30578, 10180, 19626, 12109, 9546, 9214, 9223, 8427, 12546, 8410, 9187, 9903, 13810, 11980, 9191, 10997, 27932, 12105, 9212, 11266, 9587, 15912, 9188, 12917, 9424, 9378, 12555, 10526, 13756, 9188, 13810, 9191, 10997, 27932, 11324, 9190, 12812, 9332, 28171, 9208, 9186, 29098, 9262, 14803, 30893, 36010, 33019, 9278, 11492, 13810, 13821, 9191, 9285, 21595, 9464, 24600, 35707, 9188, 11492, 13810, 21354, 12843, 9227, 10224, 10418, 9456, 37192, 9961, 9392, 9330, 33019, 9214, 9579, 9415, 9187, 1], [9186, 29375, 9495, 37419, 29012, 9270, 9357, 9721, 10642, 9196, 9249, 13552, 9193, 9464, 9470, 13810, 9342, 9187, 9255, 9197, 11492, 15659, 12119, 9421, 9234, 24322, 9206, 9850, 12518, 9536, 9290, 9365, 9986, 8141, 32937, 8308, 11492, 9190, 27298, 25849, 9244, 12619, 17537, 14459, 29659, 9470, 19120, 9228, 8416, 9200, 10286, 9721, 10642, 9188, 10099, 10212, 9194, 9187, 10069, 9297, 9921, 10309, 39387, 9270, 9736, 9385, 30691, 14923, 9195, 10451, 22858, 17309, 10088, 10316, 8677, 12109, 9228, 8308, 12948, 9873, 11492, 13810, 25488, 18580, 11492, 9190, 15780, 27357, 9195, 9193, 10126, 11420, 10784, 6416, 15061, 9228, 8416, 9187, 9357, 9721, 10642, 9196, 12650, 9204, 9277, 9187, 12403, 10618, 12575, 9192, 35168, 10346, 15250, 9348, 11862, 15962, 9188, 14013, 19267, 9262, 11770, 9641, 13629, 29693, 9845, 34129, 9862, 9295, 9514, 10424, 24160, 18680, 9285, 40599, 20669, 9192, 18346, 8416, 9200, 10286, 9721, 10642, 9188, 10099, 10212, 9194, 9187, 9357, 9721, 10642, 9189, 33787, 12743, 13438, 9201, 10146, 9351, 9236, 25380, 11770, 9940, 16346, 13810, 9228, 8308, 27932, 16081, 15057, 11980, 19987, 9186, 8199, 9266, 9200, 11065, 14606, 16712, 9188, 13722, 9270, 9267, 10140, 9348, 9474, 9451, 9217, 23161, 11492, 9190, 15780, 30313, 9208, 19384, 9191, 9685, 9579, 9279, 11347, 9244, 9214, 9186, 8199, 9187, 11980, 6430, 13810, 9197, 9903, 15025, 9546, 11033, 9190, 12545, 23964, 37163, 14223, 24212, 28193, 13233, 9834, 9200, 13267, 10502, 13573, 11525, 23698, 10591, 12131, 9186, 36357, 13139, 19574, 9188, 10226, 9270, 9267, 9194, 9187, 1]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'labels': [[9720, 13857, 9196, 43291, 9193, 17778, 10582, 9301, 26047, 10075, 11087, 9645, 11770, 9940, 11203, 9191, 9285, 11492, 13810, 9191, 9285, 11236, 10309, 9190, 13001, 13973, 9243, 8416, 9187, 10922, 9421, 15273, 25380, 16034, 9210, 18878, 17005, 11492, 9190, 9234, 24322, 9193, 25380, 9464, 9470, 13810, 9213, 8253, 9805, 8861, 10661, 11576, 9870, 8623, 9375, 8413, 9187, 1], [9357, 9721, 10642, 9196, 11492, 9190, 27298, 25849, 9244, 12619, 17537, 14459, 29659, 9470, 19120, 9228, 8416, 9187, 9357, 9721, 10642, 9196, 10346, 15250, 9348, 11862, 15962, 9188, 14013, 19267, 9262, 11770, 9641, 13629, 29693, 9845, 34129, 9862, 9295, 9514, 10424, 24160, 18680, 9285, 40599, 20669, 9192, 18346, 8416, 9187, 1]]}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_function(raw_datasets['train'][:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zS-6iXTkIrJT"
   },
   "source": [
    "To apply this function on all the pairs of sentences in our dataset, we just use the `map` method of our `dataset` object we created earlier. This will apply the function on all the elements of all the splits in `dataset`, so our training, validation and testing data will be preprocessed in one single command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "DDtsaJeVIrJT",
    "outputId": "aa4734bf-4ef5-4437-9948-2c16363da719"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e037008a73e24ce5ac38588076cad1d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/566 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "voWiw8C7IrJV"
   },
   "source": [
    "Even better, the results are automatically cached by the 🤗 Datasets library to avoid spending time on this step the next time you run your notebook. The 🤗 Datasets library is normally smart enough to detect when the function you pass to map has changed (and thus requires to not use the cache data). For instance, it will properly detect if you change the task in the first cell and rerun the notebook. 🤗 Datasets warns you when it uses cached files, you can pass `load_from_cache_file=False` in the call to `map` to not use the cached files and force the preprocessing to be applied again.\n",
    "\n",
    "Note that we passed `batched=True` to encode the texts by batches together. This is to leverage the full benefit of the fast tokenizer we loaded earlier, which will use multi-threading to treat the texts in a batch concurrently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "545PP3o8IrJV"
   },
   "source": [
    "## Fine-tuning the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FBiW8UpKIrJW"
   },
   "source": [
    "Now that our data is ready, we can download the pretrained model and fine-tune it. Since our task is of the sequence-to-sequence kind, we use the `AutoModelForSeq2SeqLM` class. Like with the tokenizer, the `from_pretrained` method will download and cache the model for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TlqNaB8jIrJW",
    "outputId": "84916cf3-6e6c-47f3-d081-032ec30a4132"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unrecognized configuration class <class 'transformers.models.big_bird.configuration_big_bird.BigBirdConfig'> for this kind of AutoModel: AutoModelForSeq2SeqLM.\nModel type should be one of BartConfig, BigBirdPegasusConfig, BlenderbotConfig, BlenderbotSmallConfig, EncoderDecoderConfig, FSMTConfig, LEDConfig, LongT5Config, M2M100Config, MarianConfig, MBartConfig, MT5Config, MvpConfig, PegasusConfig, PegasusXConfig, PLBartConfig, ProphetNetConfig, SwitchTransformersConfig, T5Config, XLMProphetNetConfig.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n\u001b[0;32m----> 3\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForSeq2SeqLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_checkpoint\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:467\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    463\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[1;32m    464\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m    465\u001b[0m         pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39mmodel_args, config\u001b[38;5;241m=\u001b[39mconfig, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhub_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    466\u001b[0m     )\n\u001b[0;32m--> 467\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    468\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    469\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    470\u001b[0m )\n",
      "\u001b[0;31mValueError\u001b[0m: Unrecognized configuration class <class 'transformers.models.big_bird.configuration_big_bird.BigBirdConfig'> for this kind of AutoModel: AutoModelForSeq2SeqLM.\nModel type should be one of BartConfig, BigBirdPegasusConfig, BlenderbotConfig, BlenderbotSmallConfig, EncoderDecoderConfig, FSMTConfig, LEDConfig, LongT5Config, M2M100Config, MarianConfig, MBartConfig, MT5Config, MvpConfig, PegasusConfig, PegasusXConfig, PLBartConfig, ProphetNetConfig, SwitchTransformersConfig, T5Config, XLMProphetNetConfig."
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CczA5lJlIrJX"
   },
   "source": [
    "Note that  we don't get a warning like in our classification example. This means we used all the weights of the pretrained model and there is no randomly initialized head in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_N8urzhyIrJY"
   },
   "source": [
    "To instantiate a `Seq2SeqTrainer`, we will need to define three more things. The most important is the [`Seq2SeqTrainingArguments`](https://huggingface.co/transformers/main_classes/trainer.html#transformers.Seq2SeqTrainingArguments), which is a class that contains all the attributes to customize the training. It requires one folder name, which will be used to save the checkpoints of the model, and all other arguments are optional:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    f\"{model_name}-finetuned-paper\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=3,\n",
    "    predict_with_generate=True,\n",
    "    fp16=True,\n",
    "    gradient_checkpointing=True,\n",
    "    gradient_accumulation_steps=8,\n",
    "    push_to_hub=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "km3pGVdTIrJc"
   },
   "source": [
    "Here we set the evaluation to be done at the end of each epoch, tweak the learning rate, use the `batch_size` defined at the top of the cell and customize the weight decay. Since the `Seq2SeqTrainer` will save the model regularly and our dataset is quite large, we tell it to make three saves maximum. Lastly, we use the `predict_with_generate` option (to properly generate summaries) and activate mixed precision training (to go a bit faster).\n",
    "\n",
    "The last argument to setup everything so we can push the model to the [Hub](https://huggingface.co/models) regularly during training. Remove it if you didn't follow the installation steps at the top of the notebook. If you want to save your model locally in a name that is different than the name of the repository it will be pushed, or if you want to push your model under an organization and not your name space, use the `hub_model_id` argument to set the repo name (it needs to be the full name, including your namespace: for instance `\"sgugger/t5-finetuned-xsum\"` or `\"huggingface/t5-finetuned-xsum\"`).\n",
    "\n",
    "Then, we need a special kind of data collator, which will not only pad the inputs to the maximum length in the batch, but also the labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7sZOdRlRIrJd"
   },
   "source": [
    "The last thing to define for our `Seq2SeqTrainer` is how to compute the metrics from the predictions. We need to define a function for this, which will just use the `metric` we loaded earlier, and we have to do a bit of pre-processing to decode the predictions into texts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UmvbnJ9JIrJd"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    # Replace -100 in the labels as we can't decode them.\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    # Rouge expects a newline after each sentence\n",
    "    decoded_preds = [\"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds]\n",
    "    decoded_labels = [\"\\n\".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels]\n",
    "    \n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "    # Extract a few results\n",
    "    result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\n",
    "    \n",
    "    # Add mean generated length\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    \n",
    "    return {k: round(v, 4) for k, v in result.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rXuFTAzDIrJe"
   },
   "source": [
    "Then we just need to pass all of this along with our datasets to the `Seq2SeqTrainer`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "imY1oC3SIrJf"
   },
   "outputs": [],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CdzABDVcIrJg"
   },
   "source": [
    "We can now finetune our model by just calling the `train` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uNx5pyRlIrJh",
    "outputId": "077e661e-d36c-469b-89b8-7ff7f73541ec"
   },
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now upload the result of the training to the Hub, just execute this instruction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.push_to_hub()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now share this model with all your friends, family, favorite pets: they can all load it with the identifier `\"your-username/the-name-you-picked\"` so for instance:\n",
    "\n",
    "```python\n",
    "from transformers import AutoModelForSeq2SeqLM\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"sgugger/my-awesome-model\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Summarization",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13 (main, Aug 25 2022, 23:26:10) \n[GCC 11.2.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "a65938c2071cd56bf5a6fc327f44f1acfab0f7c43acc4a9a67d518e041005e56"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
