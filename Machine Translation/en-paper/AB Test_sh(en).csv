Original
"In many ways, graphs are the main modality of data we receive from nature. This is due to the fact that most of the patterns we see, both in natural and artificial systems, are elegantly representable using the language of graph structures. Prominent examples include molecules (represented as graphs of atoms and bonds), social networks and transportation networks. This potential has already been seen by key scientific and industrial groups, with alreadyimpacted application areas including traffic forecasting, drug discovery, social network analysis and recommender systems. Further, some of the most successful domains of application for machine learning in previous years—images, text and speech processing—can be seen as special cases of graph representation learning, and consequently there has been significant exchange of information between these areas. The main aim of this short survey is to enable the reader to assimilate the key concepts in the area, and position graph representation learning in a proper context with related fiel"
"In this survey, I will present a vibrant and exciting area of deep learning research: graph representation learning. Or, put simply, building machine learning models over data that lives on graphs (interconnected structures of nodes connected by edges). These models are commonly known as graph neural networks, or GNNs for short. There is very good reason to study data on graphs. From the molecule (a graph of atoms connected by chemical bonds) all the way to the connectomic structure of the brain (a graph of neurons connected by synapses), graphs are a universal language for describing living organisms, at all levels of organisation. Similarly, most relevant artificial constructs of interest to humans, from the transportation network (a graph of intersections connected by roads) to the social network (a graph of users connected by friendship links), are best reasoned about in terms of graphs. This potential has been realised in recent years by both scientific and industrial groups, with GNNs now being used to discover novel potent antibiotics (Stokes et al., 2020), serve estimated travel times in Google Maps (Derrow-Pinion et al., 2021), power content recommendations in Pinterest (Ying et al., 2018) and product recommendations in Amazon (Hao et al., 2020), and design the latest generation of machine learning hardware: the TPUv5 (Mirhoseini et al., 2021). Further, GNNbased systems have helped mathematicians uncover the hidden structure of mathematical objects (Davies et al., 2021), leading to new top-tier conjectures in the area of representation theory (Blundell et al., 2021). It would not be an understatement to say that billions of people are coming into contact with predictions of a GNN, on a day-to-day basis. As such, it is likely a valuable pursuit to study GNNs, even without aiming to directly contribute to their development. Beyond this, it is likely that the very cognition processes driving our reasoning and decision-making are, in some sense, graph-structured. That is, paraphrasing a quote from Forrester (1971), nobody really imagines in their head all the information known to them; rather, they imagine only selected concepts, and relationships between them, and use those to represent the real system. If we subscribe to this interpretation of cognition, it is quite unlikely that we will be able to build a generally intelligent system without some component relying on graph representation learning. Note that this finding does not clash with the fact that many recent skillful ML systems are based on the Transformer architecture (Vaswani et al., 2017)—as we will uncover in this review, Transformers are themselves a special case of GNNs."
"This review does not attempt to be a comprehensive overview of specific GNN layers. That being said: representative convolutional GNNs include the Chebyshev network (Defferrard et al., 2016, ChebyNet), graph convolutional network (Kipf and Welling, 2017, GCN) and the simplified graph convolution (Wu et al., 2019, SGC); representative attentional GNNs include the mixture model CNN (Monti et al., 2017, MoNet), graph attention network (Veliˇckovi´c et al., 2018, GAT) and its recent “v2” variant (Brody et al., 2022, GATv2); and representative message-passing GNNs include interaction networks (Battaglia et al., 2016, IN), message passing neural networks (Gilmer et al., 2017, MPNN) and graph networks (Battaglia et al., 2018, GN). Given such a GNN layer, we can learn (m)any interesting tasks over a graph, by appropriately combining hu. I exemplify the three principal such tasks, grounded in biological examples: Node classification. If the aim is to predict targets for each node u ∈ V, then our output is equivariant, and we can learn a shared classifier directly on hu. A canonical example of this is classifying protein functions (e.g. using gene ontology data (Zitnik and Leskovec, 2017)) in a given protein-protein interaction network, as first done by GraphSAGE (Hamilton et al., 2017). Graph classification. If we want to predict targets for the entire graph, then we want an invariant output, hence need to first reduce all the hu into a common representation, e.g. by performing L u∈V hu, then learning a classifier over the resulting flat vector. A canonical example is classifying molecules for their quantum-chemical properties (Gilmer et al., 2017), estimating pharmacological properties like toxicity or solubility (Duvenaud et al., 2015; Xiong et al., 2019; Jiang et al., 2021) or virtual drug screening (Stokes et al., 2020). Link prediction. Lastly, we may be interested in predicting properties of edges (u, v), or even predicting whether an edge exists; giving rise to the name “link prediction”. In this case, a classifier can be learnt over the concatenation of features hukhv, along with any given edge-level features. Canonical tasks include predicting links between drugs and diseases—drug repurposing (Morselli Gysi et al., 2021), drugs and targets—binding affinity prediction (Lim et al., 2019; Jiang et al., 2020), or drugs and drugs—predicting adverse side-effects from polypharmacy (Zitnik et al., 2018; Deac et al., 2019). It is possible to use the building blocks from the principal tasks above to go beyond classifying the entities given by the input graph, and have systems that produce novel molecules (Mercado et al., 2021) or even perform retrosynthesis—the estimation of which reactions to utilise to synthesise given molecules (Somnath et al., 2021; Liu et al., 2022)."
"Text-to-image synthesis has recently seen significant progress thanks to large pretrained language models, large-scale training data, and the introduction of scalable model families such as diffusion and autoregressive models. However, the best-performing models require iterative evaluation to generate a single sample. In contrast, generative adversarial networks (GANs) only need a single forward pass. They are thus much faster, but they currently remain far behind the stateof-the-art in large-scale text-to-image synthesis. This paper aims to identify the necessary steps to regain competitiveness. Our proposed model, StyleGAN-T, addresses the specific requirements of large-scale text-to-image synthesis, such as large capacity, stable training on diverse datasets, strong text alignment, and controllable variation vs. text alignment tradeoff. StyleGAN-T significantly improves over previous GANs and outperforms distilled diffusion models— the previous state-of-the-art in fast text-to-image synthesis— in terms of sample quality and speed."
"We choose StyleGAN-XL as our baseline architecture because of its strong performance in class-conditional ImageNet synthesis (Sauer et al., 2022). In this section, we modify this baseline piece by piece, focusing on the generator (Section 3.1), discriminator (Section 3.2), and variation vs. text alignment tradeoff mechanisms (Section 3.3) in turn. Throughout the redesign process, we measure the effect of our changes using zero-shot MS COCO. For practical reasons, the tests use a limited compute budget, smaller models, and a smaller dataset than the large-scale experiments in Section 4; see Appendix A for details. We quantify sample quality using FID (Heusel et al., 2017) and text alignment using CLIP score (Hessel et al., 2021). Following prior art (Balaji et al., 2022), we compute the CLIP score using a ViT-g-14 model trained on LAION-2B (Schuhmann et al., 2022). To change the class conditioning to text conditioning in our baseline model, we embed the text prompts using a pretrained CLIP ViT-L/14 text encoder (Radford et al., 2021) and use them in place of the class embedding. Accordingly, we also remove the training-time classifier guidance. This simple conditioning mechanism matches the early text-toimage models (Reed et al., 2016a;b). As shown in Table 1, this baseline reaches a zero-shot FID of 51.88 and CLIP score of 5.58 in our lightweight training configuration."
"Guiding the text encoder. Interestingly, the earlier methods listed above that use a pretrained generator did not report encountering low-level image artifacts. We hypothesize that the frozen generator acts as a prior that suppresses them. We build on this insight to further improve the text alignment. In our primary training phase, the generator is trainable and the text encoder is frozen. We then introduce a secondary phase, where the generator is frozen and the text encoder becomes trainable instead. We only train the text encoder as far as the generator conditioning is concerned; the discriminator and the guidance term (Eq. 2) still receive ctext from the original frozen encoder. This secondary phase allows a very high CLIP guidance weight of 50 without introducing artifacts and significantly improves text alignment without compromising FID (Section 4.2). Compared to the primary phase, the secondary phase can be much shorter. After convergence, we continue with the primary phase."
"Potential harms of large language models can be mitigated by watermarking model output, i.e., embedding signals into generated text that are invisible to humans but algorithmically detectable from a short span of tokens. We propose a watermarking framework for proprietary language models. The watermark can be embedded with negligible impact on text quality, and can be detected using an efficient open-source algorithm without access to the language model API or parameters. The watermark works by selecting a randomized set of “whitelist” tokens before a word is generated, and then softly promoting use of whitelist tokens during sampling. We propose a statistical test for detecting the watermark with interpretable p-values, and derive an information-theoretic framework for analyzing the sensitivity of the watermark. We test the watermark using a multibillion parameter model from the Open Pretrained Transformer (OPT) family, and discuss robustness and security"
"How hard is it to remove the watermark? The use of the one proportion z-test makes removal of the watermark difficult. Consider the case of a watermarked sequence of length T = 1000. Suppose an adversary modifies 200 tokens in the sequence to add blacklist words and scrub the watermark. A modified token at position t can violate the blacklist rule at position t. Furthermore, the value of st determines the blacklist for token st+1, and a maximally adversarial choice of st will put st+1 in violation of the blacklist rule as well. For this reason, 200 token flips can create at most 400 violations of the blacklist rule. Unfortunately for the attacker, this maximally adversarial sequence with 600 remaining whitelist tokens still produces a z-statistic of 2(600−1000/2)/ √ 1000 ≈ 6.3, and a p-value of ≈ 10−10 , leaving the watermark readily detectable with extremely high confidence. In general, removing the watermark of a long sequence requires modifying roughly one quarter of the tokens or more. Note the analysis above assumes the attacker has complete knowledge of the watermark, and each selected token is maximally adversarial (which likely has a negative impact on quality). Without knowledge of the watermark algorithm, each flipped token has only a 50% chance of being blacklisted, as does the adjacent token. In this case, the attacker above only creates 200 blacklist words (in expectation) by modifying 200 tokens. Methods for keeping the watermark algorithm secret but available via API are discussed in Section 5."
"7.1. Degradation Under Attack: Span Replacement Using a LM We study a realistic black-box attack by attempting to remove the presence of the watermark by replacing spans in the original output text using another language model. We treat the watermark algorithm as if it is private, mocking seclusion behind an API. The attacker does not have access to the locations of whitelisted tokens and instead tries to modify the text through token replacement at random indices until a certain word replacement budget, ε, is reached. The budget constraint maintains a level semantic similarity between the original watermarked text and the attacked text, otherwise the “utility” of the original text for its intended task may be lost. Also, each span replacement in the attack is performed via inference using a multi-million parameter language model, roughly a third the size of the target model, but still with an associated cost per execution that requires a base level of efficiency with respect to model calls. In our experiment, we adopt T5-Large (Raffel et al., 2020) as the replacement model and iteratively select and replace tokens until the attacker either reaches the budget or no more suitable replacement candidates are returned."
"Deep learning has been widely successful in practice and most state-of-the-art machine learning methods are based on neural networks. Lacking, however, is a rigorous mathematical theory that adequately explains the amazing performance of deep neural networks. In this article, we present a relatively new mathematical framework that provides the beginning of a deeper understanding of deep learning. This framework precisely characterizes the functional properties of neural networks that are trained to fit to data. The key mathematical tools which support this framework include transform-domain sparse regularization, the Radon transform of computed tomography, and approximation theory, which are all techniques deeply rooted in signal processing. This framework explains the effect of weight decay regularization in neural network training, the use of skip connections and low-rank weight matrices in network architectures, the role of sparsity in neural networks, and explains why neural networks can perform well in high-dimensional problems."
"Translation Prompt: ChatGPT is essentially a large language model, which needs prompts as guidance to trigger its translation ability. The style of prompts may affect the quality of translation outputs. For example, how to mention the source or target language information matters in multilingual machine translation models, which is usually solved by attaching language tokens (Johnson et al., 2017; Fan et al., 2021). • Multilingual Translation: ChatGPT is a single model handling various NLP tasks and covering different languages, which can be considered a unified multilingual machine translation model. Thus, we are curious about how ChatGPT performs on different language pairs considering both the resource difference (e.g., high vs. low) and language family (e.g., European vs. Asian). • Translation Robustness: ChatGPT is developed upon GPT3, which was trained on largescale datasets that cover various domains. Therefore, we wonder if it can perform robustly well on domain-specific or even noisy sentences."
"Resource Difference. We consider the resource difference of languages in the same family. In machine translation, German⇔English translation is usually regarded as a high-resource task supported by over ten million sentence pairs (Farhad et al., 2021) while Romanian⇔English translation is supported by much less data (Bojar et al., 2016). As shown in Table 4, ChatGPT performs competitively with Google Translate and DeepL Translate for both German⇒English and English⇒German translations. However, it lags behind them significantly on Romanian⇒English and English⇒Romanian. Specifically, ChatGPT obtains a BLEU score on English⇒Romanian that is 46.4% lower than Google Translate and the value is 10.3% on Romanian⇒English. We speculate that the huge resource difference of monolingual data between English and Romanian limits the language modeling capability of Romanian, which partially explains the poor performance on English⇒Romanian. On the contrary, Romanian⇒English can benefit from the strong language modeling capability of English such that the resource gap of parallel data can be somewhat compensated."
