Original
"In conclusion, the performance of the germanium detectors for high-energy γ-rays was
evaluated using the 992-keV resonance in the 27Al(p, γ)
28Si reaction. The energy calibration
was conducted using the energy reference from NNDC. The excitation energy of the level
at 12.5 MeV was adopted to be 12.5407(2) MeV from the residual analysis. The calibration
function has a non-linearity at around 1.5 MeV. By correcting the non-linearity, the energy
accuracy is achieved at 0.3 keV for the overall energy region. The photo-peak efficiency
in the wide energy range was also measured. The extrapolation of the efficiency curve in
the low-energy region overestimates the efficiency measured in the high-energy region. The
evaluation of the non-linearity of the energy calibration and photo-peak efficiencies by the
actual measurement is crucial for accurate spectroscopy of high-energy photons."
"The experiment was performed at the RIKEN tandem accelerator (Pelletron 5SDH-2, 1.7
MV max). The experimental setup is shown in Fig. 1. A 0.8-µm thick aluminum target (20
mmφ
) was irradiated with the proton beam at 1 MeV in the beam duct (1-mm thick, made of
stainless). The proton beam intensity was approximately 300 nA, and the measurement time
was 6.5 hours. Two large volume Ge detectors, GMX80 (Ortec) and GX5019 (Canberra),
were used for γ-ray detection. GMX80 is an n-type 80% coaxial detector, and GX5019 is a
p-type 50% coaxial detector. The distance from the target to the detector surface was 5 cm
(GMX80) and 10 cm (GX5019), respectively. Signals from the detectors were acquired by a
waveform digitizer (Caen V1730B, sampling rate of 500MHz and resolution of 14bit). The
maximum energy of the input dynamic range of the digitizer was set to about 20 MeV."
"We study the effects of three-nucleon short-range correlations on nuclear coordinate-space densities. For this purpose, novel three-body densities are calculated for ground state nuclei using the
auxiliary-field diffusion Monte Carlo method. The results are analyzed in terms of the Generalized
Contact Formalism, extended to include three-body correlations, revealing the universal behavior of
nucleon triplets at short distances. We identify the quantum numbers of such correlated triplets and
extract scaling factors of triplet abundances that can be compared to upcoming inclusive electron
scattering data."
"Short-range correlations (SRCs) are an integral part
of strongly interacting many-body quantum systems, including nuclei and atomic systems. Strong SRCs between nucleons pose one of the main challenges in the
description of nuclei. Accounting for the impact of SRC
physics is crucial for the description of two-body densities
and momentum distributions [1–6], electron and neutrino
scattering [7–11], spectroscopic factors [12–24], neutrinoless double beta decay matrix elements [25–27], neutron
star properties [28–31] and more"
"In order to unitarily evolve a quantum system, an agent requires knowledge of time, a parameter
which no physical clock can ever perfectly characterise. In this letter, we study how limitations on
acquiring knowledge of time impact controlled quantum operations in different paradigms. We show
that the quality of timekeeping an agent has access to limits the gate complexity they are able to
achieve within circuit-based quantum computation. It also exponentially impacts state preparation
for measurement-based quantum computation. Another area where quantum control is relevant is
quantum thermodynamics. In that context, we show that cooling a qubit can be achieved using a
timer of arbitrary quality for control: timekeeping error only impacts the rate of cooling and not the
achievable temperature. Our analysis combines techniques from the study of autonomous quantum
clocks and the theory of quantum channels to understand the effect of imperfect timekeeping on
controlled quantum dynamics."
"In classical computation, bits of information are abundant and their manipulation is simple. This underpins
the great success of the abstract theory of classical information processing, which allows the design of algorithms
without much consideration for the physical semiconductor hardware that would carry them out. Quantum computation is different: whilst any algorithm before measurement can simply be thought of as a rotation on a
high-dimensional Bloch ball, such a unitary operation is
necessarily generated by a time-dependent Hamiltonian.
And an abstraction to perfect qubits is prohibited by the
impossibility of (almost) perfect error correction, which
is far more demanding for quantum information."
"Zero-photon subtraction (ZPS) is a conditional measurement process that can reduce the mean photon number
of quantum optical states without physically removing any photons. Here we show that ZPS can also be used
to transform certain super-Poissonian states into sub-Poissonian states, and vice versa. Combined with a wellknown “no-go” theorem on conditional measurements, this effect leads to a new set of non-classicality criteria
that can be experimentally tested through ZPS measurements."
"The investigation of super-Poissonian and sub-Poissonian
light sources plays a significant role in the history of quantum optics [1, 2]. The photon-number distributions of these
sources display variances that are, respectively, wider or narrower than the benchmark Poissonian statistics of a coherent state with the same average photon number [3]. Experimentally, these properties can be conveniently characterized
by Mandel’s Q parameter, with Q > 0 for super-Poissonian
sources and −1 ≤ Q < 0 for sub-Poissonian sources"
"In this paper, the TF system of two-coupled Black-Scholes equations for pricing the convertible
bonds is solved numerically by using the P1 and P2 finite elements with the inequality constraints
approximated by the penalty method. The corresponding finite element ODE system is numerically
solved by using a modified Crank-Nicolson scheme, in which the non-linear system is solved at each
time step by the Newton-Raphson method for non-smooth functions. Moreover, the corresponding
Greeks are also calculated by taking advantage of the P1-P2 finite element approximation functions.
Numerical solutions by the finite element method compare favorably with the solutions by the finite
difference method in literature"
"Pair trading is one of the most effective statistical arbitrage strategies which seeks a neutral profit by hedging a pair of selected assets.
Existing methods generally decompose the task into two separate
steps: pair selection and trading. However, the decoupling of two
closely related subtasks can block information propagation and
lead to limited overall performance. For pair selection, ignoring the
trading performance results in the wrong assets being selected with
irrelevant price movements, while the agent trained for trading can
overfit to the selected assets without any historical information
of other assets. To address it, in this paper, we propose a paradigm for automatic pair trading as a unified task rather than a
two-step pipeline. We design a hierarchical reinforcement learning
framework to jointly learn and optimize two subtasks. A high-level
policy would select two assets from all possible combinations and
a low-level policy would then perform a series of trading actions.
Experimental results on real-world stock data demonstrate the effectiveness of our method on pair trading compared with both
existing pair selection and trading methods."
"For pair selection, previous methods aim to find two assets whose
prices have moved together historically in a formation period, and
their future spread is assumed to be historical mean-reverted [23].
They generally adopted statistical or fundamental similarity measurements based on historical price information to perform asset
pair selection before trading. The distance approach was first introduced [8, 10, 16, 19, 35, 36] for pair selection, which simply
adopted distance metrics such as the sum of Euclidean squared
distance (SSD) for the price time series to model the connection
between two assets. However, an ideal asset pair in these modelfree methods were expected to be two assets with exactly the same
price movement in historical time, which have zero trading opportunities for no fluctuations of price spread. There were also
methods [5, 7, 12, 15, 27, 38, 39, 46] that directly model the tradability of a candidate pair based on the Engle-Granger cointegration
test, which performs linear regression using the price series of two
assets and expects the residual to be stationary."

