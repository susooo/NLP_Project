,Original,Model,Google,Papago,Bing
0,"In many ways, graphs are the main modality of data we receive from nature. This is due to the fact that most of the patterns we see, both in natural and artificial systems, are elegantly representable using the language of graph structures. Prominent examples include molecules (represented as graphs of atoms and bonds), social networks and transportation networks. This potential has already been seen by key scientific and industrial groups, with alreadyimpacted application areas including traffic forecasting, drug discovery, social network analysis and recommender systems. Further, some of the most successful domains of application for machine learning in previous years—images, text and speech processing—can be seen as special cases of graph representation learning, and consequently there has been significant exchange of information between these areas. The main aim of this short survey is to enable the reader to assimilate the key concepts in the area, and position graph representation learning in a proper context with related fiel","여러모로 그래프는 우리가 자연으로부터 받게 되는 데이터의 주요 모달리티인데, 이는 우리가 보는 패턴이 자연계와 인공계 모두 그래프 구조의 언어를 이용하여 우아하게 표현되고 있기 때문인데, 이 잠재력은 이미 주요 과학계와 산업계에서 응용 분야로 교통 예측, 의약품 발견, 소셜 네트워크 분석 및 추천자 시스템을 포함하고 있으며, 이전 시기의 가장 성공적인 응용 영역들 중 이미지, 텍스트 및 음성 처리의 일부는 그래프 표현 학습의 특수한 사례로 볼 수 있으며, 결과적으로 이들 영역들 간에 상당한 정보 교환이 있었다. 본 짧은 조사의 주요 목적은 독자가 해당 영역의 핵심 개념을 동화시키고, 관련 그래프 표현 학습을 적절한 맥락에서 하도록 하는 것이다.","Translated(src=en, dest=ko, text=여러면에서 그래프는 자연에서받는 데이터의 주요 양식입니다.이것은 자연 및 인공 시스템에서 우리가 보는 대부분의 패턴이 그래프 구조의 언어를 사용하여 우아하게 표현할 수 있기 때문입니다.눈에 띄는 예에는 분자 (원자 및 결합의 그래프로 표시), 소셜 네트워크 및 운송 네트워크가 포함됩니다.이 잠재력은 이미 주요 과학 및 산업 그룹에서 볼 수 있었으며, 트래픽 예측, 약물 발견, 소셜 네트워크 분석 및 추천 시스템을 포함한 이미 영향을받는 응용 분야가 있습니다.또한, 지난 몇 년간 이미지, 텍스트 및 음성 처리를 위해 기계 학습을위한 가장 성공적인 애플리케이션 영역 중 일부는 그래프 표현 학습의 특별한 경우로 간주 될 수 있으며, 결과적으로 이러한 영역간에 정보 교환이 상당히 교환되었습니다.이 짧은 설문 조사의 주요 목표는 독자가 해당 지역의 주요 개념을 동화하고 관련 필드와 적절한 맥락에서 그래프 표현 학습을 위치시키는 것입니다., pronunciation=yeoleomyeon-eseo geulaepeuneun jayeon-eseobadneun deiteoui juyo yangsig-ibnida. igeos-eun jayeon mich ingong siseutem-eseo uliga boneun daebubun-ui paeteon-i geulaepeu gujoui eon-eoleul sayonghayeo uahage pyohyeonhal su issgi ttaemun-ibnida. nun-e ttuineun yeeneun bunja (wonja mich gyeolhab-ui geulaepeulo pyosi), sosyeol neteuwokeu mich unsong neteuwokeuga pohamdoebnida. i jamjaelyeog-eun imi juyo gwahag mich san-eob geulub-eseo bol su iss-eoss-eumyeo, teulaepig yecheug, yagmul balgyeon, sosyeol neteuwokeu bunseog mich chucheon siseutem-eul pohamhan imi yeonghyang-eulbadneun eung-yong bun-yaga issseubnida. ttohan, jinan myeoch nyeongan imiji, tegseuteu mich eumseong cheolileul wihae gigye hagseub-eul-wihan gajang seong-gongjeog-in aepeullikeisyeon yeong-yeog jung ilbuneun geulaepeu pyohyeon hagseub-ui teugbyeolhan gyeong-ulo ganju doel su iss-eumyeo, gyeolgwajeog-eulo ileohan yeong-yeoggan-e jeongbo gyohwan-i sangdanghi gyohwandoeeossseubnida. i jjalb-eun seolmun josaui juyo mogpyoneun dogjaga haedang jiyeog-ui juyo gaenyeom-eul donghwahago gwanlyeon pildeuwa jeogjeolhan maeglag-eseo geulaepeu pyohyeon hagseub-eul wichisikineun geos-ibnida., extra_data=""{'confiden..."")",,
1,"In this survey, I will present a vibrant and exciting area of deep learning research: graph representation learning. Or, put simply, building machine learning models over data that lives on graphs (interconnected structures of nodes connected by edges). These models are commonly known as graph neural networks, or GNNs for short. There is very good reason to study data on graphs. From the molecule (a graph of atoms connected by chemical bonds) all the way to the connectomic structure of the brain (a graph of neurons connected by synapses), graphs are a universal language for describing living organisms, at all levels of organisation. Similarly, most relevant artificial constructs of interest to humans, from the transportation network (a graph of intersections connected by roads) to the social network (a graph of users connected by friendship links), are best reasoned about in terms of graphs. This potential has been realised in recent years by both scientific and industrial groups, with GNNs now being used to discover novel potent antibiotics (Stokes et al., 2020), serve estimated travel times in Google Maps (Derrow-Pinion et al., 2021), power content recommendations in Pinterest (Ying et al., 2018) and product recommendations in Amazon (Hao et al., 2020), and design the latest generation of machine learning hardware: the TPUv5 (Mirhoseini et al., 2021). Further, GNNbased systems have helped mathematicians uncover the hidden structure of mathematical objects (Davies et al., 2021), leading to new top-tier conjectures in the area of representation theory (Blundell et al., 2021). It would not be an understatement to say that billions of people are coming into contact with predictions of a GNN, on a day-to-day basis. As such, it is likely a valuable pursuit to study GNNs, even without aiming to directly contribute to their development. Beyond this, it is likely that the very cognition processes driving our reasoning and decision-making are, in some sense, graph-structured. That is, paraphrasing a quote from Forrester (1971), nobody really imagines in their head all the information known to them; rather, they imagine only selected concepts, and relationships between them, and use those to represent the real system. If we subscribe to this interpretation of cognition, it is quite unlikely that we will be able to build a generally intelligent system without some component relying on graph representation learning. Note that this finding does not clash with the fact that many recent skillful ML systems are based on the Transformer architecture (Vaswani et al., 2017)—as we will uncover in this review, Transformers are themselves a special case of GNNs.","이번 조사에서는 딥러닝 연구의 활기차고 흥미로운 영역인 그래프 표현학습(graph representation learning) 즉, 간단히 말하면 그래프(그래프에 의해 연결된 노드의 상호연결 구조)에 생명체를 설명하는 자료를 구축하는 머신러닝 모델을 그래프 신경망(그래프 신경망, 즉 GNN)으로 연구할 수 있다. 분자(화학적 결합에 의해 연결된 원소의 그래프)에서부터 뇌의 연결구조(뉴런의 그래프)에 이르기까지 까지 그래프는 생명체를 설명하는 보편적인 언어이다. 마찬가지로 교통망(도로로 연결된 교차점의 그래프)에서부터 소셜 네트워크(우정 링크로 연결된 사용자의 그래프)에 이르기까지 인간에게 관심있는 대부분의 관련 인공구조물은 그래프의 측면에서 가장 잘 추론되고 있으며, 최근 과학계와 산업계 모두에서 GNN이 새로운 유효 항생제(Stokes et al., 2020), 구글 지도(Derrow-Pinion et al., 2021), 파파인드(Ying et al., 2018), 제품 추천(Hao et al., 2020), 최신 세대 머신러닝 하드웨어인 TPUv5(Mirhoseini et al., 2021)를 개발하고 있다. 나아가 GNN 기반 시스템은 수학자들이 수학 객체의 숨겨진 구조를 밝혀내는 데 도움을 주었다(Davies et al., 2021), 그리고 표현 이론 영역에서 새로운 최상위 범주(Blundell et al., 2021)로 이어지고 있으며, 일상적으로 수십억 명이 GNN의 예측과 접촉하고 있다고 해도 이를 넘어서 우리의 추론과 의사 결정을 주도하는 바로 그 인지 과정이 그래프로 구조화되었다(Blundell et al., 2021)는 말해도 과언이 아닐 것 같다. 즉, 포레스터(Forrester, 1971)의 인용을 인용하면, 누구도 자신에게 알려진 모든 정보를 실제로 상상하지 않고, 선택된 개념들, 그리고 그들 사이의 관계만을 상상하고, 이를 실제 시스템을 표현하는데 이용한다(Vaswani et al., 2017)--이는 이 리뷰에서 밝혀지겠지만, 최근의 숙련된 ML 시스템들이 그래프 표현 학습에 의존하지 않고 어떤 구성 요소에 의존하지 않는 일반적인 지능형 시스템을 구축할 가능성은 매우 낮다는 사실과 충돌하지 않는다.","Translated(src=en, dest=ko, text=이 설문 조사에서 나는 딥 러닝 연구의 활기차고 흥미 진진한 영역을 제시 할 것입니다 : 그래프 표현 학습.또는 간단히 말해서, 그래프 (가장자리로 연결된 노드의 상호 연결된 구조)에있는 데이터를 통해 기계 학습 모델을 구축하십시오.이 모델은 일반적으로 그래프 신경망 또는 GNN으로 알려져 있습니다.그래프에 대한 데이터를 연구 할 좋은 이유가 있습니다.분자 (화학 결합에 의해 연결된 원자의 그래프)에서 뇌의 연결 구조 (시냅스로 연결된 뉴런의 그래프)까지, 그래프는 모든 수준의 조직에서 살아있는 유기체를 설명하는 보편적 인 언어입니다.마찬가지로, 교통 네트워크 (도로로 연결된 교차로의 그래프)에서 소셜 네트워크 (우정 링크로 연결된 사용자의 그래프)에 이르기까지 인간에게 가장 관련이있는 인공적인 구성은 그래프 측면에서 가장 잘 정리되어 있습니다.이 잠재력은 최근 과학 및 산업 그룹에 의해 실현되었으며, GNN은 이제 새로운 강력한 항생제를 발견하는 데 사용되었으며 (Stokes et al., 2020) Google지도에서 추정 된 여행 시간을 제공합니다 (Derrow-Pinion et al., 2021), Pinterest (Ying et al., 2018) 및 Amazon (Hao et al., 2020)의 Power Content 권장 사항 및 최신 기계 학습 하드웨어 : TPUV5 (Mirhoseini et al., 2021)의 제품 권장 사항.또한, GNN 기반 시스템은 수학자들이 수학 대상의 숨겨진 구조를 밝히도록 도와 주었다 (Davies et al., 2021).수십억 명의 사람들이 매일 GNN의 예측과 접촉하고 있다고 말하는 것은 과소 평가가 아닙니다.따라서, 개발에 직접 기여하지 않더라도 GNN을 연구하는 것은 귀중한 추구 일 것입니다.이 외에도, 우리의 추론과 의사 결정을 주도하는 인식 과정은 어떤 의미에서는 그래프 구조화되었을 가능성이 높습니다.즉, Forrester (1971)의 인용문을 말하면 아무도 그들에게 알려진 모든 정보를 머리 속에 상상하지 못한다.오히려 그들은 선택된 개념과 그들 사이의 관계 만 상상하고 실제 시스템을 나타내는 데 사용합니다.우리 가이 인식에 대한 해석을 구독한다면, 그래프 표현 학습에 의존하는 일부 구성 요소없이 일반적으로 지능적인 시스템을 구축 할 수있을 것 같지 않습니다.이 발견은 최근의 많은 기술이 많은 ML 시스템이 변압기 아키텍처 (Vaswani et al., 2017)를 기반으로한다는 사실과 충돌하지 않습니다.이 검토에서 발견 할 수 있듯이 트랜스포머는 GNN의 특별한 경우입니다., pronunciation=i seolmun josa-eseo naneun dib leoning yeonguui hwalgichago heungmi jinjinhan yeong-yeog-eul jesi hal geos-ibnida : geulaepeu pyohyeon hagseub. ttoneun gandanhi malhaeseo, geulaepeu (gajangjalilo yeongyeoldoen nodeuui sangho yeongyeoldoen gujo)eissneun deiteoleul tonghae gigye hagseub model-eul guchughasibsio. i model-eun ilbanjeog-eulo geulaepeu singyeongmang ttoneun GNNeulo allyeojyeo issseubnida. geulaepeue daehan deiteoleul yeongu hal joh-eun iyuga issseubnida. bunja (hwahag gyeolhab-e uihae yeongyeoldoen wonjaui geulaepeu)eseo noeui yeongyeol gujo (sinaebseulo yeongyeoldoen nyuleon-ui geulaepeu)kkaji, geulaepeuneun modeun sujun-ui jojig-eseo sal-aissneun yugicheleul seolmyeonghaneun bopyeonjeog in eon-eoibnida. machangajilo, gyotong neteuwokeu (dololo yeongyeoldoen gyochaloui geulaepeu)eseo sosyeol neteuwokeu (ujeong lingkeulo yeongyeoldoen sayongjaui geulaepeu)e ileugikkaji ingan-ege gajang gwanlyeon-iissneun ingongjeog-in guseong-eun geulaepeu cheugmyeon-eseo gajang jal jeonglidoeeo issseubnida. i jamjaelyeog-eun choegeun gwahag mich san-eob geulub-e uihae silhyeondoeeoss-eumyeo, GNNeun ije saeloun ganglyeoghan hangsaengjeleul balgyeonhaneun de sayongdoeeoss-eumyeo (Stokes et al., 2020) Googlejido-eseo chujeong doen yeohaeng sigan-eul jegonghabnida (Derrow-Pinion et al., 2021 ), Pinterest (Ying et al., 2018) mich Amazon (Hao et al., 2020)ui Power Content gwonjang sahang mich choesin gigye hagseub hadeuweeo : TPUV5 (Mirhoseini et al., 2021)ui jepum gwonjang sahang. ttohan, GNN giban siseutem-eun suhagjadeul-i suhag daesang-ui sumgyeojin gujoleul balghidolog dowa jueossda (Davies et al., 2021). susib-eog myeong-ui salamdeul-i maeil GNNui yecheuggwa jeobchoghago issdago malhaneun geos-eun gwaso pyeong-gaga anibnida. ttalaseo, gaebal-e jigjeob giyeohaji anhdeolado GNNeul yeonguhaneun geos-eun gwijunghan chugu il geos-ibnida. i oeedo, uliui chulongwa uisa gyeoljeong-eul judohaneun insig gwajeong-eun eotteon uimieseoneun geulaepeu gujohwadoeeoss-eul ganeungseong-i nopseubnida. jeug, Forrester (1971)ui in-yongmun-eul malhamyeon amudo geudeul-ege allyeojin modeun jeongboleul meoli sog-e sangsanghaji moshanda. ohilyeo geudeul-eun seontaegdoen gaenyeomgwa geudeul saiui gwangye man sangsanghago silje siseutem-eul natanaeneun de sayonghabnida. uli gai insig-e daehan haeseog-eul gudoghandamyeon, geulaepeu pyohyeon hagseub-e uijonhaneun ilbu guseong yoso-eobs-i ilbanjeog-eulo jineungjeog-in siseutem-eul guchug hal su-iss-eul geos gatji anhseubnida. i balgyeon-eun choegeun-ui manh-eun gisul-i manh-eun ML siseutem-i byeon-abgi akitegcheo (Vaswani et al., 2017)leul giban-eulohandaneun sasilgwa chungdolhaji anhseubnida.i geomto-eseo balgyeon hal su issdeus-i teulaenseupomeoneun GNNui teugbyeolhan gyeong-u-ibnida., extra_data=""{'confiden..."")",,
2,"This review does not attempt to be a comprehensive overview of specific GNN layers. That being said: representative convolutional GNNs include the Chebyshev network (Defferrard et al., 2016, ChebyNet), graph convolutional network (Kipf and Welling, 2017, GCN) and the simplified graph convolution (Wu et al., 2019, SGC); representative attentional GNNs include the mixture model CNN (Monti et al., 2017, MoNet), graph attention network (Veliˇckovi´c et al., 2018, GAT) and its recent “v2” variant (Brody et al., 2022, GATv2); and representative message-passing GNNs include interaction networks (Battaglia et al., 2016, IN), message passing neural networks (Gilmer et al., 2017, MPNN) and graph networks (Battaglia et al., 2018, GN). Given such a GNN layer, we can learn (m)any interesting tasks over a graph, by appropriately combining hu. I exemplify the three principal such tasks, grounded in biological examples: Node classification. If the aim is to predict targets for each node u ∈ V, then our output is equivariant, and we can learn a shared classifier directly on hu. A canonical example of this is classifying protein functions (e.g. using gene ontology data (Zitnik and Leskovec, 2017)) in a given protein-protein interaction network, as first done by GraphSAGE (Hamilton et al., 2017). Graph classification. If we want to predict targets for the entire graph, then we want an invariant output, hence need to first reduce all the hu into a common representation, e.g. by performing L u∈V hu, then learning a classifier over the resulting flat vector. A canonical example is classifying molecules for their quantum-chemical properties (Gilmer et al., 2017), estimating pharmacological properties like toxicity or solubility (Duvenaud et al., 2015; Xiong et al., 2019; Jiang et al., 2021) or virtual drug screening (Stokes et al., 2020). Link prediction. Lastly, we may be interested in predicting properties of edges (u, v), or even predicting whether an edge exists; giving rise to the name “link prediction”. In this case, a classifier can be learnt over the concatenation of features hukhv, along with any given edge-level features. Canonical tasks include predicting links between drugs and diseases—drug repurposing (Morselli Gysi et al., 2021), drugs and targets—binding affinity prediction (Lim et al., 2019; Jiang et al., 2020), or drugs and drugs—predicting adverse side-effects from polypharmacy (Zitnik et al., 2018; Deac et al., 2019). It is possible to use the building blocks from the principal tasks above to go beyond classifying the entities given by the input graph, and have systems that produce novel molecules (Mercado et al., 2021) or even perform retrosynthesis—the estimation of which reactions to utilise to synthesise given molecules (Somnath et al., 2021; Liu et al., 2022).","본 검토는 특정 GNN 계층에 대한 포괄적인 개요를 시도하는 것은 아니다. 이와 같이 대표적인 콘볼루션 GNN은 Chebyshev 네트워크(Defferrard et al., 2016, ChebyNet), 그래프 콘볼루션 네트워크(Kipf and Welling, 2017, GCN) 및 단순화된 그래프 콘볼루션(Wu et al., 2019, SGC)을 포함하고, 대표적인 어텐션 GNN은 혼합 모델 CNN(Monti et al., 2017, MoNet), 그래프 어텐션 네트워크(Veliˇckovi ́c et al., 2018, GAT) 및 최근의 변형(Brody et al., 2022, GATv2)를 포함하고, 대표적인 메시지 통과 GNN은 상호작용 네트워크(Battaglia et al., 2016, IN), 메시지 통과 신경망(Gilmer et al., 2017, MPNN) 및 그래프 네트워크(Battaglia et al 이와 같은 GNN 계층을 주어 그래프를 통해 (m)any interesting tasks를 적절히 조합하여 학습할 수 있는데, 생물학적 예들에 근거한 3가지를 예로 들면, Node classification. 각 node u ∈ V에 대한 목표를 예측하는 것을 목표로 하면, 우리의 산출량은 equivariant이며, hu 상의 공유 분류자를 직접 학습할 수 있다(Hamilton et al., 2017). Graph classification. GraphSAGE(Zitnik and Leskovec, 2017)에서 먼저 수행한 것과 같이 단백질-단백질 상호작용 네트워크에서 단백질 함수들을 분류(e.g. 유전자 온톨로지 데이터를 이용한다. 전체 그래프에 대한 목표를 예측하고자 한다면, 불변 출력을 원한다면, 먼저 모든 hu를 공통 표현으로 축소하는 것, 예를 들어 L u∈V hu를 수행한 후, 결과적인 평형 벡터를 통해 분자를 분류(Gilmer et al., 2017), 또는 독성이나 용해성(Duvenaud et al., 2015; Xiong et al., 2019; Jiang et al., 2021)과 같은 약학적 특성을 위해 분자를 분류(Stokes et al., 2020). Link prediction이다. 마지막으로 우리는 엣지(u, v)의 특성 예측, 또는 엣지의 존재 여부를 예측하는 것까지 관심을 가질 수 있는데, 이는 엣지의 특성 예측이라는 명칭을 낳을 수 있으며, 이 경우 어떤 엣지 수준의 특징과 함께 분류기를 학습할 수 있다(Morselli Gysi et al., 2021), 의약품과 의약품 사이의 연관성 예측(Lim et al., 2019; Jiang et al., 2020), 의약품과 의약품으로부터의 부작용 예측(Zitnik et al., 2018; Deac et al., 2019) 등이다. 입력 그래프에 의해 주어진 개체를 분류하는 것을 넘어서, 새로운 분자를 생성하는 시스템(Mercado et al., 2021)을 가질 수도 있고, 소급(retrosynthesis)-주어진 분자를 합성하기 위해 어떤 반응을 이용할 것인지의 추정(Somnath et al., 2021; Liu et al., 2022)까지도 가능하다.","Translated(src=en, dest=ko, text=이 검토는 특정 GNN 계층에 대한 포괄적 인 개요가되지 않습니다.대표적인 Convolutional GNN에는 Chebyshev 네트워크 (Defferrard et al., 2016, Chebynet), 그래프 컨볼 루션 네트워크 (KIPF and Welling, 2017, GCN) 및 단순화 된 그래프 컨볼 루션 (Wu et al., 2019, SGC);대표적인주의 GNN에는 혼합 모델 CNN (Monti et al., 2017, Monet), 그래프주의 네트워크 (Veliˇckovi´c et al., 2018, Gat) 및 최근 ""V2""변형 (Brody et al., 2022, GATV2);대표적인 메시지 통과 GNN에는 상호 작용 네트워크 (Battaglia et al., 2016, IN), 메시지 전달 신경망 (Gilmer et al., 2017, MPNN) 및 그래프 네트워크 (Battaglia et al., 2018, GN)가 포함됩니다.그러한 GNN 층을 감안할 때, 우리는 HU를 적절하게 결합하여 그래프를 통해 흥미로운 작업을 배울 수 있습니다.나는 생물학적 사례에 근거한 세 가지 주요 과제를 보여줍니다 : 노드 분류.목표가 각 노드 u ∈ V에 대한 대상을 예측하는 것이라면, 우리의 출력은 동일하며 HU에서 직접 공유 분류기를 배울 수 있습니다.이에 대한 표준의 예는 단백질 기능을 분류하는 것입니다 (예 : 유전자 온톨로지 데이터 사용 (Zitnik and Leskovec, 2017)).그래프 분류.전체 그래프의 대상을 예측하려면 불변 출력을 원하므로 먼저 모든 HU를 공통 표현으로 줄여야합니다 (예 :l u∈V hu를 수행함으로써 결과 플랫 벡터에 대한 분류기를 학습합니다.표준의 예는 독성 또는 용해도와 같은 약리학 적 특성 추정 (Gilmer et al., 2017)에 대한 분자를 분류하는 것입니다 (Duvenaud et al., 2015; Xiong et al., 2019; Jiang et al., 2021).또는 가상 약물 스크리닝 (Stokes et al., 2020).링크 예측.마지막으로, 우리는 가장자리의 특성을 예측하거나 (u, v) 에지가 존재하는지 여부를 예측하는 데 관심이있을 수 있습니다.""링크 예측""이라는 이름을 제시합니다.이 경우, 주어진 가장자리 수준 기능과 함께 기능 HUKHV의 연결을 통해 분류기를 배울 수 있습니다.표준 과제에는 약물과 질병 간의 연관성 (Morselli Gysi et al., 2021), 약물 및 목표 - 결합 친화력 예측 (Lim et al., 2019; Jiang et al., 2020) 또는 약물 및 약물 - 약물 및 약물 사이의 연관성을 예측하는 것이 포함됩니다.다 약국의 부작용 예측 (Zitnik et al., 2018; Deac et al., 2019).위의 주요 작업의 빌딩 블록을 사용하여 입력 그래프에 의해 주어진 엔티티를 분류하는 것 이상으로 신규 분자를 생성하는 시스템 (Mercado et al., 2021) 또는 심지어 재고 합성을 수행하는 시스템이 있습니다.주어진 분자를 합성에 이용하기 위해 (Somnath et al., 2021; Liu et al., 2022)., pronunciation=i geomtoneun teugjeong GNN gyecheung-e daehan pogwaljeog in gaeyogadoeji anhseubnida. daepyojeog-in Convolutional GNNeneun Chebyshev neteuwokeu (Defferrard et al., 2016, Chebynet), geulaepeu keonbol lusyeon neteuwokeu (KIPF and Welling, 2017, GCN) mich dansunhwa doen geulaepeu keonbol lusyeon (Wu et al., 2019, SGC); daepyojeog-injuui GNNeneun honhab model CNN (Monti et al., 2017, Monet), geulaepeujuui neteuwokeu (Veliˇckovi´c et al., 2018, Gat) mich choegeun ""V2""byeonhyeong (Brody et al., 2022, GATV2 ); daepyojeog-in mesiji tong-gwa GNNeneun sangho jag-yong neteuwokeu (Battaglia et al., 2016, IN), mesiji jeondal singyeongmang (Gilmer et al., 2017, MPNN) mich geulaepeu neteuwokeu (Battaglia et al., 2018, GN)ga pohamdoebnida. geuleohan GNN cheung-eul gam-anhal ttae, ulineun HUleul jeogjeolhage gyeolhabhayeo geulaepeuleul tonghae heungmiloun jag-eob-eul baeul su issseubnida. naneun saengmulhagjeog salyee geungeohan se gaji juyo gwajeleul boyeojubnida : nodeu bunlyu. mogpyoga gag nodeu u ∈ Ve daehan daesang-eul yecheughaneun geos-ilamyeon, uliui chullyeog-eun dong-ilhamyeo HUeseo jigjeob gong-yu bunlyugileul baeul su issseubnida. ie daehan pyojun-ui yeneun danbaegjil gineung-eul bunlyuhaneun geos-ibnida (ye : yujeonja ontolloji deiteo sayong (Zitnik and Leskovec, 2017)). geulaepeu bunlyu. jeonche geulaepeuui daesang-eul yecheughalyeomyeon bulbyeon chullyeog-eul wonhameulo meonjeo modeun HUleul gongtong pyohyeon-eulo jul-yeoyahabnida (ye : l u∈V huleul suhaengham-eulosseo gyeolgwa peullaes begteoe daehan bunlyugileul hagseubhabnida. pyojun-ui yeneun dogseong ttoneun yonghaedowa gat-eun yaglihag jeog teugseong chujeong (Gilmer et al., 2017)e daehan bunjaleul bunlyuhaneun geos-ibnida (Duvenaud et al., 2015; Xiong et al., 2019; Jiang et al., 2021). ttoneun gasang yagmul seukeulining (Stokes et al., 2020). lingkeu yecheug. majimag-eulo, ulineun gajangjaliui teugseong-eul yecheughageona (u, v) ejiga jonjaehaneunji yeobuleul yecheughaneun de gwansim-iiss-eul su issseubnida. ""lingkeu yecheug""ilaneun ileum-eul jesihabnida. i gyeong-u, jueojin gajangjali sujun gineung-gwa hamkke gineung HUKHVui yeongyeol-eul tonghae bunlyugileul baeul su issseubnida. pyojun gwajeeneun yagmulgwa jilbyeong gan-ui yeongwanseong (Morselli Gysi et al., 2021), yagmul mich mogpyo - gyeolhab chinhwalyeog yecheug (Lim et al., 2019; Jiang et al., 2020) ttoneun yagmul mich yagmul - yagmul mich yagmul saiui yeongwanseong-eul yecheughaneun geos-i pohamdoebnida. da yaggug-ui bujag-yong yecheug (Zitnik et al., 2018; Deac et al., 2019). wiui juyo jag-eob-ui bilding beullog-eul sayonghayeo iblyeog geulaepeue uihae jueojin entitileul bunlyuhaneun geos isang-eulo singyu bunjaleul saengseonghaneun siseutem (Mercado et al., 2021) ttoneun simjieo jaego habseong-eul suhaenghaneun siseutem-i issseubnida. jueojin bunjaleul habseong-e iyonghagi wihae (Somnath et al., 2021; Liu et al., 2022)., extra_data=""{'confiden..."")",,
3,"Text-to-image synthesis has recently seen significant progress thanks to large pretrained language models, large-scale training data, and the introduction of scalable model families such as diffusion and autoregressive models. However, the best-performing models require iterative evaluation to generate a single sample. In contrast, generative adversarial networks (GANs) only need a single forward pass. They are thus much faster, but they currently remain far behind the stateof-the-art in large-scale text-to-image synthesis. This paper aims to identify the necessary steps to regain competitiveness. Our proposed model, StyleGAN-T, addresses the specific requirements of large-scale text-to-image synthesis, such as large capacity, stable training on diverse datasets, strong text alignment, and controllable variation vs. text alignment tradeoff. StyleGAN-T significantly improves over previous GANs and outperforms distilled diffusion models— the previous state-of-the-art in fast text-to-image synthesis— in terms of sample quality and speed.","텍스트-영상 합성은 최근 대형의 정형화 언어 모델, 대규모의 학습 데이터, 확산 및 자기 회귀 모델과 같은 확장 가능한 모델군의 도입에 힘입어 상당한 진보를 보이고 있으나, 최상위 모형은 단일 샘플을 생성하기 위해 반복적인 평가를 필요로 하지만, 생성적 적대적 네트워크(GANs)는 단일 순회전(순회전)을 필요로 하기 때문에 매우 빠르지만, 현재는 대규모의 텍스트-영상 합성 기술에 크게 뒤처져 있다. 본고에서는 경쟁력을 회복하기 위한 필요한 단계를 규명하고자 한다. 우리의 제안된 모델인 StyleGAN-T는 큰 용량, 다양한 데이터세트에 대한 안정적인 트레이닝, 강한 텍스트 정렬, 통제 가능한 변동성 대 텍스트 정렬(text alignment tradeoff)과 같은 대규모 텍스트-영상 합성의 특정 요건들을 다루고 있으며, 이전의 GAN들에 비해 표본 품질과 속도 측면에서 이전의 최첨단-텍스트-영상 합성 모델들을 능가한다.","Translated(src=en, dest=ko, text=텍스트-이미지 합성은 최근 대규모 사전 미리 언어 모델, 대규모 교육 데이터 및 확산 및자가 회귀 모델과 같은 확장 가능한 모델 패밀리의 도입 덕분에 상당한 진전을 보였습니다.그러나 가장 잘 수행되는 모델은 단일 샘플을 생성하기 위해 반복 평가가 필요합니다.대조적으로, 생성 적대적 네트워크 (GANS)에는 단일 전진 패스 만 있으면됩니다.따라서 그들은 훨씬 빠르지 만 현재 대규모 텍스트-이미지 합성의 상태보다 훨씬 뒤떨어져 있습니다.이 백서는 경쟁력을 회복하는 데 필요한 단계를 식별하는 것을 목표로합니다.우리의 제안 된 모델 인 Stylegan-T는 다양한 데이터 세트에 대한 대용량, 안정적인 교육, 강력한 텍스트 정렬 및 텍스트 정렬 트레이드 오프와 같은 대규모 텍스트-이미지 합성의 특정 요구 사항을 해결합니다.Stylegan-T는 샘플 품질과 속도 측면에서 이전 GANS 및 성능 증류 확산 모델 (빠른 텍스트-이미지 합성의 이전 최첨단)을 능가합니다., pronunciation=tegseuteu-imiji habseong-eun choegeun daegyumo sajeon mili eon-eo model, daegyumo gyoyug deiteo mich hwagsan michjaga hoegwi modelgwa gat-eun hwagjang ganeunghan model paemilliui doib deogbun-e sangdanghan jinjeon-eul boyeossseubnida. geuleona gajang jal suhaengdoeneun model-eun dan-il saempeul-eul saengseonghagi wihae banbog pyeong-gaga pil-yohabnida. daejojeog-eulo, saengseong jeogdaejeog neteuwokeu (GANS)eneun dan-il jeonjin paeseu man iss-eumyeondoebnida. ttalaseo geudeul-eun hwolssin ppaleuji man hyeonjae daegyumo tegseuteu-imiji habseong-ui sangtaeboda hwolssin dwitteol-eojyeo issseubnida. i baegseoneun gyeongjaenglyeog-eul hoeboghaneun de pil-yohan dangyeleul sigbyeolhaneun geos-eul mogpyolohabnida. uliui jean doen model in Stylegan-Tneun dayanghan deiteo seteue daehan daeyonglyang, anjeongjeog-in gyoyug, ganglyeoghan tegseuteu jeonglyeol mich tegseuteu jeonglyeol teuleideu opeuwa gat-eun daegyumo tegseuteu-imiji habseong-ui teugjeong yogu sahang-eul haegyeolhabnida. Stylegan-Tneun saempeul pumjilgwa sogdo cheugmyeon-eseo ijeon GANS mich seongneung jeunglyu hwagsan model (ppaleun tegseuteu-imiji habseong-ui ijeon choecheomdan)eul neung-gahabnida., extra_data=""{'confiden..."")",,
4,"We choose StyleGAN-XL as our baseline architecture because of its strong performance in class-conditional ImageNet synthesis (Sauer et al., 2022). In this section, we modify this baseline piece by piece, focusing on the generator (Section 3.1), discriminator (Section 3.2), and variation vs. text alignment tradeoff mechanisms (Section 3.3) in turn. Throughout the redesign process, we measure the effect of our changes using zero-shot MS COCO. For practical reasons, the tests use a limited compute budget, smaller models, and a smaller dataset than the large-scale experiments in Section 4; see Appendix A for details. We quantify sample quality using FID (Heusel et al., 2017) and text alignment using CLIP score (Hessel et al., 2021). Following prior art (Balaji et al., 2022), we compute the CLIP score using a ViT-g-14 model trained on LAION-2B (Schuhmann et al., 2022). To change the class conditioning to text conditioning in our baseline model, we embed the text prompts using a pretrained CLIP ViT-L/14 text encoder (Radford et al., 2021) and use them in place of the class embedding. Accordingly, we also remove the training-time classifier guidance. This simple conditioning mechanism matches the early text-toimage models (Reed et al., 2016a;b). As shown in Table 1, this baseline reaches a zero-shot FID of 51.88 and CLIP score of 5.58 in our lightweight training configuration.","우리는 스타일GAN-XL을 클래스-조건부 이미지넷 합성(Sauer et al., 2022)에서 강한 성능을 얻기 때문에 우리의 베이스라인 아키텍처로 선택하는데, 재설계 과정에서 생성기(제3.1절), 판별기(제3.2절), 변이 vs. 텍스트 정렬 메커니즘(제3.3절)을 중심으로 이 베이스라인 조각을 조각별로 수정한다.(제3.3절) 실험들은 실무적인 이유로 제한된 계산예산, 더 작은 모형, 더 작은 데이터세트를 사용한다. FID(Heusel et al., 2017)와 CLIP score(Hessel et al., 2021)를 이용하여 표본 품질을 정량화하고, 선행기술(Balaji et al., 2022)에 따라 LAION-2B(Schuhmann et al., 2022)에 학습된 ViT-g-14 모형을 이용하여 CLIP score를 계산하고, 교정-시간 분류자 안내(Radford et al., 2021)를 이용하여 CLIP ViT-L/14 텍스트 인코더(Hessel et al., 2021)를 사용하여 텍스트 프롬프트를 임베딩하고, 교정-시간 분류자 안내를 제거한다. 이 간단한 컨디셔닝 메커니즘은 초기 텍스트-투 이미지 모델(Reed et al., 2016a;b)과 일치하는데, Table 1에서 보는 바와 같이, 이 베이스라인은 우리의 경량 훈련 구성에서 제로-샷 FID 51.88, CLIP score 5.58에 도달한다.","Translated(src=en, dest=ko, text=우리는 클래스 조건부 Imagenet 합성에서 강력한 성능으로 인해 기준 아키텍처로 Stylegan-XL을 선택합니다 (Sauer et al., 2022).이 섹션에서는이 기준선을 생성기 (섹션 3.1), 식별기 (섹션 3.2) 및 변형 대 텍스트 정렬 트레이드 오프 메커니즘 (섹션 3.3)에 중점을별로 수정합니다.재 설계 프로세스를 통해 제로 샷 MS Coco를 사용하여 변경의 효과를 측정합니다.실제적인 이유로, 테스트는 섹션 4의 대규모 실험보다 제한된 컴퓨팅 예산, 소규모 모델 및 더 작은 데이터 세트를 사용합니다.자세한 내용은 부록 A를 참조하십시오.우리는 FID (Heusel et al., 2017)와 클립 점수 (Hessel et al., 2021)를 사용한 텍스트 정렬을 사용하여 샘플 품질을 정량화합니다.Prior Art (Balaji et al., 2022)에 따라, 우리는 Laion-2B에서 훈련 된 VIT-G-14 모델을 사용하여 클립 점수를 계산합니다 (Schuhmann et al., 2022).기준선 모델에서 클래스 컨디셔닝을 텍스트 컨디셔닝으로 변경하기 위해, 우리는 사전 처리 된 클립 VIT-L/14 텍스트 인코더 (Radford et al., 2021)를 사용하여 텍스트 프롬프트를 포함시키고 클래스 임베딩 대신 사용합니다.따라서 교육 시간 분류기 안내도 제거합니다.이 간단한 컨디셔닝 메커니즘은 초기 텍스트-투시 모델과 일치합니다 (Reed et al., 2016a; b).표 1에 도시 된 바와 같이,이 기준선은 경량 훈련 구성에서 51.88의 제로 샷 FID에 도달하고 클립 점수는 5.58에 도달합니다., pronunciation=ulineun keullaeseu jogeonbu Imagenet habseong-eseo ganglyeoghan seongneung-eulo inhae gijun akitegcheolo Stylegan-XLeul seontaeghabnida (Sauer et al., 2022). i segsyeon-eseoneun-i gijunseon-eul saengseong-gi (segsyeon 3.1), sigbyeolgi (segsyeon 3.2) mich byeonhyeong dae tegseuteu jeonglyeol teuleideu opeu mekeonijeum (segsyeon 3.3)e jungjeom-eulbyeollo sujeonghabnida. jae seolgye peuloseseuleul tonghae jelo syas MS Cocoleul sayonghayeo byeongyeong-ui hyogwaleul cheugjeonghabnida. siljejeog-in iyulo, teseuteuneun segsyeon 4ui daegyumo silheomboda jehandoen keompyuting yesan, sogyumo model mich deo jag-eun deiteo seteuleul sayonghabnida. jasehan naeyong-eun bulog Aleul chamjohasibsio. ulineun FID (Heusel et al., 2017)wa keullib jeomsu (Hessel et al., 2021)leul sayonghan tegseuteu jeonglyeol-eul sayonghayeo saempeul pumjil-eul jeonglyanghwahabnida. Prior Art (Balaji et al., 2022)e ttala, ulineun Laion-2Beseo hunlyeon doen VIT-G-14 model-eul sayonghayeo keullib jeomsuleul gyesanhabnida (Schuhmann et al., 2022). gijunseon model-eseo keullaeseu keondisyeoning-eul tegseuteu keondisyeoning-eulo byeongyeonghagi wihae, ulineun sajeon cheoli doen keullib VIT-L/14 tegseuteu inkodeo (Radford et al., 2021)leul sayonghayeo tegseuteu peulompeuteuleul pohamsikigo keullaeseu imbeding daesin sayonghabnida. ttalaseo gyoyug sigan bunlyugi annaedo jegeohabnida. i gandanhan keondisyeoning mekeonijeum-eun chogi tegseuteu-tusi modelgwa ilchihabnida (Reed et al., 2016a; b). pyo 1e dosi doen bawa gat-i,i gijunseon-eun gyeonglyang hunlyeon guseong-eseo 51.88ui jelo syas FIDe dodalhago keullib jeomsuneun 5.58e dodalhabnida., extra_data=""{'confiden..."")",,
5,"Guiding the text encoder. Interestingly, the earlier methods listed above that use a pretrained generator did not report encountering low-level image artifacts. We hypothesize that the frozen generator acts as a prior that suppresses them. We build on this insight to further improve the text alignment. In our primary training phase, the generator is trainable and the text encoder is frozen. We then introduce a secondary phase, where the generator is frozen and the text encoder becomes trainable instead. We only train the text encoder as far as the generator conditioning is concerned; the discriminator and the guidance term (Eq. 2) still receive ctext from the original frozen encoder. This secondary phase allows a very high CLIP guidance weight of 50 without introducing artifacts and significantly improves text alignment without compromising FID (Section 4.2). Compared to the primary phase, the secondary phase can be much shorter. After convergence, we continue with the primary phase.",텍스트 인코더. 흥미롭게도 위에서 열거한 앞서의 방법들은 저수준의 이미지 아티팩트들을 겪는 경우를 보고하지 않았다. 동결된 인코더가 이들을 억제하는 선행으로서 작용한다는 가설을 기반으로 하여 1차 트레이닝 단계에서는 인코더가 트레이닝되고 이어서 2차 트레이닝 단계에서는 인코더가 트레이닝되고 판별기(Eq.2)는 여전히 원래의 동결된 인코더로부터 ctext를 수신한다. 이 2차 국면은 아티팩트를 도입하지 않고도 50의 매우 높은 CLIP 가이던스 가중치를 허용하고 FID를 손상시키지 않으면서 텍스트 정렬을 대폭 개선한다(제4.2). 1차 국면에 비해 2차 국면은 수렴이 훨씬 짧을 수 있다.,"Translated(src=en, dest=ko, text=텍스트 인코더 안내.흥미롭게도, 사전에 발생한 발전기를 사용하는 위에 나열된 초기 방법은 저수준 이미지 아티팩트가 발생하는 것을보고하지 않았습니다.우리는 냉동 발전기가 그들을 억제하는 사전 역할을한다는 가설을 세웁니다.우리는 텍스트 정렬을 더욱 향상시키기 위해이 통찰력을 기반으로합니다.1 차 훈련 단계에서 발전기는 훈련 가능하고 텍스트 인코더가 동결됩니다.그런 다음 발전기가 얼어 붙고 텍스트 인코더가 대신 훈련 가능하게되는 보조 단계를 도입합니다.우리는 발전기 컨디셔닝에 관한 한 텍스트 인코더를 훈련시킵니다.판별 자 및 지침 용어 (식 2)는 원래의 냉동 인코더로부터 여전히 CTEXT를받습니다.이 보조 단계는 아티팩트를 도입하지 않고 50의 매우 높은 클립 안내 무게를 허용하고 FID를 손상시키지 않고 텍스트 정렬을 크게 향상시킵니다 (섹션 4.2).1 차 단계와 비교하여 2 차 단계는 훨씬 짧을 수 있습니다.수렴 후, 우리는 1 차 단계를 계속합니다., pronunciation=tegseuteu inkodeo annae. heungmilobgedo, sajeon-e balsaenghan baljeongileul sayonghaneun wie nayeoldoen chogi bangbeob-eun jeosujun imiji atipaegteuga balsaenghaneun geos-eulbogohaji anh-assseubnida. ulineun naengdong baljeongiga geudeul-eul eogjehaneun sajeon yeoghal-eulhandaneun gaseol-eul se-ubnida. ulineun tegseuteu jeonglyeol-eul deoug hyangsangsikigi wihaei tongchallyeog-eul giban-eulohabnida. 1 cha hunlyeon dangyeeseo baljeongineun hunlyeon ganeunghago tegseuteu inkodeoga dong-gyeoldoebnida. geuleon da-eum baljeongiga eol-eo butgo tegseuteu inkodeoga daesin hunlyeon ganeunghagedoeneun bojo dangyeleul doibhabnida. ulineun baljeongi keondisyeoning-e gwanhan han tegseuteu inkodeoleul hunlyeonsikibnida. panbyeol ja mich jichim yong-eo (sig 2)neun wonlaeui naengdong inkodeolobuteo yeojeonhi CTEXTleulbadseubnida. i bojo dangyeneun atipaegteuleul doibhaji anhgo 50ui maeu nop-eun keullib annae mugeleul heoyonghago FIDleul sonsangsikiji anhgo tegseuteu jeonglyeol-eul keuge hyangsangsikibnida (segsyeon 4.2). 1 cha dangyewa bigyohayeo 2 cha dangyeneun hwolssin jjalb-eul su issseubnida. sulyeom hu, ulineun 1 cha dangyeleul gyesoghabnida., extra_data=""{'confiden..."")",,
6,"Potential harms of large language models can be mitigated by watermarking model output, i.e., embedding signals into generated text that are invisible to humans but algorithmically detectable from a short span of tokens. We propose a watermarking framework for proprietary language models. The watermark can be embedded with negligible impact on text quality, and can be detected using an efficient open-source algorithm without access to the language model API or parameters. The watermark works by selecting a randomized set of “whitelist” tokens before a word is generated, and then softly promoting use of whitelist tokens during sampling. We propose a statistical test for detecting the watermark with interpretable p-values, and derive an information-theoretic framework for analyzing the sensitivity of the watermark. We test the watermark using a multibillion parameter model from the Open Pretrained Transformer (OPT) family, and discuss robustness and security","큰 언어 모델들의 잠재적 해는 워터마크링 모델 출력, 즉, 인간에게는 보이지 않지만 알고리즘적으로는 토큰들의 짧은 기간으로부터 검출가능한 생성된 텍스트에 신호를 내장함으로써 완화될 수 있고, 언어 모델 API나 파라미터들에 접근하지 않고 효율적인 오픈소스 알고리즘을 이용하여 워터마크를 내장할 수 있으며, 워터마크는 텍스트 품질에 미미한 영향을 미치면서 샘플링 동안 워터마크의 사용을 소프트하게 촉진함으로써 작동한다. 해석 가능한 p-값으로 워터마크를 검출하기 위한 통계적 검정을 제안하고, 워터마크의 민감도를 분석하기 위한 정보 이론적 프레임워크를 도출하며, Open Pretrained Transformer(OPT)의 다억 파라미터 모형을 이용하여 워터마크를 검출하고 강건성과 보안성을 논의한다.","Translated(src=en, dest=ko, text=대형 언어 모델의 잠재적 피해는 워터 마킹 모델 출력, 즉 인간에게는 보이지 않지만 짧은 토큰에서 알고리즘 적으로 감지 할 수있는 생성 된 텍스트에 신호를 포함시킬 수 있습니다.우리는 독점 언어 모델을위한 워터 마킹 프레임 워크를 제안합니다.워터 마크는 텍스트 품질에 무시할 수있는 영향으로 내장 될 수 있으며 언어 모델 API 또는 매개 변수에 액세스하지 않고 효율적인 오픈 소스 알고리즘을 사용하여 감지 할 수 있습니다.워터 마크는 단어가 생성되기 전에 무작위 ""화이트리스트""토큰 세트를 선택한 다음 샘플링 중에 화이트리스트 토큰의 사용을 부드럽게 촉진하여 작동합니다.해석 가능한 p- 값으로 워터 마크를 감지하기위한 통계 테스트를 제안하고 워터 마크의 민감도를 분석하기위한 정보 이론적 프레임 워크를 도출합니다.OPT (Open Pretrained Transformer) 제품군의 수백만 파라미터 모델을 사용하여 워터 마크를 테스트하고 견고성 및 보안에 대해 논의합니다., pronunciation=daehyeong eon-eo model-ui jamjaejeog pihaeneun woteo making model chullyeog, jeug ingan-egeneun boiji anhjiman jjalb-eun tokeun-eseo algolijeum jeog-eulo gamji hal su-issneun saengseong doen tegseuteue sinholeul pohamsikil su issseubnida. ulineun dogjeom eon-eo model-eul-wihan woteo making peuleim wokeuleul jeanhabnida. woteo makeuneun tegseuteu pumjil-e musihal su-issneun yeonghyang-eulo naejang doel su iss-eumyeo eon-eo model API ttoneun maegae byeonsue aegseseuhaji anhgo hyoyuljeog-in opeun soseu algolijeum-eul sayonghayeo gamji hal su issseubnida. woteo makeuneun dan-eoga saengseongdoegi jeon-e mujag-wi ""hwaiteuliseuteu""tokeun seteuleul seontaeghan da-eum saempeulling jung-e hwaiteuliseuteu tokeun-ui sayong-eul budeuleobge chogjinhayeo jagdonghabnida. haeseog ganeunghan p- gabs-eulo woteo makeuleul gamjihagiwihan tong-gye teseuteuleul jeanhago woteo makeuui mingamdoleul bunseoghagiwihan jeongbo ilonjeog peuleim wokeuleul dochulhabnida. OPT (Open Pretrained Transformer) jepumgun-ui subaegman palamiteo model-eul sayonghayeo woteo makeuleul teseuteuhago gyeongoseong mich boan-e daehae non-uihabnida., extra_data=""{'confiden..."")",,
7,"How hard is it to remove the watermark? The use of the one proportion z-test makes removal of the watermark difficult. Consider the case of a watermarked sequence of length T = 1000. Suppose an adversary modifies 200 tokens in the sequence to add blacklist words and scrub the watermark. A modified token at position t can violate the blacklist rule at position t. Furthermore, the value of st determines the blacklist for token st+1, and a maximally adversarial choice of st will put st+1 in violation of the blacklist rule as well. For this reason, 200 token flips can create at most 400 violations of the blacklist rule. Unfortunately for the attacker, this maximally adversarial sequence with 600 remaining whitelist tokens still produces a z-statistic of 2(600−1000/2)/ √ 1000 ≈ 6.3, and a p-value of ≈ 10−10 , leaving the watermark readily detectable with extremely high confidence. In general, removing the watermark of a long sequence requires modifying roughly one quarter of the tokens or more. Note the analysis above assumes the attacker has complete knowledge of the watermark, and each selected token is maximally adversarial (which likely has a negative impact on quality). Without knowledge of the watermark algorithm, each flipped token has only a 50% chance of being blacklisted, as does the adjacent token. In this case, the attacker above only creates 200 blacklist words (in expectation) by modifying 200 tokens. Methods for keeping the watermark algorithm secret but available via API are discussed in Section 5.","워터마크 제거가 얼마나 어려운가?1 비례 z-검정의 사용은 워터마크 제거를 어렵게 한다. 길이 T=1000의 워터마크 시퀀스의 adversary가 200개의 토큰을 수정하여 블랙리스트 단어를 추가하고 워터마크를 스크러핑하면, t위치의 수정 토큰은 t위치의 블랙리스트 규칙을 위반할 수 있으며, 더 나아가 st의 값은 토큰 st+1에 대해 최대적으로 적대적 선택이 st+1을 블랙리스트 규칙을 위반하게 할 수 있다. 이 때문에 200개의 토큰 플립은 최대 400개의 블랙리스트 규칙 위반을 생성할 수 있다. 공격자에게는 불행하게도, 600개의 남아있는 화이트리스트 토큰을 갖는 이 최대적으로 적대적 시퀀스는 여전히 z-통계량 2(600−1000/2)/ √ 1000 ≈ 6.3, p-value ≈ 10−10 을 산출하여, 일반적으로 긴 시퀀스의 워터마크 제거는 매우 높은 신뢰도로 워터마크를 검출할 수 있다.(위의 분석은 공격자가 워터마크 알고리즘에 대한 완전한 지식을 갖고 있다고 가정한다.) 이 경우 위의 공격자는 200개의 토큰을 수정하여 200개의 블랙리스트 단어(예상)만 생성한다. 워터마크 알고리즘은 비밀을 유지하되 API를 통해 이용 가능한 방법은 5절에서 논의된다.","Translated(src=en, dest=ko, text=워터 마크를 제거하는 것이 얼마나 어렵습니까?하나의 비율 z- 테스트를 사용하면 워터 마크를 제거하기가 어렵습니다.길이 t = 1000의 워터 마크 시퀀스의 경우를 고려하십시오. 적대자가 블랙리스트 단어를 추가하고 워터 마크를 문지르기 위해 순서로 200 개의 토큰을 수정한다고 가정하십시오.위치 t에서 수정 된 토큰은 위치 t에서 블랙리스트 규칙을 위반할 수 있습니다.또한 St의 값은 Token ST+1의 블랙리스트를 결정하고 ST의 최대 적대적 선택은 ST+1을 블랙리스트 규칙을 위반할 수 있습니다.이러한 이유로 200 개의 토큰 플립은 블랙리스트 규칙을 최대 400 개의 위반으로 만들 수 있습니다.불행히도 공격자의 경우 600 개의 남아있는 화이트리스트 토큰을 갖는이 최대 적대적 시퀀스는 여전히 2 (600-1000/ 2)/ √ 1000 ≈ 6.3의 z- 스캔과 ≈ 10-10의 p- 값을 생성하여 워터 마크를 쉽게 남겨 둡니다.매우 높은 신뢰로 탐지 할 수 있습니다.일반적으로, 긴 시퀀스의 워터 마크를 제거하려면 토큰의 약 1/4 이상을 수정해야합니다.위의 분석은 공격자가 워터 마크에 대한 완전한 지식을 가지고 있다고 가정하고, 선택된 각 토큰은 최대 적대적 대적 (품질에 부정적인 영향을 미칠 수 있음)입니다.워터 마크 알고리즘에 대한 지식이 없다면, 각 플루트 토큰은 인접한 토큰과 마찬가지로 블랙리스트에 50%의 확률로 50% 밖에 걸리지 않습니다.이 경우 위의 공격자는 200 개의 토큰을 수정하여 200 개의 블랙리스트 단어 (예상) 만 생성합니다.워터 마크 알고리즘을 비밀로 유지하지만 API를 통해 사용할 수있는 방법은 섹션 5에서 설명합니다., pronunciation=woteo makeuleul jegeohaneun geos-i eolmana eolyeobseubnikka? hanaui biyul z- teseuteuleul sayonghamyeon woteo makeuleul jegeohagiga eolyeobseubnida. gil-i t = 1000ui woteo makeu sikwonseuui gyeong-uleul golyeohasibsio. jeogdaejaga beullaegliseuteu dan-eoleul chugahago woteo makeuleul munjileugi wihae sunseolo 200 gaeui tokeun-eul sujeonghandago gajeonghasibsio. wichi teseo sujeong doen tokeun-eun wichi teseo beullaegliseuteu gyuchig-eul wibanhal su issseubnida. ttohan Stui gabs-eun Token ST+1ui beullaegliseuteuleul gyeoljeonghago STui choedae jeogdaejeog seontaeg-eun ST+1eul beullaegliseuteu gyuchig-eul wibanhal su issseubnida. ileohan iyulo 200 gaeui tokeun peullib-eun beullaegliseuteu gyuchig-eul choedae 400 gaeui wiban-eulo mandeul su issseubnida. bulhaenghido gong-gyeogjaui gyeong-u 600 gaeui nam-aissneun hwaiteuliseuteu tokeun-eul gajneun-i choedae jeogdaejeog sikwonseuneun yeojeonhi 2 (600-1000/ 2)/ √ 1000 ≈ 6.3ui z- seukaengwa ≈ 10-10ui p- gabs-eul saengseonghayeo woteo makeuleul swibge namgyeo dubnida. maeu nop-eun sinloelo tamji hal su issseubnida. ilbanjeog-eulo, gin sikwonseuui woteo makeuleul jegeohalyeomyeon tokeun-ui yag 1/4 isang-eul sujeonghaeyahabnida. wiui bunseog-eun gong-gyeogjaga woteo makeue daehan wanjeonhan jisig-eul gajigo issdago gajeonghago, seontaegdoen gag tokeun-eun choedae jeogdaejeog daejeog (pumjil-e bujeongjeog-in yeonghyang-eul michil su iss-eum)ibnida. woteo makeu algolijeum-e daehan jisig-i eobsdamyeon, gag peulluteu tokeun-eun injeobhan tokeungwa machangajilo beullaegliseuteue 50%ui hwaglyullo 50% bakk-e geolliji anhseubnida. i gyeong-u wiui gong-gyeogjaneun 200 gaeui tokeun-eul sujeonghayeo 200 gaeui beullaegliseuteu dan-eo (yesang) man saengseonghabnida. woteo makeu algolijeum-eul bimillo yujihajiman APIleul tonghae sayonghal su-issneun bangbeob-eun segsyeon 5eseo seolmyeonghabnida., extra_data=""{'confiden..."")",,
8,"7.1. Degradation Under Attack: Span Replacement Using a LM We study a realistic black-box attack by attempting to remove the presence of the watermark by replacing spans in the original output text using another language model. We treat the watermark algorithm as if it is private, mocking seclusion behind an API. The attacker does not have access to the locations of whitelisted tokens and instead tries to modify the text through token replacement at random indices until a certain word replacement budget, ε, is reached. The budget constraint maintains a level semantic similarity between the original watermarked text and the attacked text, otherwise the “utility” of the original text for its intended task may be lost. Also, each span replacement in the attack is performed via inference using a multi-million parameter language model, roughly a third the size of the target model, but still with an associated cost per execution that requires a base level of efficiency with respect to model calls. In our experiment, we adopt T5-Large (Raffel et al., 2020) as the replacement model and iteratively select and replace tokens until the attacker either reaches the budget or no more suitable replacement candidates are returned.","7.1. 공격하에서의 저하(Degradation Under Attack: Span Replacement) LM을 이용하여 원본 출력 텍스트에서 Span을 다른 언어 모델을 이용하여 워터마크의 존재를 제거하려고 시도함으로써 현실적인 블랙박스 공격을 연구하는 공격자는 화이트리스트된 토큰의 위치에 접근하지 않고 API 뒤에 있는 워터마크 알고리즘을 사물처럼 취급하여 원본 텍스트와 공격자의 의도된 태스크를 위한 원본 텍스트의 의미 유사성을 유지하는 일정한 단어대체예정(예정 ε)에 도달할 때까지 토큰대체정을 시도한다. 또한, 공격에서의 각각의 스파크 대체는 타겟 모델의 크기가 대략 3분의 1인 다수 개의 파라미터 언어 모델을 이용한 추론을 통해 수행되지만, 여전히 모델 호출에 대한 기저 수준의 효율성을 요구하는 실행당 비용으로 T5-Large(Raffel et al., 2020)를 대체 모델로 채택하여 공격자가 예산에 도달하거나 더 적합한 대체 후보가 반환되지 않을 때까지 반복적으로 토큰을 선택하고 대체한다.","Translated(src=en, dest=ko, text=7.1.공격 하원의 저하 : SPAN LM을 사용한 스팬 교체 우리는 다른 언어 모델을 사용하여 원래 출력 텍스트에서 스팬을 교체하여 워터 마크의 존재를 제거하려고 시도함으로써 현실적인 블랙 박스 공격을 연구합니다.Watermark 알고리즘을 API 뒤에 비공개 인 것처럼 취급합니다.공격자는 화이트리스트 토큰의 위치에 액세스 할 수 없으며 대신 특정 단어 교체 예산 인 ε에 도달 할 때까지 무작위 지수로 토큰 교체를 통해 텍스트를 수정하려고합니다.예산 제약 조건은 원래 워터 마크 텍스트와 공격 텍스트 사이의 수준 의미 론적 유사성을 유지하며, 그렇지 않으면 의도 된 작업에 대한 원본 텍스트의 ""유틸리티""가 손실 될 수 있습니다.또한 공격의 각 스팬 교체는 대상 모델의 크기의 대략 3 분의 1 인 수백만 매개 변수 언어 모델을 사용하여 추론을 통해 수행되지만 모델과 관련하여 기본 수준의 효율이 필요한 실행 당 관련 비용이 여전히 있습니다.전화.우리의 실험에서, 우리는 교체 모델로 T5-Large (Raffel et al., 2020)를 채택하고 공격자가 예산에 도달하거나 더 이상 적절한 교체 후보자가 반환 될 때까지 토큰을 선택하고 교체합니다., pronunciation=7.1. gong-gyeog hawon-ui jeoha : SPAN LMeul sayonghan seupaen gyoche ulineun daleun eon-eo model-eul sayonghayeo wonlae chullyeog tegseuteueseo seupaen-eul gyochehayeo woteo makeuui jonjaeleul jegeohalyeogo sidoham-eulosseo hyeonsiljeog-in beullaeg bagseu gong-gyeog-eul yeonguhabnida. Watermark algolijeum-eul API dwie bigong-gae in geoscheoleom chwigeubhabnida. gong-gyeogjaneun hwaiteuliseuteu tokeun-ui wichie aegseseu hal su eobs-eumyeo daesin teugjeong dan-eo gyoche yesan in ee dodal hal ttaekkaji mujag-wi jisulo tokeun gyocheleul tonghae tegseuteuleul sujeonghalyeogohabnida. yesan jeyag jogeon-eun wonlae woteo makeu tegseuteuwa gong-gyeog tegseuteu saiui sujun uimi lonjeog yusaseong-eul yujihamyeo, geuleohji anh-eumyeon uido doen jag-eob-e daehan wonbon tegseuteuui ""yutilliti""ga sonsil doel su issseubnida. ttohan gong-gyeog-ui gag seupaen gyocheneun daesang model-ui keugiui daelyag 3 bun-ui 1 in subaegman maegae byeonsu eon-eo model-eul sayonghayeo chulon-eul tonghae suhaengdoejiman modelgwa gwanlyeonhayeo gibon sujun-ui hyoyul-i pil-yohan silhaeng dang gwanlyeon biyong-i yeojeonhi issseubnida. jeonhwa. uliui silheom-eseo, ulineun gyoche modello T5-Large (Raffel et al., 2020)leul chaetaeghago gong-gyeogjaga yesan-e dodalhageona deo isang jeogjeolhan gyoche hubojaga banhwan doel ttaekkaji tokeun-eul seontaeghago gyochehabnida., extra_data=""{'confiden..."")",,
9,"Deep learning has been widely successful in practice and most state-of-the-art machine learning methods are based on neural networks. Lacking, however, is a rigorous mathematical theory that adequately explains the amazing performance of deep neural networks. In this article, we present a relatively new mathematical framework that provides the beginning of a deeper understanding of deep learning. This framework precisely characterizes the functional properties of neural networks that are trained to fit to data. The key mathematical tools which support this framework include transform-domain sparse regularization, the Radon transform of computed tomography, and approximation theory, which are all techniques deeply rooted in signal processing. This framework explains the effect of weight decay regularization in neural network training, the use of skip connections and low-rank weight matrices in network architectures, the role of sparsity in neural networks, and explains why neural networks can perform well in high-dimensional problems.","딥러닝은 실무에서 널리 성공적이었고 대부분의 최신의 머신러닝 방법은 신경망을 기반으로 하며, 그러나 딥 뉴럴 네트워크의 놀라운 성능을 적절히 설명하는 엄격한 수학 이론이 부족하다. 이 글에서는 딥러닝에 대한 보다 깊은 이해의 시작을 제공하는 비교적 새로운 수학적 프레임워크를 제시하는데, 이 프레임워크를 지원하는 핵심적인 수학적 도구는 변환 영역(transform-domain sparse regularization, Radon 변환)과 컴퓨팅된 tomography, 근사 이론이다. 이 프레임워크는 뉴럴 네트워크 트레이닝에서 가중치 퇴화 정규화의 효과, 네트워크 아키텍처에서의 스킵 연결 및 저위치 가중치 행렬의 사용, 뉴럴 네트워크에서의 희소성의 역할을 설명하고, 뉴럴 네트워크가 왜 고차원 문제에서 잘 수행할 수 있는지를 설명한다.","Translated(src=en, dest=ko, text=딥 러닝은 실제로 널리 성공했으며 대부분의 최첨단 기계 학습 방법은 신경망을 기반으로합니다.그러나 부족한 것은 깊은 신경망의 놀라운 성능을 적절하게 설명하는 엄격한 수학적 이론입니다.이 기사에서는 딥 러닝에 대한 더 깊은 이해의 시작을 제공하는 비교적 새로운 수학적 프레임 워크를 제시합니다.이 프레임 워크는 데이터에 맞도록 훈련 된 신경망의 기능적 특성을 정확하게 특성화합니다.이 프레임 워크를 지원하는 주요 수학적 도구에는 Transform-Domain Sparse 정규화, 컴퓨터 단층 촬영의 라돈 변환 및 신호 처리에 깊이 뿌리 박힌 모든 기술인 근사 이론이 포함됩니다.이 프레임 워크는 신경망 훈련에서 중량 붕괴 정규화, 네트워크 아키텍처에서의 스킵 연결 및 저 순위 무게 매트릭스의 사용, 신경망에서의 희소성의 역할 및 신경 네트워크가 고차원 문제에서 잘 수행 할 수있는 이유를 설명합니다.., pronunciation=dib leoning-eun siljelo neolli seong-gonghaess-eumyeo daebubun-ui choecheomdan gigye hagseub bangbeob-eun singyeongmang-eul giban-eulohabnida. geuleona bujoghan geos-eun gip-eun singyeongmang-ui nollaun seongneung-eul jeogjeolhage seolmyeonghaneun eomgyeoghan suhagjeog ilon-ibnida. i gisa-eseoneun dib leoning-e daehan deo gip-eun ihaeui sijag-eul jegonghaneun bigyojeog saeloun suhagjeog peuleim wokeuleul jesihabnida. i peuleim wokeuneun deiteoe majdolog hunlyeon doen singyeongmang-ui gineungjeog teugseong-eul jeonghwaghage teugseonghwahabnida. i peuleim wokeuleul jiwonhaneun juyo suhagjeog dogueneun Transform-Domain Sparse jeong-gyuhwa, keompyuteo dancheung chwal-yeong-ui ladon byeonhwan mich sinho cheolie gip-i ppuli baghin modeun gisul-in geunsa ilon-i pohamdoebnida. i peuleim wokeuneun singyeongmang hunlyeon-eseo junglyang bung-goe jeong-gyuhwa, neteuwokeu akitegcheoeseoui seukib yeongyeol mich jeo sun-wi muge maeteuligseuui sayong, singyeongmang-eseoui huisoseong-ui yeoghal mich singyeong neteuwokeuga gochawon munjeeseo jal suhaeng hal su-issneun iyuleul seolmyeonghabnida. ., extra_data=""{'confiden..."")",,
10,"Translation Prompt: ChatGPT is essentially a large language model, which needs prompts as guidance to trigger its translation ability. The style of prompts may affect the quality of translation outputs. For example, how to mention the source or target language information matters in multilingual machine translation models, which is usually solved by attaching language tokens (Johnson et al., 2017; Fan et al., 2021). • Multilingual Translation: ChatGPT is a single model handling various NLP tasks and covering different languages, which can be considered a unified multilingual machine translation model. Thus, we are curious about how ChatGPT performs on different language pairs considering both the resource difference (e.g., high vs. low) and language family (e.g., European vs. Asian). • Translation Robustness: ChatGPT is developed upon GPT3, which was trained on largescale datasets that cover various domains. Therefore, we wonder if it can perform robustly well on domain-specific or even noisy sentences.","번역 프롬프트: ChatGPT는 본질적으로 큰 언어 모델로서, 그 번역 능력을 트리거하기 위한 안내로서 프롬프트가 필요하며, 프롬프트의 스타일은 번역 출력의 품질에 영향을 미칠 수 있으며, 예를 들어 언어 토큰(Johnson et al., 2017; Fan et al., 2021)을 첨부함으로써 통상적으로 해결된다(Johnson et al., 2017), 다국어 번역 프롬프트: ChatGPT는 통합된 다국어 기계 번역 모델로 간주될 수 있다. 이에 ChatGPT가 자원 차이(예: 높음 vs 낮음)와 언어 계열(예: 유럽 vs 아시아)을 모두 고려한 상이한 언어 쌍에 대해 어떻게 수행하는지 궁금하다. • 번역 Robustness: ChatGPT는 다양한 도메인을 포괄하는 대규모 데이터세트에 훈련된 GPT3을 기반으로 개발되었기 때문에 도메인 특정 문장이나 시끄러운 문장에 강건하게 수행할 수 있을지 궁금하다.","Translated(src=en, dest=ko, text=번역 프롬프트 : Chatgpt는 본질적으로 큰 언어 모델로 변환 능력을 트리거하기위한 지침으로 프롬프트가 필요합니다.프롬프트 스타일은 번역 출력의 품질에 영향을 줄 수 있습니다.예를 들어, 다국어 기계 번역 모델에서 소스 또는 대상 언어 정보를 언급하는 방법은 일반적으로 언어 토큰을 첨부하여 해결됩니다 (Johnson et al., 2017; Fan et al., 2021).• 다국어 번역 : ChatGpt는 다양한 NLP 작업을 처리하고 다른 언어를 다루는 단일 모델로 통합 된 다국어 기계 번역 모델로 간주 될 수 있습니다.따라서 우리는 자원 차이 (예 : 높은 대 낮음)와 언어 패밀리 (예 : 유럽 대 아시아)를 고려하여 ChatGpt가 다른 언어 쌍에서 어떻게 수행되는지 궁금합니다.• 번역 견고성 : ChatGpt는 GPT3에서 개발되었으며 다양한 도메인을 다루는 대형 데이터 세트에서 교육을 받았습니다.따라서 도메인 별 또는 시끄러운 문장에서 강력하게 잘 수행 할 수 있는지 궁금합니다., pronunciation=beon-yeog peulompeuteu : Chatgptneun bonjiljeog-eulo keun eon-eo modello byeonhwan neunglyeog-eul teuligeohagiwihan jichim-eulo peulompeuteuga pil-yohabnida. peulompeuteu seutail-eun beon-yeog chullyeog-ui pumjil-e yeonghyang-eul jul su issseubnida. yeleul deul-eo, dagug-eo gigye beon-yeog model-eseo soseu ttoneun daesang eon-eo jeongboleul eongeubhaneun bangbeob-eun ilbanjeog-eulo eon-eo tokeun-eul cheombuhayeo haegyeoldoebnida (Johnson et al., 2017; Fan et al., 2021). • dagug-eo beon-yeog : ChatGptneun dayanghan NLP jag-eob-eul cheolihago daleun eon-eoleul daluneun dan-il modello tonghab doen dagug-eo gigye beon-yeog modello ganju doel su issseubnida. ttalaseo ulineun jawon chai (ye : nop-eun dae naj-eum)wa eon-eo paemilli (ye : yuleob dae asia)leul golyeohayeo ChatGptga daleun eon-eo ssang-eseo eotteohge suhaengdoeneunji gung-geumhabnida. • beon-yeog gyeongoseong : ChatGptneun GPT3eseo gaebaldoeeoss-eumyeo dayanghan domein-eul daluneun daehyeong deiteo seteueseo gyoyug-eul bad-assseubnida. ttalaseo domein byeol ttoneun sikkeuleoun munjang-eseo ganglyeoghage jal suhaeng hal su issneunji gung-geumhabnida., extra_data=""{'confiden..."")",,
11,"Resource Difference. We consider the resource difference of languages in the same family. In machine translation, German⇔English translation is usually regarded as a high-resource task supported by over ten million sentence pairs (Farhad et al., 2021) while Romanian⇔English translation is supported by much less data (Bojar et al., 2016). As shown in Table 4, ChatGPT performs competitively with Google Translate and DeepL Translate for both German⇒English and English⇒German translations. However, it lags behind them significantly on Romanian⇒English and English⇒Romanian. Specifically, ChatGPT obtains a BLEU score on English⇒Romanian that is 46.4% lower than Google Translate and the value is 10.3% on Romanian⇒English. We speculate that the huge resource difference of monolingual data between English and Romanian limits the language modeling capability of Romanian, which partially explains the poor performance on English⇒Romanian. On the contrary, Romanian⇒English can benefit from the strong language modeling capability of English such that the resource gap of parallel data can be somewhat compensated.","자원 차이. 동일한 계열의 언어의 자원 차이를 살펴보면, 기계 번역에서 독일⇔영어 번역은 통상 1,000만 문장 쌍이 지원하는 고자원 작업(Farhad et al., 2021)인 반면, 루마니아⇔영어 번역은 훨씬 적은 데이터(Bojar et al., 2016)로 분석한 결과, ChatGPT는 Table 4에서와 같이 독일⇒영어⇒독일어 번역 모두에서 구글 번역과 DeepL 번역이 경쟁적으로 수행하지만, 루마니아⇒영어⇒독일어 번역에서는 그 값이 46.4% 낮은 BLEU 점수를 획득한다. 우리는 영어와 루마니아어의 단일 언어 자료의 엄청난 자원 차이가 루마니아어의 언어 모델링 능력에 제한을 준다고 추측하고 있는데, 이는 영어⇒루마니아어에 대한 부실한 성능을 부분적으로 설명하는 것이기도 하지만, 반대로 루마니아⇒영어의 강력한 언어 모델링 능력으로 인해 병행 자료의 자원 격차가 어느 정도 보상될 수 있다.","Translated(src=en, dest=ko, text=자원 차이.우리는 같은 가족의 언어의 자원 차이를 고려합니다.기계 번역에서, 독일인 english 번역은 일반적으로 1 천만 개가 넘는 문장 쌍 (Farhad et al., 2021)이 지원하는 고 자원 과제로 간주되는 반면, 루마니아의 영어 번역은 훨씬 적은 데이터에 의해 지원됩니다 (Bojar et al., 2016.).표 4에서 볼 수 있듯이 ChatGpt는 Google Translate와 경쟁적으로 성과를 거두며 Deepl은 German (German translate) 및 English (English별) 번역을 위해 번역합니다.그러나 그것은 루마니아 인과 영어와 영어에 크게 뒤쳐져 있습니다.특히 Chatgpt는 Google Translate보다 46.4% 낮은 English Thermanian에서 BLEU 점수를 얻었으며 Romanian 대업에서는 10.3%입니다.우리는 영어와 루마니아의 단일 언어 데이터의 거대한 자원 차이가 루마니아어의 언어 모델링 기능을 제한한다고 추측합니다. 이는 영어 ⇒ 로마의 성능이 좋지 않다고 설명합니다.반대로, Romanian향 엔술은 영어의 강력한 언어 모델링 기능을 활용하여 병렬 데이터의 자원 간격이 다소 보상 될 수 있습니다., pronunciation=jawon chai. ulineun gat-eun gajog-ui eon-eoui jawon chaileul golyeohabnida. gigye beon-yeog-eseo, dog-il-in english beon-yeog-eun ilbanjeog-eulo 1 cheonman gaega neomneun munjang ssang (Farhad et al., 2021)i jiwonhaneun go jawon gwajelo ganjudoeneun banmyeon, lumaniaui yeong-eo beon-yeog-eun hwolssin jeog-eun deiteoe uihae jiwondoebnida (Bojar et al., 2016. ). pyo 4eseo bol su issdeus-i ChatGptneun Google Translatewa gyeongjaengjeog-eulo seong-gwaleul geodumyeo Deepleun German (German translate) mich English (Englishbyeol) beon-yeog-eul wihae beon-yeoghabnida. geuleona geugeos-eun lumania ingwa yeong-eowa yeong-eoe keuge dwichyeojyeo issseubnida. teughi Chatgptneun Google Translateboda 46.4% naj-eun English Thermanian-eseo BLEU jeomsuleul eod-eoss-eumyeo Romanian daeeob-eseoneun 10.3%ibnida. ulineun yeong-eowa lumaniaui dan-il eon-eo deiteoui geodaehan jawon chaiga lumania-eoui eon-eo modelling gineung-eul jehanhandago chucheughabnida. ineun yeong-eo ⇒ lomaui seongneung-i johji anhdago seolmyeonghabnida. bandaelo, Romanianhyang ensul-eun yeong-eoui ganglyeoghan eon-eo modelling gineung-eul hwal-yonghayeo byeonglyeol deiteoui jawon gangyeog-i daso bosang doel su issseubnida., extra_data=""{'confiden..."")",,
0,"In conclusion, the performance of the germanium detectors for high-energy γ-rays was
evaluated using the 992-keV resonance in the 27Al(p, γ)
28Si reaction. The energy calibration
was conducted using the energy reference from NNDC. The excitation energy of the level
at 12.5 MeV was adopted to be 12.5407(2) MeV from the residual analysis. The calibration
function has a non-linearity at around 1.5 MeV. By correcting the non-linearity, the energy
accuracy is achieved at 0.3 keV for the overall energy region. The photo-peak efficiency
in the wide energy range was also measured. The extrapolation of the efficiency curve in
the low-energy region overestimates the efficiency measured in the high-energy region. The
evaluation of the non-linearity of the energy calibration and photo-peak efficiencies by the
actual measurement is crucial for accurate spectroscopy of high-energy photons.","결론적으로 27Al(p, γ) 28Si 반응에서의 992-keV 공명을 이용하여 고에너지 γ-ray에 대한 germanium 검출기의 성능을 평가하였으며, NNDC로부터의 에너지 기준을 이용하여 12.5 MeV에서의 excitation energy를 residual analysis에서 12.5407(2) MeV로 채택하여 캘리브레이션 함수를 1.5 MeV 내외로 보정함으로써 전체 에너지 영역에서 0.3 keV로 에너지 정확도를 달성하고 광-피크 효율도 측정하였다. 저에너지 영역에서의 효율 곡선의 외삽은 고에너지 영역에서 측정된 효율을 과대평가한다. 실제 측정에 의한 에너지 교정 및 광-점 효율의 비선형성 평가는 고에너지 광의 정확한 분광분석에 중요하다.","Translated(src=en, dest=ko, text=결론적으로, 고 에너지 γ- 선에 대한 게르마늄 탐지기의 성능은
27AL에서 992-KEV 공명을 사용하여 평가했습니다 (P, γ)
28SI 반응.에너지 교정
NNDC의 에너지 기준을 사용하여 수행되었습니다.레벨의 흥분 에너지
12.5에서 MEV는 잔류 분석으로부터 12.5407 (2) MEV로 채택되었다.교정
함수는 약 1.5 MeV에서 비선형 성입니다.비선형 성을 교정함으로써 에너지
전체 에너지 영역의 경우 0.3 keV에서 정확도가 달성됩니다.포토 피크 효율
넓은 에너지 범위에서도 측정되었습니다.효율 곡선의 외삽
저에너지 영역은 고 에너지 영역에서 측정 된 효율을 과대 평가합니다.그만큼
에너지 교정의 비선형 성 평가 및 광-피크 효율성의 평가
실제 측정은 고 에너지 광자의 정확한 분광법에 중요합니다., pronunciation=gyeollonjeog-eulo, go eneoji g- seon-e daehan geleumanyum tamjigiui seongneung-eun
27ALeseo 992-KEV gongmyeong-eul sayonghayeo pyeong-gahaessseubnida (P, g)
28SI ban-eung. eneoji gyojeong
NNDCui eneoji gijun-eul sayonghayeo suhaengdoeeossseubnida. lebel-ui heungbun eneoji
12.5eseo MEVneun janlyu bunseog-eulobuteo 12.5407 (2) MEVlo chaetaegdoeeossda. gyojeong
hamsuneun yag 1.5 MeVeseo biseonhyeong seong-ibnida. biseonhyeong seong-eul gyojeongham-eulosseo eneoji
jeonche eneoji yeong-yeog-ui gyeong-u 0.3 keVeseo jeonghwagdoga dalseongdoebnida. poto pikeu hyoyul
neolb-eun eneoji beom-wieseodo cheugjeongdoeeossseubnida. hyoyul gogseon-ui oesab
jeoeneoji yeong-yeog-eun go eneoji yeong-yeog-eseo cheugjeong doen hyoyul-eul gwadae pyeong-gahabnida. geumankeum
eneoji gyojeong-ui biseonhyeong seong pyeong-ga mich gwang-pikeu hyoyulseong-ui pyeong-ga
silje cheugjeong-eun go eneoji gwangjaui jeonghwaghan bungwangbeob-e jung-yohabnida., extra_data=""{'confiden..."")",,
1,"The experiment was performed at the RIKEN tandem accelerator (Pelletron 5SDH-2, 1.7
MV max). The experimental setup is shown in Fig. 1. A 0.8-µm thick aluminum target (20
mmφ
) was irradiated with the proton beam at 1 MeV in the beam duct (1-mm thick, made of
stainless). The proton beam intensity was approximately 300 nA, and the measurement time
was 6.5 hours. Two large volume Ge detectors, GMX80 (Ortec) and GX5019 (Canberra),
were used for γ-ray detection. GMX80 is an n-type 80% coaxial detector, and GX5019 is a
p-type 50% coaxial detector. The distance from the target to the detector surface was 5 cm
(GMX80) and 10 cm (GX5019), respectively. Signals from the detectors were acquired by a
waveform digitizer (Caen V1730B, sampling rate of 500MHz and resolution of 14bit). The
maximum energy of the input dynamic range of the digitizer was set to about 20 MeV.","실험은 RIKEN tandem accelerator(Pelletron 5SDH-2, 1.7 MV max)에서 실시하였으며, 실험 셋업은 Fig. 1과 같으며, γ-ray 검출을 위해 2대의 대형 체적 Ge 검출기인 GMX80(Ortec)과 GX5019(Canberra)를 사용하였으며, GMX80은 n형 80% 동축 검출기이며, GX5019는 p형 50% 동축 검출기이다. 타겟으로부터 검출기 표면까지의 거리는 각각 5 cm(GMX80), 10 cm(GX5019)로 검출기로부터 파형 디지타이저(Caen V1730B, 500MHz, 샘플링 속도 500MHz, 14bit)에 의해 신호를 획득하였으며, 디지타이저의 입력 동적 범위의 최대 에너지는 약 20 MeV로 설정하였다.","Translated(src=en, dest=ko, text=실험은 Riken Tandem Accelerator (Pelletron 5SDH-2, 1.7에서 수행되었다.
MV MAX).실험 설정은 그림 1에 나와 있습니다. 0.8-µm 두께의 알루미늄 표적 (20
mmφ
) 빔 덕트에서 1 MeV에서 양성자 빔으로 조사 하였다 (1mm 두께,
스테인리스).양성자 빔 강도는 약 300 Na였으며 측정 시간
6.5 시간이었다.2 개의 대량 GE 검출기, GMX80 (ORTEC) 및 GX5019 (캔버라),
γ- 선 검출에 사용되었습니다.GMX80은 N- 타입 80% 동축 검출기이고 GX5019는
P 형 50% 동축 검출기.표적에서 검출기 표면까지의 거리는 5cm였습니다.
(GMX80) 및 10cm (GX5019).검출기로부터의 신호는 a
파형 디지티터 (Caen V1730B, 500MHz의 샘플링 속도 및 14 비트 해상도).그만큼
디지타이저의 입력 동적 범위의 최대 에너지는 약 20 meV로 설정되었습니다., pronunciation=silheom-eun Riken Tandem Accelerator (Pelletron 5SDH-2, 1.7eseo suhaengdoeeossda.
MV MAX). silheom seoljeong-eun geulim 1e nawa issseubnida. 0.8-µm dukke-ui alluminyum pyojeog (20
mmph
) bim deogteueseo 1 MeVeseo yangseongja bim-eulo josa hayeossda (1mm dukke,
seuteinliseu). yangseongja bim gangdoneun yag 300 Nayeoss-eumyeo cheugjeong sigan
6.5 sigan-ieossda. 2 gaeui daelyang GE geomchulgi, GMX80 (ORTEC) mich GX5019 (kaenbeola),
g- seon geomchul-e sayongdoeeossseubnida. GMX80eun N- taib 80% dongchug geomchulgiigo GX5019neun
P hyeong 50% dongchug geomchulgi. pyojeog-eseo geomchulgi pyomyeonkkajiui geolineun 5cmyeossseubnida.
(GMX80) mich 10cm (GX5019). geomchulgilobuteoui sinhoneun a
pahyeong dijititeo (Caen V1730B, 500MHzui saempeulling sogdo mich 14 biteu haesangdo). geumankeum
dijitaijeoui iblyeog dongjeog beom-wiui choedae eneojineun yag 20 meVlo seoljeongdoeeossseubnida., extra_data=""{'confiden..."")",,
2,"We study the effects of three-nucleon short-range correlations on nuclear coordinate-space densities. For this purpose, novel three-body densities are calculated for ground state nuclei using the
auxiliary-field diffusion Monte Carlo method. The results are analyzed in terms of the Generalized
Contact Formalism, extended to include three-body correlations, revealing the universal behavior of
nucleon triplets at short distances. We identify the quantum numbers of such correlated triplets and
extract scaling factors of triplet abundances that can be compared to upcoming inclusive electron
scattering data.","3핵체 단거리 상관관계가 핵 좌표-공간 밀도에 미치는 영향을 연구하고, 이를 위해 보조-장 확산 몬테카를로 방법을 사용하여 지상 상태 핵체에 대해 새로운 3핵체 밀도를 계산하고, 3핵체 상관관계를 확장하여 단거리에서 핵체 트리플릿의 보편적 행태를 밝히는 Generalized Contact Formalism의 관점에서 결과를 분석하고, 이와 같은 상관관계의 핵체 트리플릿의 양수를 파악하고, 앞으로의 포괄적인 전자산출 자료와 비교할 수 있는 핵체 풍부의 스케일링 요인을 추출한다.","Translated(src=en, dest=ko, text=우리는 핵 좌표 공간 밀도에 대한 3- 뉴클레온 단거리 상관 관계의 효과를 연구합니다.이를 위해, 새로운 3- 바디 밀도는 지상 상태 핵에 대해 계산됩니다.
보조 필드 확산 Monte Carlo Method.결과는 일반화 된 측면에서 분석됩니다
3 바디 상관 관계를 포함하도록 확장 된 연락처 형식주의,
짧은 거리에서 핵 트리플렛.우리는 그러한 상관 트리플렛의 양자 수를 식별하고
다가오는 포괄적 인 전자와 비교할 수있는 삼중 항 풍성의 스케일링 계수 추출
산란 데이터., pronunciation=ulineun haeg jwapyo gong-gan mildo-e daehan 3- nyukeulle-on dangeoli sang-gwan gwangyeui hyogwaleul yeonguhabnida. ileul wihae, saeloun 3- badi mildoneun jisang sangtae haeg-e daehae gyesandoebnida.
bojo pildeu hwagsan Monte Carlo Method. gyeolgwaneun ilbanhwa doen cheugmyeon-eseo bunseogdoebnida
3 badi sang-gwan gwangyeleul pohamhadolog hwagjang doen yeonlagcheo hyeongsigjuui,
jjalb-eun geolieseo haeg teulipeulles. ulineun geuleohan sang-gwan teulipeulles-ui yangja suleul sigbyeolhago
dagaoneun pogwaljeog in jeonjawa bigyohal su-issneun samjung hang pungseong-ui seukeilling gyesu chuchul
sanlan deiteo., extra_data=""{'confiden..."")",,
3,"Short-range correlations (SRCs) are an integral part
of strongly interacting many-body quantum systems, including nuclei and atomic systems. Strong SRCs between nucleons pose one of the main challenges in the
description of nuclei. Accounting for the impact of SRC
physics is crucial for the description of two-body densities
and momentum distributions [1–6], electron and neutrino
scattering [7–11], spectroscopic factors [12–24], neutrinoless double beta decay matrix elements [25–27], neutron
star properties [28–31] and more","단거리 상관관계(SRCs)는 핵과 원자 체계를 포함하는 다세포 양자 체계를 강하게 상호작용하는 핵심적인 부분이며, 핵 사이의 강한 SRC는 핵의 설명에 주요한 과제 중 하나이다. SRC 물리학의 영향은 2체밀도와 운동량 분포(1–6], 전자와 네트trino 산산화[7–1], 분광 요인(12–24], 네트trino 없는 이중 베타 퇴적 행성 요소[25–27], neutron 별 특성[28–31] 등이다.","Translated(src=en, dest=ko, text=단거리 상관 관계 (SRC)는 필수 부분입니다
핵 및 원자 시스템을 포함한 많은 바디 양자 시스템을 강력하게 상호 작용하는 것.핵 사이의 강한 SRC는
핵에 대한 설명.SRC의 영향을 설명합니다
물리학은 2 바디 밀도에 대한 설명에 중요합니다
및 운동량 분포 [1–6], 전자 및 중성미자
산란 [7-11], 분광 인자 [12–24], 중성미자 이중 베타 붕괴 행렬 요소 [25–27], 중성자
별 속성 [28–31] 등, pronunciation=dangeoli sang-gwan gwangye (SRC)neun pilsu bubun-ibnida
haeg mich wonja siseutem-eul pohamhan manh-eun badi yangja siseutem-eul ganglyeoghage sangho jag-yonghaneun geos. haeg saiui ganghan SRCneun
haeg-e daehan seolmyeong. SRCui yeonghyang-eul seolmyeonghabnida
mullihag-eun 2 badi mildo-e daehan seolmyeong-e jung-yohabnida
mich undonglyang bunpo [1–6], jeonja mich jungseongmija
sanlan [7-11], bungwang inja [12–24], jungseongmija ijung beta bung-goe haenglyeol yoso [25–27], jungseongja
byeol sogseong ​​[28–31] deung, extra_data=""{'confiden..."")",,
4,"In order to unitarily evolve a quantum system, an agent requires knowledge of time, a parameter
which no physical clock can ever perfectly characterise. In this letter, we study how limitations on
acquiring knowledge of time impact controlled quantum operations in different paradigms. We show
that the quality of timekeeping an agent has access to limits the gate complexity they are able to
achieve within circuit-based quantum computation. It also exponentially impacts state preparation
for measurement-based quantum computation. Another area where quantum control is relevant is
quantum thermodynamics. In that context, we show that cooling a qubit can be achieved using a
timer of arbitrary quality for control: timekeeping error only impacts the rate of cooling and not the
achievable temperature. Our analysis combines techniques from the study of autonomous quantum
clocks and the theory of quantum channels to understand the effect of imperfect timekeeping on
controlled quantum dynamics.","양자 시스템을 단일적으로 진화시키기 위해 에이전트는 물리적 시계가 결코 완벽하게 특성화할 수 없는 파라미터인 시간에 대한 지식을 필요로 하는데, 이 글에서는 시간에 대한 지식을 습득하는 한계가 상이한 패러다임들에서 제어된 양자 연산들에 어떻게 영향을 미치는지 연구하고, 에이전트가 회로기반 양자 계산 내에서 달성할 수 있는 게이트 복잡성(gate complexity)에 접근할 수 있는 양자 계산의 질이 측정기반 양자 계산에 지수적으로 영향을 미친다는 것을 보여준다. 양자 제어의 또 다른 영역은 양자 온도역학이다. 우리의 분석은 자율 양자 시계 연구와 양자 채널 이론으로부터의 기법을 결합하여 불완전한 시간 관리가 조절 양자 역학에 미치는 영향을 파악한다.","Translated(src=en, dest=ko, text=Quantum System을 단위로 진화시키기 위해 에이전트는 시간, 매개 변수에 대한 지식이 필요합니다.
물리적 시계가 완벽하게 특성화 될 수 없습니다.이 서한에서 우리는 제한이 어떻게되는지 연구합니다
다른 패러다임에서 시간 충격 통제 양자 운영에 대한 지식 습득.우리는 보여줍니다
에이전트 타임 키핑의 품질은 그들이 할 수있는 게이트 복잡성을 제한하는 데 액세스 할 수 있다는
회로 기반 양자 계산 내에서 달성합니다.또한 상태 준비에 기하 급수적으로 영향을 미칩니다
측정 기반 양자 계산 용.양자 제어가 관련된 또 다른 영역은 다음과 같습니다
양자 열역학.이러한 맥락에서, 우리는 qubit을 냉각시키는 것을 사용하여 달성 할 수 있음을 보여줍니다.
제어를위한 임의의 품질 타이머 : 타임 키핑 오류는
달성 가능한 온도.우리의 분석은 자율 양자 연구의 기술을 결합합니다.
불완전한 시간 유지의 효과를 이해하기위한 시계 및 양자 채널 이론
제어 된 양자 역학., pronunciation=Quantum System-eul dan-wilo jinhwasikigi wihae eijeonteuneun sigan, maegae byeonsue daehan jisig-i pil-yohabnida.
mullijeog sigyega wanbyeoghage teugseonghwa doel su eobs-seubnida. i seohan-eseo ulineun jehan-i eotteohgedoeneunji yeonguhabnida
daleun paeleodaim-eseo sigan chung-gyeog tongje yangja un-yeong-e daehan jisig seubdeug. ulineun boyeojubnida
eijeonteu taim kiping-ui pumjil-eun geudeul-i hal su-issneun geiteu bogjabseong-eul jehanhaneun de aegseseu hal su issdaneun
hoelo giban yangja gyesan naeeseo dalseonghabnida. ttohan sangtae junbie giha geubsujeog-eulo yeonghyang-eul michibnida
cheugjeong giban yangja gyesan yong. yangja jeeoga gwanlyeondoen tto daleun yeong-yeog-eun da-eumgwa gatseubnida
yangja yeol-yeoghag. ileohan maeglag-eseo, ulineun qubit-eul naeng-gagsikineun geos-eul sayonghayeo dalseong hal su iss-eum-eul boyeojubnida.
jeeoleul-wihan im-uiui pumjil taimeo : taim kiping olyuneun
dalseong ganeunghan ondo. uliui bunseog-eun jayul yangja yeonguui gisul-eul gyeolhabhabnida.
bul-wanjeonhan sigan yujiui hyogwaleul ihaehagiwihan sigye mich yangja chaeneol ilon
jeeo doen yangja yeoghag., extra_data=""{'confiden..."")",,
5,"In classical computation, bits of information are abundant and their manipulation is simple. This underpins
the great success of the abstract theory of classical information processing, which allows the design of algorithms
without much consideration for the physical semiconductor hardware that would carry them out. Quantum computation is different: whilst any algorithm before measurement can simply be thought of as a rotation on a
high-dimensional Bloch ball, such a unitary operation is
necessarily generated by a time-dependent Hamiltonian.
And an abstraction to perfect qubits is prohibited by the
impossibility of (almost) perfect error correction, which
is far more demanding for quantum information.","고전적 연산에서는 정보의 비트가 풍부하고 그 조작이 간단하며, 이는 고전적 정보 처리의 추상화 이론의 큰 성공을 가능하게 하며, 이를 수행할 물리적 반도체 하드웨어에 대한 별다른 고려 없이 알고리즘의 설계가 가능하다. 양적 연산은 측정 전의 어떤 알고리즘이 단순히 고차원 블로치볼에 대한 회전으로만 생각될 수 있는 반면, 이와 같은 단위 연산은 반드시 시간에 의존하는 해밀기(Hamiltonian)에 의해 추상화되고, 양적 정보의 추상화가 금지된다.","Translated(src=en, dest=ko, text=고전적인 계산에서는 정보가 풍부하고 조작은 간단합니다.이것은 뒷받침됩니다
고전 정보 처리의 추상적 이론의 큰 성공은 알고리즘 설계를 허용합니다.
물리적 반도체 하드웨어를 많이 고려하지 않고 수행 할 수 있습니다.양자 계산이 다릅니다. 측정 전 알고리즘은 단순히 회전으로 생각할 수 있습니다.
고차원적인 블로치 볼, 그러한 단일 작업이 있습니다
시간 의존적 해밀턴에 의해 반드시 생성됩니다.
그리고 완벽한 큐브에 대한 추상화는
(거의) 완벽한 오류 수정의 불가능성.
양자 정보에 대해 훨씬 더 까다 롭습니다., pronunciation=gojeonjeog-in gyesan-eseoneun jeongboga pungbuhago jojag-eun gandanhabnida. igeos-eun dwisbadchimdoebnida
gojeon jeongbo cheoliui chusangjeog ilon-ui keun seong-gong-eun algolijeum seolgyeleul heoyonghabnida.
mullijeog bandoche hadeuweeoleul manh-i golyeohaji anhgo suhaeng hal su issseubnida. yangja gyesan-i daleubnida. cheugjeong jeon algolijeum-eun dansunhi hoejeon-eulo saeng-gaghal su issseubnida.
gochawonjeog-in beullochi bol, geuleohan dan-il jag-eob-i issseubnida
sigan uijonjeog haemilteon-e uihae bandeusi saengseongdoebnida.
geuligo wanbyeoghan kyubeue daehan chusanghwaneun
(geoui) wanbyeoghan olyu sujeong-ui bulganeungseong.
yangja jeongbo-e daehae hwolssin deo kkada lobseubnida., extra_data=""{'confiden..."")",,
6,"Zero-photon subtraction (ZPS) is a conditional measurement process that can reduce the mean photon number
of quantum optical states without physically removing any photons. Here we show that ZPS can also be used
to transform certain super-Poissonian states into sub-Poissonian states, and vice versa. Combined with a wellknown “no-go” theorem on conditional measurements, this effect leads to a new set of non-classicality criteria
that can be experimentally tested through ZPS measurements.","제로-포토온 감산(ZPS)은 물리적으로 어떤 포토온을 제거하지 않고 양자 광학 상태의 평균 포토온수를 감소시킬 수 있는 조건부 측정 과정이며, 여기에서는 ZPS가 특정 초-포이소니안 상태를 서브-포이소니안 상태로 변환하는데도 사용될 수 있음을 보여주는데, 이 효과는 조건부 측정에 대한 잘 알려진 ""노-고(no-go)"" 정리와 결합되어 ZPS 측정을 통해 실험적으로 검정될 수 있는 새로운 규준의 비-포이소니안성 기준을 도출한다.","Translated(src=en, dest=ko, text=제로 포톤 뺄셈 (ZPS)은 평균 광자 수를 줄일 수있는 조건부 측정 프로세스입니다.
광자를 물리적으로 제거하지 않고 양자 광학 상태의.여기서 우리는 ZPS도 사용할 수 있음을 보여줍니다
특정 슈퍼 포이 니아 국가를 포이 아스 니아 주 하위 주로 전환하고 그 반대도 마찬가지입니다.조건부 측정에 대한 잘 알려진 ""No-Go""정리와 결합 하여이 효과는 새로운 비 클래식 기준 세트로 이어집니다.
ZPS 측정을 통해 실험적으로 테스트 할 수 있습니다., pronunciation=jelo poton ppaelsem (ZPS)eun pyeong-gyun gwangja suleul jul-il su-issneun jogeonbu cheugjeong peuloseseu-ibnida.
gwangjaleul mullijeog-eulo jegeohaji anhgo yangja gwanghag sangtaeui. yeogiseo ulineun ZPSdo sayonghal su iss-eum-eul boyeojubnida
teugjeong syupeo poi nia guggaleul poi aseu nia ju hawi julo jeonhwanhago geu bandaedo machangajiibnida. jogeonbu cheugjeong-e daehan jal allyeojin ""No-Go""jeongliwa gyeolhab hayeoi hyogwaneun saeloun bi keullaesig gijun seteulo ieojibnida.
ZPS cheugjeong-eul tonghae silheomjeog-eulo teseuteu hal su issseubnida., extra_data=""{'confiden..."")",,
7,"The investigation of super-Poissonian and sub-Poissonian
light sources plays a significant role in the history of quantum optics [1, 2]. The photon-number distributions of these
sources display variances that are, respectively, wider or narrower than the benchmark Poissonian statistics of a coherent state with the same average photon number [3]. Experimentally, these properties can be conveniently characterized
by Mandel’s Q parameter, with Q > 0 for super-Poissonian
sources and −1 ≤ Q < 0 for sub-Poissonian sources","초-포이소니안 광원과 하부-포이소니안 광원에 대한 조사는 양자 광학의 역사에서 중요한 역할을 하고 있는데, 이들 광원의 광자수 분포는 동일한 평균 광자수를 가진 일관성 있는 상태의 벤치마크 포이소니안 통계량보다 각각 더 넓거나 좁은 분포를 나타내는데, 초-포이소니안 광원은 Q > 0, 하부-포이소니안 광원은 −1 ≤ Q < 0으로 Mandel의 Q 파라미터로 편리하게 특성을 나타낼 수 있다.","Translated(src=en, dest=ko, text=슈퍼 포이 아스 니아 및 하위 포아 니안의 조사
광원은 양자 광학의 역사에서 중요한 역할을한다 [1, 2].이들의 광자 수 분포
소스는 평균 광자 수가 동일한 일관성 상태의 벤치 마크 포아니안 통계보다 각각 더 넓거나 좁은 차이를 나타냅니다 [3].실험적으로, 이러한 특성은 편리하게 특성화 될 수 있습니다
Mandel의 Q 매개 변수, Super-Poissonian의 경우 Q> 0이 있습니다.
소스 소스 및 포이 아스 니아 소스의 경우 -1 ≤ q <0, pronunciation=syupeo poi aseu nia mich hawi poa nian-ui josa
gwang-won-eun yangja gwanghag-ui yeogsa-eseo jung-yohan yeoghal-eulhanda [1, 2]. ideul-ui gwangja su bunpo
soseuneun pyeong-gyun gwangja suga dong-ilhan ilgwanseong sangtaeui benchi makeu poanian tong-gyeboda gaggag deo neolbgeona job-eun chaileul natanaebnida [3]. silheomjeog-eulo, ileohan teugseong-eun pyeonlihage teugseonghwa doel su issseubnida
Mandel-ui Q maegae byeonsu, Super-Poissonian-ui gyeong-u Q> 0i issseubnida.
soseu soseu mich poi aseu nia soseuui gyeong-u -1 ≤ q <0, extra_data=""{'confiden..."")",,
8,"In this paper, the TF system of two-coupled Black-Scholes equations for pricing the convertible
bonds is solved numerically by using the P1 and P2 finite elements with the inequality constraints
approximated by the penalty method. The corresponding finite element ODE system is numerically
solved by using a modified Crank-Nicolson scheme, in which the non-linear system is solved at each
time step by the Newton-Raphson method for non-smooth functions. Moreover, the corresponding
Greeks are also calculated by taking advantage of the P1-P2 finite element approximation functions.
Numerical solutions by the finite element method compare favorably with the solutions by the finite
difference method in literature","본 논문에서는 변환가능한 채합을 산정하기 위한 2결합 Black-Scholes 방정식의 TF 체계는 P1 및 P2 유한요소를 penalty방식에 의해 근사된 불평등제약을 이용하여 수식적으로 해결하고, 대응하는 유한요소 ODE 체계는 뉴턴-나이클son방식에 의해 비선형계수를 각각의 타임 스텝에서 해결하고, P1-P2 유한요소 근사함수에 의해 대응하는 그리스도 계산된다. 유한요소방식에 의한 수식해들은 문헌에서 유한요소방식에 의한 해와 유리하게 비교된다.","Translated(src=en, dest=ko, text=이 백서에서는 컨버터블 가격 책정을위한 2 개의 커플 링 블랙 찰스 방정식의 TF 시스템
채권은 불평등 제약과 함께 P1 및 P2 유한 요소를 사용하여 수치 적으로 해결됩니다.
페널티 방법에 의해 근사화됩니다.해당 유한 요소 ODE 시스템은 수치 적입니다
비선형 시스템이 각각에서 해결되는 수정 된 크랭크-니콜슨 체계를 사용하여 해결
비활성 기능에 대한 Newton-Raphson 방법에 의한 시간 단계.또한, 해당
그리스인은 또한 P1-P2 유한 요소 근사 함수를 활용하여 계산됩니다.
유한 요소 방법에 의한 수치 솔루션은 유한 한 솔루션과 호의적으로 비교됩니다.
문학의 차이 방법, pronunciation=i baegseoeseoneun keonbeoteobeul gagyeog chaegjeong-eul-wihan 2 gaeui keopeul ling beullaeg chalseu bangjeongsig-ui TF siseutem
chaegwon-eun bulpyeongdeung jeyaggwa hamkke P1 mich P2 yuhan yosoleul sayonghayeo suchi jeog-eulo haegyeoldoebnida.
peneolti bangbeob-e uihae geunsahwadoebnida. haedang yuhan yoso ODE siseutem-eun suchi jeog-ibnida
biseonhyeong siseutem-i gaggag-eseo haegyeoldoeneun sujeong doen keulaengkeu-nikolseun chegyeleul sayonghayeo haegyeol
bihwalseong gineung-e daehan Newton-Raphson bangbeob-e uihan sigan dangye. ttohan, haedang
geuliseu-in-eun ttohan P1-P2 yuhan yoso geunsa hamsuleul hwal-yonghayeo gyesandoebnida.
yuhan yoso bangbeob-e uihan suchi sollusyeon-eun yuhan han sollusyeongwa houijeog-eulo bigyodoebnida.
munhag-ui chai bangbeob, extra_data=""{'confiden..."")",,
9,"Pair trading is one of the most effective statistical arbitrage strategies which seeks a neutral profit by hedging a pair of selected assets.
Existing methods generally decompose the task into two separate
steps: pair selection and trading. However, the decoupling of two
closely related subtasks can block information propagation and
lead to limited overall performance. For pair selection, ignoring the
trading performance results in the wrong assets being selected with
irrelevant price movements, while the agent trained for trading can
overfit to the selected assets without any historical information
of other assets. To address it, in this paper, we propose a paradigm for automatic pair trading as a unified task rather than a
two-step pipeline. We design a hierarchical reinforcement learning
framework to jointly learn and optimize two subtasks. A high-level
policy would select two assets from all possible combinations and
a low-level policy would then perform a series of trading actions.
Experimental results on real-world stock data demonstrate the effectiveness of our method on pair trading compared with both
existing pair selection and trading methods.","쌍 매매는 선택된 자산의 쌍을 헤지하여 중립적인 수익을 추구하는 가장 효과적인 통계적 중개 전략의 하나로서 기존의 방법들은 일반적으로 쌍 선택과 매매라는 두 단계로 분해할 수 있으나, 밀접하게 연관된 두 하위 과제의 디커플링은 정보 전파를 차단하여 제한적인 성과로 이어질 수 있으므로 이를 해결하기 위해 쌍 매매 자동화 패러다임을 2단계 파이프라인으로 훈련된 에이전트가 다른 자산의 이력 정보와 무관하게 선택된 자산에 과대 적합하게 학습하고 최적화하는 위계적 강화 학습 프레임워크를 설계한다. 높은 수준의 정책은 모든 가능한 조합에서 두 자산을 선택하고 낮은 수준의 정책은 일련의 거래 행위를 수행할 것이며, 실제 주식 데이터에 대한 실험 결과는 기존의 쌍 선택과 거래 방법 모두에서 우리의 방법이 쌍 거래에 효과가 있음을 보여준다.","Translated(src=en, dest=ko, text=Pair Trading은 선택된 자산 쌍을 헤딩하여 중립 이익을 추구하는 가장 효과적인 통계 차익 거래 전략 중 하나입니다.
기존 방법은 일반적으로 작업을 두 개의 별도로 분해합니다
단계 : 쌍 선택 및 거래.그러나 두 개의 분리
밀접하게 관련된 하위 작업은 정보 전파를 차단할 수 있습니다
전반적인 성능이 제한됩니다.쌍 선택을 위해, 무시
거래 성과는 잘못된 자산을 선택합니다.
관련없는 가격 변동, 거래를 위해 훈련 된 대리인은
역사적 정보가없는 선택된 자산에 과잉
다른 자산의.이 논문에서, 우리는 자동 쌍 거래를위한 패러다임을
2 단계 파이프 라인.우리는 계층 적 강화 학습을 설계합니다
두 개의 하위 작업을 공동으로 배우고 최적화하는 프레임 워크.높은 수준
정책은 가능한 모든 조합에서 두 자산을 선택하고
그런 다음 저수준 정책은 일련의 거래 조치를 수행합니다.
실제 주식 데이터에 대한 실험 결과는 두 가지와 비교하여 쌍 거래에 대한 방법의 효과를 보여줍니다.
기존 쌍 선택 및 거래 방법., pronunciation=Pair Trading-eun seontaegdoen jasan ssang-eul hedinghayeo junglib iig-eul chuguhaneun gajang hyogwajeog-in tong-gye chaig geolae jeonlyag jung hanaibnida.
gijon bangbeob-eun ilbanjeog-eulo jag-eob-eul du gaeui byeoldolo bunhaehabnida
dangye : ssang seontaeg mich geolae. geuleona du gaeui bunli
miljeobhage gwanlyeondoen hawi jag-eob-eun jeongbo jeonpaleul chadanhal su issseubnida
jeonbanjeog-in seongneung-i jehandoebnida. ssang seontaeg-eul wihae, musi
geolae seong-gwaneun jalmosdoen jasan-eul seontaeghabnida.
gwanlyeon-eobsneun gagyeog byeondong, geolaeleul wihae hunlyeon doen daeliin-eun
yeogsajeog jeongboga-eobsneun seontaegdoen jasan-e gwaing
daleun jasan-ui. i nonmun-eseo, ulineun jadong ssang geolaeleul-wihan paeleodaim-eul
2 dangye paipeu lain. ulineun gyecheung jeog ganghwa hagseub-eul seolgyehabnida
du gaeui hawi jag-eob-eul gongdong-eulo baeugo choejeoghwahaneun peuleim wokeu. nop-eun sujun
jeongchaeg-eun ganeunghan modeun johab-eseo du jasan-eul seontaeghago
geuleon da-eum jeosujun jeongchaeg-eun illyeon-ui geolae jochileul suhaenghabnida.
silje jusig deiteoe daehan silheom gyeolgwaneun du gajiwa bigyohayeo ssang geolaee daehan bangbeob-ui hyogwaleul boyeojubnida.
gijon ssang seontaeg mich geolae bangbeob., extra_data=""{'confiden..."")",,
10,"For pair selection, previous methods aim to find two assets whose
prices have moved together historically in a formation period, and
their future spread is assumed to be historical mean-reverted [23].
They generally adopted statistical or fundamental similarity measurements based on historical price information to perform asset
pair selection before trading. The distance approach was first introduced [8, 10, 16, 19, 35, 36] for pair selection, which simply
adopted distance metrics such as the sum of Euclidean squared
distance (SSD) for the price time series to model the connection
between two assets. However, an ideal asset pair in these modelfree methods were expected to be two assets with exactly the same
price movement in historical time, which have zero trading opportunities for no fluctuations of price spread. There were also
methods [5, 7, 12, 15, 27, 38, 39, 46] that directly model the tradability of a candidate pair based on the Engle-Granger cointegration
test, which performs linear regression using the price series of two
assets and expects the residual to be stationary.","쌍의 선택은 이전의 방법들은 형성 기간에 가격이 함께 움직이는 2개의 자산을 찾고, 이들의 미래 스프레드는 역사 평균 변수(historical mean-reverted [23])로 가정하고, 일반적으로 과거의 가격 정보에 기초한 통계적 또는 근본적인 유사성 측정치들을 채택하여 거래 전에 자산 쌍 선택을 수행하는 거리 접근법(distance approach)(8, 10, 16, 19, 35, 36)을 먼저 도입하여 두 자산 간의 연결을 모델링하였다. 그러나, 이들 모델free 방법에서의 이상적인 자산 쌍은 과거의 방법들에서 가격이 역사적으로 동일하게 움직이는 2개의 자산이므로, 가격 스프레드의 변동에 대해 0의 거래 기회를 갖는다. 두 자산의 가격 계열을 이용하여 선형 회귀분석을 실시하고 잔차가 정상적으로 유지될 것으로 기대하는 Engle-Granger 공적분 검정을 기반으로 후보 쌍의 거래성을 직접 모형화하는 방법들[5, 7, 12, 15, 27, 38, 39, 46]도 있었다.","Translated(src=en, dest=ko, text=쌍 선택의 경우 이전 방법은 두 자산을 찾는 것을 목표로합니다.
가격은 역사적으로 형성 기간에 함께 이동했으며
그들의 미래의 확산은 역사적 평균 반영되는 것으로 가정된다 [23].
그들은 일반적으로 자산을 수행하기 위해 과거 가격 정보를 기반으로 통계적 또는 기본 유사성 측정을 채택했습니다.
거래 전에 쌍을 선택하십시오.거리 접근법은 쌍 선택을 위해 처음으로 [8, 10, 16, 19, 35, 36]
유클리드 제곱의 합과 같은 채택 된 원격 측정 항목
연결을 모델링하기위한 가격 시계열의 거리 (SSD)
두 자산 사이.그러나이 modelfree 방법의 이상적인 자산 쌍은 정확히 동일한 두 자산이 될 것으로 예상되었습니다.
역사적 시간의 가격 이동은 가격 확산의 변동없이 거래 기회가 없습니다.또한도있었습니다
방법 [5, 7, 12, 15, 27, 38, 39, 46] Engle-Granger 공적분을 기반으로 후보 쌍의 전매를 직접 모델링하는 방법 [5]
테스트, 2의 가격 시리즈를 사용하여 선형 회귀를 수행합니다.
자산 및 잔차가 고정 될 것으로 기대합니다., pronunciation=ssang seontaeg-ui gyeong-u ijeon bangbeob-eun du jasan-eul chajneun geos-eul mogpyolohabnida.
gagyeog-eun yeogsajeog-eulo hyeongseong gigan-e hamkke idonghaess-eumyeo
geudeul-ui milaeui hwagsan-eun yeogsajeog pyeong-gyun ban-yeongdoeneun geos-eulo gajeongdoenda [23].
geudeul-eun ilbanjeog-eulo jasan-eul suhaenghagi wihae gwageo gagyeog jeongboleul giban-eulo tong-gyejeog ttoneun gibon yusaseong cheugjeong-eul chaetaeghaessseubnida.
geolae jeon-e ssang-eul seontaeghasibsio. geoli jeobgeunbeob-eun ssang seontaeg-eul wihae cheoeum-eulo [8, 10, 16, 19, 35, 36]
yukeullideu jegob-ui habgwa gat-eun chaetaeg doen wongyeog cheugjeong hangmog
yeongyeol-eul modellinghagiwihan gagyeog sigyeyeol-ui geoli (SSD)
du jasan sai. geuleonai modelfree bangbeob-ui isangjeog-in jasan ssang-eun jeonghwaghi dong-ilhan du jasan-i doel geos-eulo yesangdoeeossseubnida.
yeogsajeog sigan-ui gagyeog idong-eun gagyeog hwagsan-ui byeondong-eobs-i geolae gihoega eobs-seubnida. ttohandoiss-eossseubnida
bangbeob [5, 7, 12, 15, 27, 38, 39, 46] Engle-Granger gongjeogbun-eul giban-eulo hubo ssang-ui jeonmaeleul jigjeob modellinghaneun bangbeob [5]
teseuteu, 2ui gagyeog silijeuleul sayonghayeo seonhyeong hoegwileul suhaenghabnida.
jasan mich janchaga gojeong doel geos-eulo gidaehabnida., extra_data=""{'confiden..."")",,
0,"Process mining algorithm’s performance is traditionally measured by how well it
achieves pareto-optimality of the mined model in terms of various properties such
as fitness and precision with respect to the available event log. The algorithm
has an additional goal of achieving generalization on future process instances.
In many practical settings, the target search space of models is quite large for
an exhaustive search; therefore, process mining algorithms, enforce a specific
representational bias to to make a trade-off (e.g. between higher fitness and
lower precision). In many real-world settings, process behaviour is not completely
captured or available for mining in the event logs [3] as these logs are often
noisy and incomplete. Process discovery algorithms when applied to real-world
complex event logs often produce either noisy or incomprehensible models that
either poorly fit the event log (low fitness) or over-generalize it (low precision or
low generalization) ","프로세스 마이닝 알고리즘의 성능은 일반적으로 이용가능한 이벤트 로그에 대한 적합성과 정밀성과 같은 다양한 속성의 측면에서 마이닝된 모델의 비교-최적성을 얼마나 잘 달성하는지에 의해 측정되며, 알고리즘은 미래의 프로세스 인스턴스에 대한 일반화를 달성하기 위한 추가 목표를 가지며, 많은 실제 설정에서는 모델의 타겟 검색 공간이 상당히 크므로, 프로세스 마이닝 알고리즘은 특정 표현 편의(예를 들어, 더 높은 적합성과 더 낮은 정밀성 사이에서 트레이드-오프)를 행한다. 프로세스 발견 알고리즘은 실제 복잡한 이벤트 로그에 적용될 때 종종 이벤트 로그(저 적합성)에 부하(저 적합성) 또는 과대-일반화(저 정밀성 또는 저 일반화)하는 시끄럽거나 불명확한 모델(저 적합성)을 생성한다.","Translated(src=en, dest=ko, text=프로세스 마이닝 알고리즘의 성능은 전통적으로 얼마나 잘 측정됩니다.
다양한 속성의 관점에서 채굴 된 모델의 파레토 최적화를 달성합니다.
사용 가능한 이벤트 로그와 관련하여 체력과 정밀도로.알고리즘
미래의 프로세스 인스턴스에서 일반화를 달성하는 추가 목표가 있습니다.
많은 실용적인 설정에서 모델의 대상 검색 공간은 매우 큽니다.
철저한 검색;따라서 프로세스 마이닝 알고리즘은 구체적으로 시행됩니다
트레이드 오프를 만드는 대표적 편견 (예 : 더 높은 체력과
더 낮은 정밀도).많은 실제 설정에서 프로세스 동작은 완전히
이 로그가 종종
시끄럽고 불완전합니다.실제 전 세계에 적용될 때 프로세스 발견 알고리즘
복잡한 이벤트 로그는 종종 시끄럽거나 이해할 수없는 모델을 생성합니다.
이벤트 로그 (낮은 피트니스)에 잘 맞지 않거나 과도한 일반화 (낮은 정밀도 또는
낮은 일반화), pronunciation=peuloseseu maining algolijeum-ui seongneung-eun jeontongjeog-eulo eolmana jal cheugjeongdoebnida.
dayanghan sogseong-ui gwanjeom-eseo chaegul doen model-ui paleto choejeoghwaleul dalseonghabnida.
sayong ganeunghan ibenteu logeuwa gwanlyeonhayeo chelyeoggwa jeongmildolo. algolijeum
milaeui peuloseseu inseuteonseueseo ilbanhwaleul dalseonghaneun chuga mogpyoga issseubnida.
manh-eun sil-yongjeog-in seoljeong-eseo model-ui daesang geomsaeg gong-gan-eun maeu keubnida.
cheoljeohan geomsaeg; ttalaseo peuloseseu maining algolijeum-eun guchejeog-eulo sihaengdoebnida
teuleideu opeuleul mandeuneun daepyojeog pyeongyeon (ye : deo nop-eun chelyeoggwa
deo naj-eun jeongmildo). manh-eun silje seoljeong-eseo peuloseseu dongjag-eun wanjeonhi
i logeuga jongjong
sikkeuleobgo bul-wanjeonhabnida. silje jeon segyee jeog-yongdoel ttae peuloseseu balgyeon algolijeum
bogjabhan ibenteu logeuneun jongjong sikkeuleobgeona ihaehal sueobsneun model-eul saengseonghabnida.
ibenteu logeu (naj-eun piteuniseu)e jal maj-ji anhgeona gwadohan ilbanhwa (naj-eun jeongmildo ttoneun
naj-eun ilbanhwa), extra_data=""{'confiden..."")",,
1,"Running Example - Sepsis Patient Administration: At a high-level, medical diagnosis process consists of gathering data, classifying and diagnosing a
specific problem and suggesting a particular course of treatment. The notion of
a clinical process (often also referred to as a careflow) underpins the practice of
medicine. Clinical pathways are care plans that attempt to standardise clinical
or medical treatment processes. The process data representing such pathways,
is often recorded in hospital information systems and clinical data warehouses.
To illustrate the workings of our framework, let us consider an example of sepsis
treatment careflow, where clinicians face complicated decision problems during
the patient treatment process in an emergency room. In practice, sepsis treatment is highly knowledge-driven and consists of predictable and unpredictable
elements. As the process evolves, knowledge workers (team of doctors) involved
in the process make decisions based on clinical observations and patient data
that is being constantly updated","Running Example - Sepsis Patient Administration: 고차원에서 의학적 진단 프로세스는 데이터를 수집하고 특정 문제를 분류 및 진단하고 특정 치료 코스를 제시하는 것으로 구성되며, 임상 프로세스(일명 케어플로우)라는 개념이 의학의 관행을 뒷받침한다. 임상 프로세스들은 임상 또는 의학적 치료 프로세스를 표준화하고자 하는 병원 정보 시스템과 임상 데이터 웨어하우스에 기록되는 프로세스 데이터를 대표한다. 임상 의사들이 응급실에서 환자 치료 프로세스에서 복잡한 결정 문제에 직면하는 Sepsis 치료 케어플로우의 일례를 생각해보자. 실무에서 패혈증 치료는 지식 중심이 높고 예측 불가능한 요소와 예측 불가능한 요소로 구성되어 있으며, 프로세스 진화에 참여하는 지식 근로자(의사팀)는 임상 관찰과 끊임없이 업데이트되고 있는 환자 데이터를 기반으로 의사 결정을 한다.","Translated(src=en, dest=ko, text=실행 예 - 패혈증 환자 투여 : 높은 수준에서 의료 진단 과정은 데이터 수집, 분류 및 진단으로 구성됩니다.
특정 문제 및 특정 치료 과정을 제안합니다.의 개념
임상 과정 (종종 간호 흐름이라고도 함)은
약.임상 경로는 임상을 표준화하려는 치료 계획입니다.
또는 치료 과정.이러한 경로를 나타내는 프로세스 데이터,
종종 병원 정보 시스템 및 임상 데이터웨어 하우스에 기록됩니다.
프레임 워크의 작업을 설명하려면 패혈증의 예를 고려해 봅시다
임상의가 복잡한 의사 결정 문제에 직면하는 치료 치료 흐름
응급실에서의 환자 치료 과정.실제로 패혈증 치료는 지식 중심이며 예측 가능하고 예측할 수있는 것으로 구성됩니다.
집단.프로세스가 발전함에 따라 지식 근로자 (의사 팀)가 관련
이 과정에서 임상 관찰 및 환자 데이터를 기반으로 결정을 내립니다.
그것은 끊임없이 업데이트되고 있습니다, pronunciation=silhaeng ye - paehyeoljeung hwanja tuyeo : nop-eun sujun-eseo uilyo jindan gwajeong-eun deiteo sujib, bunlyu mich jindan-eulo guseongdoebnida.
teugjeong munje mich teugjeong chilyo gwajeong-eul jeanhabnida. ui gaenyeom
imsang gwajeong (jongjong ganho heuleum-ilagodo ham)eun
yag. imsang gyeongloneun imsang-eul pyojunhwahalyeoneun chilyo gyehoeg-ibnida.
ttoneun chilyo gwajeong. ileohan gyeongloleul natanaeneun peuloseseu deiteo,
jongjong byeong-won jeongbo siseutem mich imsang deiteoweeo hauseue gilogdoebnida.
peuleim wokeuui jag-eob-eul seolmyeonghalyeomyeon paehyeoljeung-ui yeleul golyeohae bobsida
imsang-uiga bogjabhan uisa gyeoljeong munjee jigmyeonhaneun chilyo chilyo heuleum
eung-geubsil-eseoui hwanja chilyo gwajeong. siljelo paehyeoljeung chilyoneun jisig jungsim-imyeo yecheug ganeunghago yecheughal su-issneun geos-eulo guseongdoebnida.
jibdan. peuloseseuga baljeonham-e ttala jisig geunloja (uisa tim)ga gwanlyeon
i gwajeong-eseo imsang gwanchal mich hwanja deiteoleul giban-eulo gyeoljeong-eul naelibnida.
geugeos-eun kkeunh-im-eobs-i eobdeiteudoego issseubnida, extra_data=""{'confiden..."")",,
2,"Inverse Reinforcement Learning and Offline Reinforcement Learning. RLHF, IRL and offline
learning are all approaches that can be used to incorporate human preferences or expertise into the
decision-making process of an agent. However, they differ in the way that they use human input to guide
the agent’s behavior. In IRL and imitation learning, we only observe an expert’s behavior and would
like to infer the expert’s preferences or goals (Ng et al., 2000; Abbeel and Ng, 2004; Ziebart et al., 2008;
Ramachandran and Amir, 2007; Neu and Szepesv´ari, 2009; Ho and Ermon, 2016; Florence et al., 2022;
Hussein et al., 2017). In offline learning, we directly observe the cardinal rewards for the state. But the
actions are likely to be sub-optimal. In RLHF, we observe ordinal comparisons between pairs or a set of
actions. In one of the popular IRL frameworks, max-entropy IRL (Ziebart et al., 2008), it is also assumed
that human choice follows a PL model. We unify the problem of RLHF and max-entropy IRL, and provide
the first sample complexity analysis for both problems.","역강화학습(Inverse Reinforcement Learning)과 Offline Reinforcement Learning. RLHF, IRL, Offline learning 모두 인간의 선호나 전문성을 에이전트의 의사결정 과정에 접목하는 방법으로 사용될 수 있지만, IRL과 모방학습에서는 전문가의 행동만을 관찰하고 전문가의 선호나 목표를 추론하는 방식이 다르다(Ng et al., 2000; Abbeel and Ng, 2004; Ziebart et al., 2008; Ramachandran and Amir, 2007; Neu and Szepesv ́ari, 2009; Ho and Ermon, 2016; Florence et al., 2022; Hussein et al., 2017). 하지만 행동들은 하위 최적일 가능성이 높다. RLHF에서는 쌍들 또는 행동들의 집합 간의 평균 비교를 관찰하며(Ziebart et al., 2008), 대중적인 IRL 프레임워크 중 하나인 max-entropy IRL에서도 인간의 선택이 PL 모형을 따른다고 가정하고 RLHF와 max-entropy IRL의 문제를 일원화하고 두 문제에 대한 첫 번째 표본 복잡성 분석을 제공한다.","Translated(src=en, dest=ko, text=역 강화 학습 및 오프라인 강화 학습.RLHF, IRL 및 오프라인
학습은 모두 인간의 선호 또는 전문 지식을
에이전트의 의사 결정 과정.그러나 그들은 인간의 입력을 사용하여 안내하는 방식이 다릅니다.
에이전트의 행동.IRL 및 모방 학습에서 우리는 전문가의 행동 만 관찰하며
전문가의 선호도 또는 목표를 추론하고 싶습니다 (Ng et al., 2000; Abbeel and Ng, 2004; Ziebart et al., 2008;
Ramachandran and Amir, 2007;Neu and Szepesv´ari, 2009;Ho and Ermon, 2016;Florence et al., 2022;
Hussein et al., 2017).오프라인 학습에서 우리는 국가에 대한 기본 보상을 직접 관찰합니다.하지만
행동은 차선 일 가능성이 높습니다.RLHF에서 우리는 쌍 또는 세트 사이의 서수 비교를 관찰합니다.
행위.인기있는 IRL 프레임 워크 중 하나 인 Max-Entropy IRL (Ziebart et al., 2008)에서도 가정됩니다.
그 인간의 선택은 PL 모델을 따릅니다.우리는 RLHF와 Max-Entropy IRL의 문제를 통일하고
두 문제 모두에 대한 첫 번째 샘플 복잡성 분석., pronunciation=yeog ganghwa hagseub mich opeulain ganghwa hagseub. RLHF, IRL mich opeulain
hagseub-eun modu ingan-ui seonho ttoneun jeonmun jisig-eul
eijeonteuui uisa gyeoljeong gwajeong. geuleona geudeul-eun ingan-ui iblyeog-eul sayonghayeo annaehaneun bangsig-i daleubnida.
eijeonteuui haengdong. IRL mich mobang hagseub-eseo ulineun jeonmungaui haengdong man gwanchalhamyeo
jeonmungaui seonhodo ttoneun mogpyoleul chulonhago sipseubnida (Ng et al., 2000; Abbeel and Ng, 2004; Ziebart et al., 2008;
Ramachandran and Amir, 2007; Neu and Szepesv´ari, 2009; Ho and Ermon, 2016; Florence et al., 2022;
Hussein et al., 2017). opeulain hagseub-eseo ulineun gugga-e daehan gibon bosang-eul jigjeob gwanchalhabnida. hajiman
haengdong-eun chaseon il ganeungseong-i nopseubnida. RLHFeseo ulineun ssang ttoneun seteu saiui seosu bigyoleul gwanchalhabnida.
haeng-wi. ingiissneun IRL peuleim wokeu jung hana in Max-Entropy IRL (Ziebart et al., 2008)eseodo gajeongdoebnida.
geu ingan-ui seontaeg-eun PL model-eul ttaleubnida. ulineun RLHFwa Max-Entropy IRLui munjeleul tong-ilhago
du munje modue daehan cheos beonjjae saempeul bogjabseong bunseog., extra_data=""{'confiden..."")",,
3,"We consider a finite-horizon MDP described by a tuple M = (S, A, H, P, r, ρ), where S is a (possibly
infinite) state space, A is a (possibly infinite) action space, H is the horizon length, P : S × A 7→ ∆(S)
is a probability transition matrix, R : S × A 7→ ∆([0, 1]) encodes a family of reward distributions with
r : S × A 7→ [0, 1] as the expected reward function, ρ : S 7→ ∆(S) is the initial state distribution. Upon
executing action a from state s, the agent receives a deterministic reward r(s, a) and transits to the next
state s
0 with probability P(s
0
|s, a). The MDP transits to an absorbing termination state with zero reward
at step H. When H = 1 and there is no transition, the model reduces to the contextual bandit problem.
A stationary deterministic policy π : S 7→ A is a function that maps a state to an action. Correspondingly,
the value function V
π
: S 7→ R of the policy π is defined as the expected sum of rewards starting at state s
and following policy π.","우리는 튜플 M=(S, A, H, P, r, ρ)에 의해 기술된 유한 지평 MDP를 고려하는데, S는 (가능하게는 무한) 상태 공간, A는 (가능하게는 무한) 행동 공간, H는 지평 길이, P : S × A 7→ ∆(S)는 확률 전이 행렬, R : S × A 7→ ∆([0, 1])는 기대 보상 함수로 reward 분포를 가정한다. 상태 s로부터 액션 a를 실행할 때, 에이전트는 결정론적 보상 r(s, a)을 받고 확률 P(s 0 |s, a)로 다음 상태 s 0으로 transit하고, MDP는 단계 H에서 0의 보상으로 흡수 종결 상태로 transit할 때, 상태가 고정적 결정론적 정책 π : S 7→ A는 상태를 액션에 맵핑하는 함수이며, 대응하여 정책 π의 가치 함수 V π : S 7→ R은 상태 s에서 시작하여 정책 π의 기대 총합으로 정의된다.","Translated(src=en, dest=ko, text=우리는 튜플 m = (s, a, h, p, r, ρ)로 기술 된 유한 문서 MDP를 고려합니다.
무한) 상태 공간, a는 (아마도 무한) 동작 공간, h는 수평선 길이, p : s × a 7 → ∆ (들)입니다.
확률 전이 행렬, R : S × A 7 → ∆ ([0, 1])는 보상 분포 패밀리를 인코딩합니다.
R : S × A 7 → [0, 1] 예상 보상 함수로서 ρ : S 7 → ∆ (S)는 초기 상태 분포입니다.에
상태 S에서 행동 A를 실행하는 대리인은 결정 론적 보상 R (S, A)를 받고 다음을 이송합니다.
상태 s
확률 p (s
0
| s, a).MDP는 보상이없는 흡수 종단 상태로 이동합니다.
H = 1과 전환이 없을 때 H 단계에서 모델은 상황에 맞는 산적 문제로 줄어 듭니다.
고정 된 결정 론적 정책 π : s 7 → a는 상태를 행동에 맵핑하는 함수입니다.이에 따라
값 함수 v
π
: 정책 π의 s 7 → r은 State S에서 시작하는 예상 보상 합으로 정의됩니다.
그리고 다음 정책 π., pronunciation=ulineun tyupeul m = (s, a, h, p, r, r)lo gisul doen yuhan munseo MDPleul golyeohabnida.
muhan) sangtae gong-gan, aneun (amado muhan) dongjag gong-gan, hneun supyeongseon gil-i, p : s × a 7 → ∆ (deul)ibnida.
hwaglyul jeon-i haenglyeol, R : S × A 7 → ∆ ([0, 1])neun bosang bunpo paemillileul inkodinghabnida.
R : S × A 7 → [0, 1] yesang bosang hamsuloseo r : S 7 → ∆ (S)neun chogi sangtae bunpoibnida. e
sangtae Seseo haengdong Aleul silhaenghaneun daeliin-eun gyeoljeong lonjeog bosang R (S, A)leul badgo da-eum-eul isonghabnida.
sangtae s
hwaglyul p (s
0
| s, a). MDPneun bosang-ieobsneun heubsu jongdan sangtaelo idonghabnida.
H = 1gwa jeonhwan-i eobs-eul ttae H dangyeeseo model-eun sanghwang-e majneun sanjeog munjelo jul-eo deubnida.
gojeong doen gyeoljeong lonjeog jeongchaeg p : s 7 → aneun sangtaeleul haengdong-e maebpinghaneun hamsu-ibnida. ie ttala
gabs hamsu v
p
: jeongchaeg pui s 7 → reun State Seseo sijaghaneun yesang bosang hab-eulo jeong-uidoebnida.
geuligo da-eum jeongchaeg p., extra_data=""{'confiden..."")",,
4,"The previous studies did not focus on the influence of experiences stored in the buffer, although such influence
information is useful. Many of the aforementioned buffer
properties (e.g. on-policyness and optimality) are shaped
by the experiences stored in the buffer; thus, experiences
should also affect performance. What if we could quantify
the influence of the experiences? Such information could
be used for many purposes, e.g. experience cleansing and
analysis. Experience cleansing: We could improve the
performance of an agent by removing negatively influential
experiences from the buffer or save computational memory by removing non-influential experiences","선행 연구들은 그러한 영향력 정보가 유용하나, 위와 같은 버퍼 속성(예: 온-정책성과 최적성)의 대부분은 버퍼에 저장된 경험에 의해 형성되기 때문에, 경험의 영향력도 성과에 영향을 미치게 되므로, 경험을 정량화할 수 있다면, 경험 정화와 분석, 경험 정화: 부정적으로 영향을 미치는 경험을 버퍼에서 제거함으로써 에이전트의 성능을 향상시키거나 비영향적인 경험을 제거함으로써 계산 기억을 절약할 수 있다.","Translated(src=en, dest=ko, text=이전 연구는 버퍼에 저장된 경험의 영향에 초점을 두지 않았지만 그러한 영향은
정보가 유용합니다.앞서 언급 한 많은 버퍼
특성 (예 : 정책 및 최적 성)은 형성됩니다
버퍼에 저장된 경험에 의해;따라서 경험
성능에도 영향을 미칩니다.우리가 정량화 할 수 있다면 어떨까요?
경험의 영향?그러한 정보는 할 수 있습니다
많은 목적으로 사용됩니다 (예 :클렌징을 경험하고
분석.청소 경험 : 우리는 개선 할 수 있습니다
부정적인 영향력을 제거하여 에이전트의 성능
비 유파 된 경험을 제거하여 버퍼 또는 컴퓨터 메모리 저장 경험, pronunciation=ijeon yeonguneun beopeoe jeojangdoen gyeongheom-ui yeonghyang-e chojeom-eul duji anh-assjiman geuleohan yeonghyang-eun
jeongboga yuyonghabnida. apseo eongeub han manh-eun beopeo
teugseong (ye : jeongchaeg mich choejeog seong)eun hyeongseongdoebnida
beopeoe jeojangdoen gyeongheom-e uihae; ttalaseo gyeongheom
seongneung-edo yeonghyang-eul michibnida. uliga jeonglyanghwa hal su issdamyeon eotteolkkayo?
gyeongheom-ui yeonghyang? geuleohan jeongboneun hal su issseubnida
manh-eun mogjeog-eulo sayongdoebnida (ye : keullenjing-eul gyeongheomhago
bunseog. cheongso gyeongheom : ulineun gaeseon hal su issseubnida
bujeongjeog-in yeonghyanglyeog-eul jegeohayeo eijeonteuui seongneung
bi yupa doen gyeongheom-eul jegeohayeo beopeo ttoneun keompyuteo memoli jeojang gyeongheom, extra_data=""{'confiden..."")",,
5,"In practice, we need a method for efficiently estimating the
influence of experiences (Eq. 3). We can estimate the influence by comparing agents that are prepared (or retrained)
for each possible deletion of an experience. However, this
requires training an agent by repeatedly executing PI (Eqs.1
and 2) for each deletion case. This is computationally expensive and is infeasible for large D. Therefore, we need a
new method for efficiently estimating the influence of experiences without such agent comparison. We present such
a method in the following section.","실무에서는 경험의 영향력을 효율적으로 추정하는 방법(식 3). 경험의 가능한 삭제 마다 준비(또는 재학습)된 에이전트를 비교하여 영향력을 추정할 수 있지만, 이는 삭제 케이스마다 반복적으로 PI(식1, 2)를 실행하여 에이전트를 훈련시키는 것이 계산적으로 비용이 많이 들고 D에 대해서는 불가능하기 때문에 이와 같은 에이전트 비교 없이 경험의 영향력을 효율적으로 추정하는 새로운 방법이 필요하다. 이와 같은 방법을 제시한다.","Translated(src=en, dest=ko, text=실제로, 우리는 효율적으로 추정하는 방법이 필요합니다.
경험의 영향 (식 3).준비된 (또는 재교육)를 비교하여 영향을 추정 할 수 있습니다.
경험의 가능한 삭제에 대해.그러나 이것
PI를 반복적으로 실행하여 에이전트를 훈련해야합니다 (식 1
및 2) 각 삭제 사례에 대해.이것은 계산적으로 비싸고 큰 D에게는 불가능합니다. 따라서 우리는 필요합니다.
그러한 에이전트 비교없이 경험의 영향을 효율적으로 추정하는 새로운 방법.우리는 그런 것을 발표합니다
다음 섹션의 메소드., pronunciation=siljelo, ulineun hyoyuljeog-eulo chujeonghaneun bangbeob-i pil-yohabnida.
gyeongheom-ui yeonghyang (sig 3). junbidoen (ttoneun jaegyoyug)leul bigyohayeo yeonghyang-eul chujeong hal su issseubnida.
gyeongheom-ui ganeunghan sagjee daehae. geuleona igeos
PIleul banbogjeog-eulo silhaenghayeo eijeonteuleul hunlyeonhaeyahabnida (sig 1
mich 2) gag sagje salyee daehae. igeos-eun gyesanjeog-eulo bissago keun Degeneun bulganeunghabnida. ttalaseo ulineun pil-yohabnida.
geuleohan eijeonteu bigyoeobs-i gyeongheom-ui yeonghyang-eul hyoyuljeog-eulo chujeonghaneun saeloun bangbeob. ulineun geuleon geos-eul balpyohabnida
da-eum segsyeon-ui mesodeu., extra_data=""{'confiden..."")",,
6,"The problem we consider is one faced by a fund manager who has just taken in a large amount
of new capital. This capital needs to be integrated into the portfolio but transaction costs caused
by large bid/ask spreads make it extremely inefficient to directly invest the entire amount immediately (i.e., the typical buy-and-hold strategy is sub-optimal). A more efficient way is to invest
the new funds according to a solution to a dynamic mean-variance optimization that includes a
quadratic penalty on trade size. Optimal execution of large orders was formulated as a meanvariance optimization with penalization on trades in Almgren and Chriss [1], and a multi-asset
version of this problem was studied in Garleanu and Pedersen [ ˆ 2]. However in practice, many assets have heteroskedasticity, and therefore it is interesting to consider mean-variance preferences
in the setting of dynamic covariance matrices given by a multi-variate GARCH (MGARCH)
model. In addition, when volatility spikes, there is an increase in price impact (Capponi and
Cont [3]). This negative correlation between price and volatility was also found by Black [4].
Mantalos et al. [5] modeled such volatility by fitting skewness in the ARCH model. This work
assumes that the penalty on trading depends on the instantaneous value of the covariance matrix
(e.g., the condition number of the covariance matrix) to describe such price volatility. The contrary movement between degrees of freedom in covariance matrices and overall market volatility
is highlighted in Avellaneda and Lee [6] and also touched upon in Laloux et al. [7]. This heteroskedastic problem is a linear-quadratic program, but with the added feature of non-constant
coefficients that depend on the MGARCH process.","우리가 고려하는 문제는 이제 막 대량의 신규 자본을 투자한 펀드 매니저가 직면한 문제인데, 이 자본은 포트폴리오에 통합될 필요가 있지만, 대량의 입찰/애스크 스프레드에 따른 거래 비용으로 인해 전체 금액을 바로 투자하는 것은 매우 비효율적(즉, 전형적인 매수-매도 전략은 하위 최적)이며, 보다 효율적인 방법은 대량의 주문 실행을 동태적 평균-분산 최적화(dynamic mean-variance optimization)에 따른다. 그러나 실무에서는 많은 자산이 이분산성을 가지며, 따라서 다변량 GARCH(MGARCH) 모형에 의해 주어진 동태적 공분산 행렬의 설정에서 평균-분산 선호를 고려하는 것이 흥미롭고, 또한 이러한 가격 변동성은 변동성이 급증할 때(Capponi and Cont [3]) 이러한 가격과 변동성 간의 음의 상관관계를 Black [4]. Mantalos et al. [5]는 ARCH 모형을 이용하여 이러한 변동성을 설명하기 위해 공분산 행렬의 순간적인 값(예컨대 공분산 행렬의 조건수)에 따라 거래에 대한 과징금이 달라진다고 가정하였다. 공분산 행렬에서의 자유도와 전체 시장 변동성 사이의 상반된 움직임은 Avellaneda and Lee [6]에서 강조되고, Laloux et al. [7]에서도 터치된 바 있는데, 이러한 이분산 문제는 linear-quadratic 프로그램이지만 MGARCH 프로세스에 의존하는 비정상적인 계수라는 특징을 갖는다.","Translated(src=en, dest=ko, text=우리가 생각하는 문제는 방금 방금 취한 펀드 매니저가 직면 한 것입니다.
새로운 수도.이 자본은 포트폴리오에 통합되어야하지만 거래 비용이 발생했습니다.
대규모 입찰/Ask 스프레드는 전체 금액을 즉시 직접 투자하는 것이 매우 비효율적입니다 (즉, 일반적인 구매 및 보유 전략은 차선책입니다).보다 효율적인 방법은 투자하는 것입니다
새로운 자금은
무역 규모에 대한 2 차 페널티.대규모 주문의 최적 실행은 Almgren과 Chriss [1]의 거래에 대한 처벌을 통한 평균 분산 최적화로 공식화되었습니다.
이 문제의 버전은 Garleanu와 Pedersen에서 연구되었습니다 [ˆ 2].그러나 실제로 많은 자산이 이종성을 가지고 있으므로 평균 분산 선호도를 고려하는 것이 흥미 롭습니다.
다중 변량 Garch (mgarch)에 의해 주어진 동적 공분산 행렬의 설정에서
모델.또한 변동성이 급증 할 때 가격 영향이 증가합니다 (Capponi 및
계속 [3]).가격과 변동성 사이의 이러한 부정적인 상관 관계는 또한 검은 색에 의해 발견되었다 [4].
Mantalos et al.[5]는 아치 모델에서 왜곡을 피함으로써 그러한 변동성을 모델링했다.이 일
거래에 대한 벌칙은 공분산 행렬의 순간 가치에 달려 있다고 가정합니다.
(예 : 공분산 행렬의 조건 수) 그러한 가격 변동성을 설명하는 것.공분산 행렬에서 자유도 사이의 반대 운동과 전반적인 시장 변동성
Avellaneda와 Lee [6]에서 강조되며 Laloux et al.[7].이 heteroskedastic 문제
mgarch 프로세스에 의존하는 계수., pronunciation=uliga saeng-gaghaneun munjeneun bang-geum bang-geum chwihan peondeu maenijeoga jigmyeon han geos-ibnida.
saeloun sudo. i jabon-eun poteupollio-e tonghabdoeeoyahajiman geolae biyong-i balsaenghaessseubnida.
daegyumo ibchal/Ask seupeuledeuneun jeonche geum-aeg-eul jeugsi jigjeob tujahaneun geos-i maeu bihyoyuljeog-ibnida (jeug, ilbanjeog-in gumae mich boyu jeonlyag-eun chaseonchaeg-ibnida). boda hyoyuljeog-in bangbeob-eun tujahaneun geos-ibnida
saeloun jageum-eun
muyeog gyumo-e daehan 2 cha peneolti. daegyumo jumun-ui choejeog silhaeng-eun Almgrengwa Chriss [1]ui geolaee daehan cheobeol-eul tonghan pyeong-gyun bunsan choejeoghwalo gongsighwadoeeossseubnida.
i munje-ui beojeon-eun Garleanuwa Pedersen-eseo yeongudoeeossseubnida [ˆ 2]. geuleona siljelo manh-eun jasan-i ijongseong-eul gajigo iss-eumeulo pyeong-gyun bunsan seonhodoleul golyeohaneun geos-i heungmi lobseubnida.
dajung byeonlyang Garch (mgarch)e uihae jueojin dongjeog gongbunsan haenglyeol-ui seoljeong-eseo
model. ttohan byeondongseong-i geubjeung hal ttae gagyeog yeonghyang-i jeung-gahabnida (Capponi mich
gyesog [3]). gagyeoggwa byeondongseong saiui ileohan bujeongjeog-in sang-gwan gwangyeneun ttohan geom-eun saeg-e uihae balgyeondoeeossda [4].
Mantalos et al. [5]neun achi model-eseo waegog-eul piham-eulosseo geuleohan byeondongseong-eul modellinghaessda. i il
geolaee daehan beolchig-eun gongbunsan haenglyeol-ui sungan gachie dallyeo issdago gajeonghabnida.
(ye : gongbunsan haenglyeol-ui jogeon su) geuleohan gagyeog byeondongseong-eul seolmyeonghaneun geos. gongbunsan haenglyeol-eseo jayudo saiui bandae undong-gwa jeonbanjeog-in sijang byeondongseong
Avellanedawa Lee [6]eseo gangjodoemyeo Laloux et al. [7]. i heteroskedastic munje
mgarch peuloseseue uijonhaneun gyesu., extra_data=""{'confiden..."")",,
7,"Other machine learning applications in finance include Sirignano and Spiliopoulos [15] where
stochastic gradient descent (SGD) with deep NN architecture is used for computing prices of
American options on large baskets of stocks, and in Han et al. [16] where an RL approach is
used to numerically solve high-dimensional backward stochastic differential equations related
to finance. In Fischer and Krauss [17], the authors utilized an LSTM network for predicting
the price movement with daily S&P500 data1
. The performance of the LSTM is mixed during
different periods. The short-term trend prediction in the price movement on NASDAQ by the
deep network was studied by Namdari and Durrani [18]. The authors utilized features from both
fundamental and technical analysis as the network input. Kim et al. [19] used a graph network
to predict the stock price movement on S&P500 data. In Liang et al. [20], they demonstrate
how adversarial learning methods can be used to automate trading in stocks. General methods from control theory have been applied for optimal trading decisions in Barmish and Primbs
[21], Malekpour et al. [22]. The effects of transaction costs and liquidity are well-studied (Almgren and Chriss [1], Chandra and Papanicolaou [23], Rogers and Singh [24]). In particular, the
“aim portfolio” description given in Garleanu and Pedersen [ ˆ 2] has been a key result for the
management of large funds. The discussed works are based on supervised learning. Therefore, the non-supervised learning approaches should also be studied as they may address more
complicated problems.","금융 분야의 다른 기계학습 응용으로는 Sirignano와 Spiliopoulos [15]에서 깊은 NN 아키텍처를 갖는 확률적 기울기 하강(SGD)을 대형 주식 바스켓에 대한 미국 옵션의 가격을 계산하는데 사용하고, Han et al. [16]에서 저자는 금융 관련 고차원적인 후방확률적 차분 방정식을 수치적으로 풀기 위해 LSTM 네트워크를 활용하였으며, Namdari and Durrani [18]에서 LSTM의 딥 네트워크에 의한 가격변동의 단기적 추세 예측 성능을 혼합하였다. 저자들은 네트워크 입력으로 기본적인 분석과 기술적인 분석에서 나온 특징을 활용하였으며, Kim et al. [19]은 S&P500 데이터에서 주가의 움직임을 예측하기 위해 그래프 네트워크를 활용하였으며, Liang et al. [20]에서는 주식에서 적대적 학습 방법이 어떻게 매매를 자동화할 수 있는지, Barmish and Primbs [21], Malekpour et al. [22]에서 통제 이론의 일반적인 방법들을 적용하여 거래 비용과 유동성의 효과를 잘 연구하였다(Almgren and Chriss [1], Chandra and Papanicolaou [23], Rogers and Singh [24]), Garleanu and Pedersen [ ˆ 2]에서 제시한 목표 포트폴리오에 대한 설명이 주요한 결과이다. 논의된 작품들은 지도 학습에 기반을 두고 있기 때문에, 보다 복잡한 문제를 해결할 수 있기 때문에 비지도 학습 접근법도 연구되어야 한다.","Translated(src=en, dest=ko, text=금융의 다른 기계 학습 응용 프로그램에는 Sirignano 및 Spiliopoulos [15]
깊은 NN 아키텍처를 갖춘 확률 기울기 하강 (SGD)은 가격 계산에 사용됩니다.
대형 주식 바구니 및 한 등의 미국 옵션.[16] RL 접근법이있는 곳
고차원 뒤로 확률 론적 미분 방정식을 수치 적으로 해결하는 데 사용됩니다.
자금 조달.Fischer와 Krauss [17]에서 저자는 예측을 위해 LSTM 네트워크를 사용했습니다.
매일 S & P500 Data1의 가격 이동
.LSTM의 성능은 동안 혼합됩니다
다른 기간.NASDAQ의 가격 이동에서 단기 추세 예측
Deep Network는 Namdari와 Durrani에 의해 연구되었습니다 [18].저자는 두 가지 기능을 모두 활용했습니다
네트워크 입력으로서 기본 및 기술 분석.Kim et al.[19]는 그래프 네트워크를 사용했습니다
S & P500 데이터의 주가 이동을 예측합니다.Liang et al.[20], 그들은 보여줍니다
적대 학습 방법을 사용하여 주식 거래를 자동화하는 방법.Barmish 및 Primbs의 최적 거래 결정을 위해 제어 이론의 일반적인 방법이 적용되었습니다.
[21], Malekpour et al.[22].거래 비용과 유동성의 영향은 잘 연구됩니다 (Almgren and Chriss [1], Chandra and Papanicolaou [23], Rogers and Singh [24]).특히,
Garleanu와 Pedersen [ˆ 2]에 주어진 ""AIM 포트폴리오""설명은
대규모 자금 관리.논의 된 작품은 감독 학습을 기반으로합니다.따라서 감독되지 않은 학습 접근법은 또한 더 많은 것을 다룰 수 있으므로 연구해야합니다.
복잡한 문제., pronunciation=geum-yung-ui daleun gigye hagseub eung-yong peulogeulaem-eneun Sirignano mich Spiliopoulos [15]
gip-eun NN akitegcheoleul gajchun hwaglyul giulgi hagang (SGD)eun gagyeog gyesan-e sayongdoebnida.
daehyeong jusig baguni mich han deung-ui migug obsyeon. [16] RL jeobgeunbeob-iissneun gos
gochawon dwilo hwaglyul lonjeog mibun bangjeongsig-eul suchi jeog-eulo haegyeolhaneun de sayongdoebnida.
jageum jodal. Fischerwa Krauss [17]eseo jeojaneun yecheug-eul wihae LSTM neteuwokeuleul sayonghaessseubnida.
maeil S & P500 Data1ui gagyeog idong
. LSTMui seongneung-eun dong-an honhabdoebnida
daleun gigan. NASDAQui gagyeog idong-eseo dangi chuse yecheug
Deep Networkneun Namdariwa Durranie uihae yeongudoeeossseubnida [18]. jeojaneun du gaji gineung-eul modu hwal-yonghaessseubnida
neteuwokeu iblyeog-euloseo gibon mich gisul bunseog. Kim et al. [19]neun geulaepeu neteuwokeuleul sayonghaessseubnida
S & P500 deiteoui juga idong-eul yecheughabnida. Liang et al. [20], geudeul-eun boyeojubnida
jeogdae hagseub bangbeob-eul sayonghayeo jusig geolaeleul jadonghwahaneun bangbeob. Barmish mich Primbsui choejeog geolae gyeoljeong-eul wihae jeeo ilon-ui ilbanjeog-in bangbeob-i jeog-yongdoeeossseubnida.
[21], Malekpour et al. [22]. geolae biyong-gwa yudongseong-ui yeonghyang-eun jal yeongudoebnida (Almgren and Chriss [1], Chandra and Papanicolaou [23], Rogers and Singh [24]). teughi,
Garleanuwa Pedersen [ˆ 2]e jueojin ""AIM poteupollio""seolmyeong-eun
daegyumo jageum gwanli. non-ui doen jagpum-eun gamdog hagseub-eul giban-eulohabnida. ttalaseo gamdogdoeji anh-eun hagseub jeobgeunbeob-eun ttohan deo manh-eun geos-eul dalul su iss-eumeulo yeonguhaeyahabnida.
bogjabhan munje., extra_data=""{'confiden..."")",,
8,"The utilized neural network (NN) is composed of fully connected layers. The input contains
the portfolio Xt−1 that has 11 elements, plus the covariance matrix of the dollar-returns Pt =
ΨtΣtΨt whose dimension is 11×11 and also the expected value of returns µ
>Ψt which is also
11-dimensional. Therefore, the total dimension is 143. The hidden layer size was determined
by considering both the training time and the NN performance. When using more complex NN
architecture, we observed that there was no obvious improvement in performance while the training time increased considerably. On the other hand, when using even simpler NN architectures,
we observed that the deep RL algorithm suffered from under-fitting problems. Therefore, we
utilized four hidden layers, each of which contains 400 neurons. The output of the NN corresponds to ϕ(·, ·, ·; θ) in Algorithm 1, which is 11-dimensional. The activation function is Tanh.
The architecture of the utilized neural network (NN) is shown in Table 5 in the appendix shows
the detail of the architecture.","사용된 신경망(NN)은 완전히 연결된 계층으로 구성되며, 입력은 11개의 요소를 갖는 포트폴리오 Xt−1과 11×11의 치수인 달러-수익률 Pt = ΨtΣtΨt의 공분산 행렬과 11차원인 수익률 μ >Ψt의 값을 모두 고려하여 NN 성능을 측정하였으며, 보다 복잡한 NN 아키텍처를 사용하였을 경우 성능에 뚜렷한 변화가 없는 것으로 관찰하였으며, 딥 RL 알고리즘은 값이 작아질수록 값이 작아지는 것을 관찰하였다. 따라서 4개의 은닉층을 활용하였으며, 각각 400개의 뉴런을 포함하고 있으며, 뉴런의 출력은 11차원인 Algorithm 1에서 φ(·, ·, ·; θ)에 해당하며, 활성화 함수는 Tanh이며, 활용된 뉴럴 네트워크(NN)의 아키텍처는 appendix에 표 5와 같다.","Translated(src=en, dest=ko, text=사용 된 신경망 (NN)은 완전히 연결된 층으로 구성됩니다.입력에는 포함됩니다
11 개의 요소가있는 포트폴리오 XT-1과 달러 반환의 공분산 행렬 PT =
치수가 11 × 11이고 예상 값 µ의 ψtσtψt
> ψt도 있습니다
11 차원.따라서 총 치수는 143입니다. 숨겨진 층 크기가 결정되었습니다.
훈련 시간과 NN 성능을 모두 고려함으로써.더 복잡한 NN을 사용할 때
건축, 우리는 훈련 시간이 상당히 증가하는 반면 성능이 명백한 개선이 없음을 관찰했습니다.반면에, 더 간단한 NN 아키텍처를 사용할 때
우리는 Deep RL 알고리즘이 적합한 문제로 어려움을 겪고 있음을 관찰했습니다.그러므로 우리
4 개의 숨겨진 층을 이용했으며, 각각 400 개의 뉴런이 포함되어 있습니다.NN의 출력은 11 차원 인 알고리즘 1의 ϕ (·, ·, ·; θ)에 해당합니다.활성화 함수는 TANH입니다.
사용 된 신경망 (NN)의 아키텍처는 부록 쇼의 표 5에 나와 있습니다.
건축의 세부 사항., pronunciation=sayong doen singyeongmang (NN)eun wanjeonhi yeongyeoldoen cheung-eulo guseongdoebnida. iblyeog-eneun pohamdoebnida
11 gaeui yosogaissneun poteupollio XT-1gwa dalleo banhwan-ui gongbunsan haenglyeol PT =
chisuga 11 × 11igo yesang gabs µui pststpst
> pstdo issseubnida
11 chawon. ttalaseo chong chisuneun 143ibnida. sumgyeojin cheung keugiga gyeoljeongdoeeossseubnida.
hunlyeon sigangwa NN seongneung-eul modu golyeoham-eulosseo. deo bogjabhan NNeul sayonghal ttae
geonchug, ulineun hunlyeon sigan-i sangdanghi jeung-gahaneun banmyeon seongneung-i myeongbaeghan gaeseon-i eobs-eum-eul gwanchalhaessseubnida. banmyeon-e, deo gandanhan NN akitegcheoleul sayonghal ttae
ulineun Deep RL algolijeum-i jeoghabhan munjelo eolyeoum-eul gyeokkgo iss-eum-eul gwanchalhaessseubnida. geuleomeulo uli
4 gaeui sumgyeojin cheung-eul iyonghaess-eumyeo, gaggag 400 gaeui nyuleon-i pohamdoeeo issseubnida. NNui chullyeog-eun 11 chawon in algolijeum 1ui ph (:, :, :? th)e haedanghabnida. hwalseonghwa hamsuneun TANHibnida.
sayong doen singyeongmang (NN)ui akitegcheoneun bulog syoui pyo 5e nawa issseubnida.
geonchug-ui sebu sahang., extra_data=""{'confiden..."")",,
9,"In this paper, we use deep learning models that include LSTM-based models and CNNs for GDP growth
rate forecasting of major world economies. We focus on
large economies around the world that includes developing and developed countries and use data from the past
few decades to forecast the future decade. We present a
recursive deep learning framework where the direct strategy is used for model development and recursive strategy
is used for decadal forest. We also investigate what sort
of data partitioning strategy is best for model training
and development. We further compare the performance
of deep learning models with traditional time series forecasting models (ARIMA and VAR) for different countries.
Our data includes periods of smooth development, rapid
development, and periods of financial crisis in the training
set, in order to better prepare the test set and forecast
data. We first use a direct strategy to evaluate the respective deep learning models and then use the best model for
the recursive strategy where we estimate the economic indicators first, in order to forecast the decadal GDP growth
rate. Our data features GDP along with economic indicators prior to 2019 and we forecast a decadal world economy
outlook for 2020 - 2030.","본 논문에서는 세계 주요국의 GDP 성장률 예측을 위해 LSTM 기반 모형과 CNN을 포함하는 딥러닝 모형을 사용하고, 개발도상국과 선진국을 포함하는 세계 대국을 대상으로 과거 수십년간의 데이터를 이용하여 미래 10년을 예측하고, 모형 개발에 직접적인 방법이 사용되는 재귀적 딥러닝 프레임워크를 제시하고, 모형 개발에 직접적인 방법이 사용되는 재귀적 딥러닝 모형을 이용하여 모형의 테스트 집합과 예측 자료를 보다 잘 준비하기 위해 딥러닝 모형과 전통적인 시계열 예측 모형(ARIMA와 VAR)의 성능을 비교한다. 먼저 직접적인 방법을 이용하여 각각의 딥러닝 모형을 평가한 후, 먼저 경제 지표를 먼저 추정하는 재귀 모형에 최선의 모형을 적용하여 2019년 이전의 GDP와 함께 2019년 이전의 GDP 성장률을 예측하고 2020년 - 2030년의 GDP 성장률을 예측한다.","Translated(src=en, dest=ko, text=이 논문에서는 LSTM 기반 모델과 GDP 성장을위한 CNN을 포함하는 딥 러닝 모델을 사용합니다.
주요 세계 경제의 요율 예측.우리는 집중합니다
개발 도상국 및 선진국을 포함하여 과거의 데이터를 사용하는 전 세계의 대규모 경제
미래 10 년을 예측하는 데 수십 년이 걸렸습니다.우리는 a
직접 전략이 모델 개발 및 재귀 전략에 사용되는 재귀 딥 러닝 프레임 워크
10 년 숲에 사용됩니다.우리는 또한 어떤 종류의 것을 조사합니다
데이터 파티셔닝 전략은 모델 교육에 가장 좋습니다
그리고 개발.우리는 성능을 더욱 비교합니다
다른 국가의 기존 시계열 예측 모델 (ARIMA 및 VAR)을 갖춘 딥 러닝 모델.
우리의 데이터에는 원활한 개발 기간, 빠른 기간이 포함됩니다
훈련의 개발 및 금융 위기 기간
테스트 세트와 예측을 더 잘 준비하기 위해 설정
데이터.먼저 직접 전략을 사용하여 각 딥 러닝 모델을 평가 한 다음 가장 좋은 모델을 사용합니다.
10 년 GDP 성장을 예측하기 위해 경제 지표를 먼저 추정하는 재귀 전략
비율.우리의 데이터는 2019 년 이전에 경제 지표와 함께 GDP를 특징으로하며 우리는 10 년 세상 경제를 예측합니다.
2020-2030의 전망., pronunciation=i nonmun-eseoneun LSTM giban modelgwa GDP seongjang-eul-wihan CNNeul pohamhaneun dib leoning model-eul sayonghabnida.
juyo segye gyeongje-ui yoyul yecheug. ulineun jibjunghabnida
gaebal dosang-gug mich seonjingug-eul pohamhayeo gwageoui deiteoleul sayonghaneun jeon segyeui daegyumo gyeongje
milae 10 nyeon-eul yecheughaneun de susib nyeon-i geollyeossseubnida. ulineun a
jigjeob jeonlyag-i model gaebal mich jaegwi jeonlyag-e sayongdoeneun jaegwi dib leoning peuleim wokeu
10 nyeon sup-e sayongdoebnida. ulineun ttohan eotteon jonglyuui geos-eul josahabnida
deiteo patisyeoning jeonlyag-eun model gyoyug-e gajang johseubnida
geuligo gaebal. ulineun seongneung-eul deoug bigyohabnida
daleun guggaui gijon sigyeyeol yecheug model (ARIMA mich VAR)eul gajchun dib leoning model.
uliui deiteoeneun wonhwalhan gaebal gigan, ppaleun gigan-i pohamdoebnida
hunlyeon-ui gaebal mich geum-yung wigi gigan
teseuteu seteuwa yecheug-eul deo jal junbihagi wihae seoljeong
deiteo. meonjeo jigjeob jeonlyag-eul sayonghayeo gag dib leoning model-eul pyeong-ga han da-eum gajang joh-eun model-eul sayonghabnida.
10 nyeon GDP seongjang-eul yecheughagi wihae gyeongje jipyoleul meonjeo chujeonghaneun jaegwi jeonlyag
biyul. uliui deiteoneun 2019 nyeon ijeon-e gyeongje jipyowa hamkke GDPleul teugjing-eulohamyeo ulineun 10 nyeon sesang gyeongjeleul yecheughabnida.
2020-2030ui jeonmang., extra_data=""{'confiden..."")",,
10,"ARIMA and VAR are two of the more common statistical models in time series analysis and applied for forecasting in macro-economics. ARIMA models have been used
to predict Singapore’s quarterly GDP based on monthly
external trade[1]. A modfied ARIMA model was used to
predict Irish CPI by introducing an objective penalty function methods to focus on the forecast error out-of-sample
rather than ’goodness of fit’ [80]. Sims [103] introduced the
VAR model in economics in 1980 for for macroeconomic
modelling and forecasting in order to deal with endogeneity issues. Freeman and Williams[40] used a VAR model
to analyze indicator variables that relate to policy and the
economy. They compared their model to structural equation models[51] and found a VAR is better at capturing
policy endogeneity. A decade later, Robertson et al. [96]
relied on a VAR model to predict United States GDP using
six economic indicators and found that imposing imprecise
prior constraints on VAR can lead to more accurate predictions. Abonazel et al. [2] used ARIMA model to predict
Egyptian GDP in next decade from 2019, and Salisu et
al. [99] analysed how the oil uncertainty stock affect 33
countries’ GDP and the influence between the countries
using a global VAR. Iorio et al. [55] compared France and
Germany ’s unemployment rate and GDP growth rate in
the future basic on a VAR model. ARIMA and VAR models remain widely applied in a range of econometrics and
finance applications","ARIMA 모형과 VAR 모형은 시계열 분석에서 가장 보편적인 통계 모형 중의 두 가지 모형으로 거시 경제의 예측을 위해 적용되어 왔으며, ARIMA 모형은 싱가포르의 월간 대외 교역량에 기초하여 분기별 GDP를 예측하였으며, modfied ARIMA 모형을 도입하여 아일랜드 CPI를 예측하였고, Sims[103]는 1980년 거시 경제 모형 및 예측을 위해 VAR 모형을 도입하였으며, Freeman[[40]은 정책과 경제에 관련한 지표 변수들을 분석하였다. 그들은 그들의 모형을 구조 방정식 모형[51]과 비교하였으며, VAR 모형이 정책 내생성을 포착하는 데 더 좋은 것으로 나타났다. 10년 후 Robertson et al. [96]는 6가지 경제 지표를 이용하여 VAR 모형에 의존하여 2019년부터 이집트 GDP를 예측하였으며, Salisu et al. [99]는 ARIMA 모형을 사용하여 석유 불확실성이 33개국 GDP에 미치는 영향과 글로벌 VAR 모형을 이용하여 프랑스와 독일의 미래 실업률과 GDP 성장률을 비교하였다. ARIMA 모형과 VAR 모형은 다양한 계량 경제학적 응용 및 금융 응용 분야에서 널리 적용되고 있다.","Translated(src=en, dest=ko, text=ARIMA와 VAR은 시계열 분석에서 가장 일반적인 통계 모델 중 하나이며 거시 경제학 예측에 적용됩니다.Arima 모델이 사용되었습니다
월간을 기준으로 싱가포르의 분기 별 GDP를 예측합니다
외부 무역 [1].Modfied Arima 모델이 익숙해졌습니다
예측 오류 오류에 초점을 맞추기 위해 객관적인 페널티 함수 방법을 도입하여 아일랜드 CPI를 예측하십시오.
'적합의 선함'보다는 [80].Sims [103]은 다음을 소개했다
거시 경제를위한 1980 년 경제학의 VAR 모델
내성 문제를 처리하기위한 모델링 및 예측.Freeman과 Williams [40]은 VAR 모델을 사용했습니다.
정책과 관련된 지표 변수를 분석합니다.
경제.그들은 그들의 모델을 구조 방정식 모델과 비교했으며 [51] var가 캡처에 더 나은 것을 발견했습니다.
정책 내생성.10 년 후, Robertson et al.[96]
미국 GDP를 사용하여 VAR 모델에 의존했습니다.
6 가지 경제 지표가 부정확 한 것으로 나타났습니다
VAR의 사전 제약은보다 정확한 예측으로 이어질 수 있습니다.Abonazel et al.[2]는 ARIMA 모델을 사용하여 예측했습니다
2019 년부터 향후 10 년간 이집트 GDP 및 Salisu ET
알.[99] 석유 불확실성 재고가 33에 어떤 영향을 미치는지 분석했습니다.
국가의 GDP와 국가 간의 영향
글로벌 var를 사용합니다.Iorio et al.[55] 프랑스를 비교했다
독일의 실업률과 GDP 성장률
VAR 모델의 미래 기본.ARIMA 및 VAR 모델은 다양한 계량 경제학에 널리 적용되고 있습니다.
금융 응용 프로그램, pronunciation=ARIMAwa VAReun sigyeyeol bunseog-eseo gajang ilbanjeog-in tong-gye model jung hanaimyeo geosi gyeongjehag yecheug-e jeog-yongdoebnida. Arima model-i sayongdoeeossseubnida
wolgan-eul gijun-eulo sing-gapoleuui bungi byeol GDPleul yecheughabnida
oebu muyeog [1]. Modfied Arima model-i igsughaejyeossseubnida
yecheug olyu olyue chojeom-eul majchugi wihae gaeggwanjeog-in peneolti hamsu bangbeob-eul doibhayeo aillaendeu CPIleul yecheughasibsio.
'jeoghab-ui seonham'bodaneun [80]. Sims [103]eun da-eum-eul sogaehaessda
geosi gyeongjeleul-wihan 1980 nyeon gyeongjehag-ui VAR model
naeseong munjeleul cheolihagiwihan modelling mich yecheug. Freemangwa Williams [40]eun VAR model-eul sayonghaessseubnida.
jeongchaeggwa gwanlyeondoen jipyo byeonsuleul bunseoghabnida.
gyeongje. geudeul-eun geudeul-ui model-eul gujo bangjeongsig modelgwa bigyohaess-eumyeo [51] varga kaebcheoe deo na-eun geos-eul balgyeonhaessseubnida.
jeongchaeg naesaengseong. 10 nyeon hu, Robertson et al. [96]
migug GDPleul sayonghayeo VAR model-e uijonhaessseubnida.
6 gaji gyeongje jipyoga bujeonghwag han geos-eulo natanassseubnida
VARui sajeon jeyag-eunboda jeonghwaghan yecheug-eulo ieojil su issseubnida. Abonazel et al. [2]neun ARIMA model-eul sayonghayeo yecheughaessseubnida
2019 nyeonbuteo hyanghu 10 nyeongan ijibteu GDP mich Salisu ET
al. [99] seog-yu bulhwagsilseong jaegoga 33e eotteon yeonghyang-eul michineunji bunseoghaessseubnida.
guggaui GDPwa gugga gan-ui yeonghyang
geullobeol varleul sayonghabnida. Iorio et al. [55] peulangseuleul bigyohaessda
dog-il-ui sil-eoblyulgwa GDP seongjanglyul
VAR model-ui milae gibon. ARIMA mich VAR model-eun dayanghan gyelyang gyeongjehag-e neolli jeog-yongdoego issseubnida.
geum-yung eung-yong peulogeulaem, extra_data=""{'confiden..."")",,
11,"The ARIMA model has been prominent for nearly half a
century in time series forecasting after being introduced by
Box and Jenkins [11]. It is a combination of three components, auto-regressive model (AR), integrated average (I)
and moving average model (MA). ARIMA models have
three parameters to represent the three part in this model
respectively, written as ARIMA(p, d, q). AR(p) represents
the past value which is used to predict, p could be determined by PACF (partial auto-correlation function). I(d)
is the times of differences to ensure the data stable, we
use the ADF (augmented Dickey-Fuller test) to help us to
find d. MA(q) expresses the current data and errors in
the past q values, which is found by by analysing the ACF
(auto-correlation function).","ARIMA 모형은 Box and Jenkins [11]에 의해 도입된 이후 거의 반세기 동안 시계열 예측에서 두드러졌다. ARIMA 모형은 자기 회귀 모형(AR), 통합 평균 모형(I), 이동 평균 모형(MA)의 3가지 구성 요소를 조합한 모형으로, 이 모형에서 3가지 부분을 각각 ARIMA(p, d, q)로 표현하며, AR(p)는 예측에 사용되는 과거 값, p는 PACF(partial auto-correlation function)로 결정할 수 있다. I(d)는 자료의 안정성을 확보하기 위해 ADF(augmented Dickey-Fuller test)를 활용하여 d를 찾는데, MA(q)는 과거 q값에서 현재의 자료와 오차를 표현하며, ACF(auto-correlation function)를 분석하여 d를 찾는다.","Translated(src=en, dest=ko, text=Arima 모델은 거의 절반에 걸쳐 두드러졌습니다.
출시 된 후 시계열 예측의 세기
박스와 젠킨스 [11].세 가지 구성 요소, AR (Auto-Regressive Model), 통합 평균 (I)의 조합입니다.
이동 평균 모델 (MA).Arima 모델이 있습니다
이 모델의 세 부분을 나타내는 3 가지 매개 변수
각각 Arima (P, D, Q)로 작성되었습니다.ar (p)를 나타냅니다
PACF (부분 자동 상관 함수)에 의해 P를 결정할 수 있습니다.ID)
데이터가 안정되기위한 차이의 시간입니다.
ADF (Augmented Dickey-Fuller Test)를 사용하여
d를 찾으십시오.MA (Q)는 현재 데이터와 오류를 표현합니다.
ACF를 분석하여 발견되는 과거 Q 값
(자동 상관 함수)., pronunciation=Arima model-eun geoui jeolban-e geolchyeo dudeuleojyeossseubnida.
chulsi doen hu sigyeyeol yecheug-ui segi
bagseuwa jenkinseu [11]. se gaji guseong yoso, AR (Auto-Regressive Model), tonghab pyeong-gyun (I)ui johab-ibnida.
idong pyeong-gyun model (MA). Arima model-i issseubnida
i model-ui se bubun-eul natanaeneun 3 gaji maegae byeonsu
gaggag Arima (P, D, Q)lo jagseongdoeeossseubnida. ar (p)leul natanaebnida
PACF (bubun jadong sang-gwan hamsu)e uihae Pleul gyeoljeonghal su issseubnida. ID)
deiteoga anjeongdoegiwihan chaiui sigan-ibnida.
ADF (Augmented Dickey-Fuller Test)leul sayonghayeo
dleul chaj-eusibsio. MA (Q)neun hyeonjae deiteowa olyuleul pyohyeonhabnida.
ACFleul bunseoghayeo balgyeondoeneun gwageo Q gabs
(jadong sang-gwan hamsu)., extra_data=""{'confiden..."")",,
12,"In the recursive strategy shown in Figure 6, we will use
the optimal model and data partitioning evaluated in the
direct strategy. First we need to train the model, and to
ensure consistency here we use the same training set as
in the direct strategy for training. The second step is to
use the data to make forecasts. Unlike the direct strategy
where the GDP growth rate is the target, this step target is features, but the independent variables used are still
the same. Our model features multivariate input and one
output (multi-to-one), so we can only predict one feature
at a time, and we will predict all features in Step 2.3 and
Step 2.4. When we finish this two steps to forecast and
then determine whether the length of the current features
is enough for us to predict the decadal GDP growth rate
in Step 2.5. If the answer is no, then we go back to the
beginning of Step 2.3 and continue to predict the new features. However, if the answer is yes, then we make the
final prediction, i.e. the decadal GDP growth rate.","그림 6에 나타난 재귀 전략에서는 직접 전략에서 평가한 최적 모형과 자료 분할을 이용하게 되며, 여기서는 먼저 모형을 학습시켜야 하는데, 여기서는 학습을 위해 직접 전략에서와 동일한 학습 집합을 이용하게 되며, 두 번째 단계는 GDP 성장률이 목표인 직접 전략과는 달리 자료를 이용하여 예측을 하는데, 우리의 모형은 다변량 투입과 하나의 산출(multi-to-one)이므로 한 번에 하나의 특징만을 예측할 수 있으며, 2.3단계와 2.4단계에서 모든 특징을 예측하게 된다. 이 두 단계를 마무리하여 예측한 후 현재 특징의 길이가 2.5 단계에서 GDP 성장률을 예측하는데 충분한지 여부를 판단하면, 대답이 없으면 2.3 단계의 시작으로 돌아가서 새로운 특징을 계속 예측하게 되는데, 대답이 yes이면 최종 예측, 즉 GDP 성장률을 예측한다.","Translated(src=en, dest=ko, text=그림 6에 표시된 재귀 전략에서 우리는
최적의 모델 및 데이터 파티셔닝에서 평가되었습니다
직접 전략.먼저 우리는 모델을 훈련시켜야합니다.
여기서 일관성을 확인하십시오. 우리는 동일한 교육 세트를 사용합니다.
훈련을위한 직접 전략에서.두 번째 단계는
데이터를 사용하여 예측을 작성하십시오.직접 전략과 달리
GDP 성장률이 목표 인 경우,이 단계 목표는 기능이지만 사용 된 독립 변수는 여전히
똑같다.우리의 모델에는 다변량 입력과 하나가 있습니다
출력 (다중-하나)이므로 하나의 기능 만 예측할 수 있습니다.
한 번에, 우리는 2.3 단계에서 모든 기능을 예측할 것입니다.
2.4 단계.우리 가이 두 단계를 마치면 예측을 마치고
그런 다음 현재 기능의 길이 여부를 결정하십시오
우리가 10 년 GDP 성장률을 예측하기에 충분합니다.
2.5 단계에서.대답이 아니오라면, 우리는
2.3 단계의 시작 및 새로운 기능을 계속 예측합니다.그러나 대답이 예라면, 우리는
최종 예측, 즉 10 년 GDP 성장률., pronunciation=geulim 6e pyosidoen jaegwi jeonlyag-eseo ulineun
choejeog-ui model mich deiteo patisyeoning-eseo pyeong-gadoeeossseubnida
jigjeob jeonlyag. meonjeo ulineun model-eul hunlyeonsikyeoyahabnida.
yeogiseo ilgwanseong-eul hwag-inhasibsio. ulineun dong-ilhan gyoyug seteuleul sayonghabnida.
hunlyeon-eul-wihan jigjeob jeonlyag-eseo. du beonjjae dangyeneun
deiteoleul sayonghayeo yecheug-eul jagseonghasibsio. jigjeob jeonlyaggwa dalli
GDP seongjanglyul-i mogpyo in gyeong-u,i dangye mogpyoneun gineung-ijiman sayong doen doglib byeonsuneun yeojeonhi
ttoggatda. uliui model-eneun dabyeonlyang iblyeoggwa hanaga issseubnida
chullyeog (dajung-hana)imeulo hanaui gineung man yecheughal su issseubnida.
han beon-e, ulineun 2.3 dangyeeseo modeun gineung-eul yecheughal geos-ibnida.
2.4 dangye. uli gai du dangyeleul machimyeon yecheug-eul machigo
geuleon da-eum hyeonjae gineung-ui gil-i yeobuleul gyeoljeonghasibsio
uliga 10 nyeon GDP seongjanglyul-eul yecheughagie chungbunhabnida.
2.5 dangyeeseo. daedab-i aniolamyeon, ulineun
2.3 dangyeui sijag mich saeloun gineung-eul gyesog yecheughabnida. geuleona daedab-i yelamyeon, ulineun
choejong yecheug, jeug 10 nyeon GDP seongjanglyul., extra_data=""{'confiden..."")",,
13,"In particular, we identify the main channels of risk propagation in a recurrent form to account of all the existing evidence of feedback effects in a macroeconomic system by
putting all the components together in a multivariate structure. Our approach takes into account the dynamic nature
of the economy, through the multivariate training of deep
neural networks, that employ multivariate input and output
layers which are able to capture the cross correlation between macroeconomic variables. Training is performed as
one big complex network minimizing estimation errors and
double counting effects among various financial variables.
Benchmarking a series of Deep Learning algorithms versus
Bayesian Model regressions on a test sample that includes a
financial turbulent period in the US (2008 – 2012) we find that
Deep Learning models provide better forecast both on a static
perspective (model train in 1973-2005 period and forecast on
2006 – 2018 period) and a dynamic perspective (initial model
train in 1973-2005 period and rolling forecast with continuous re-training during the 2006 – 2018 period). Examining
both error metrics and relevant plots it is evident that deep
learning algorithms capture better the realized trends, especially in cases where the absence of linearities and the contemporaneous dependencies cause traditional modes to overshoot","특히, 우리는 다변량 구조에 모든 구성 요소들을 통합하여 거시 경제 시스템에서 피드백 효과가 존재하는 모든 것을 고려하여 위험 전파의 주요 경로를 순환적 형태로 파악하는데, 우리의 접근은 거시 경제 변수들 간의 교차 상관관계를 포착할 수 있는 다변량 입력과 산출 계층을 이용하는 심층 신경망의 다변량 학습을 통해 추정 오류와 이중 집계 효과를 최소화하는 하나의 큰 복합 네트워크로 학습을 수행한다. 일련의 딥러닝 알고리즘 대 베이지안 모델 회귀분석을 미국(2008-2012년)의 금융황황 기간을 포함하는 검정 표본에 대해 벤치마킹한 결과, 딥러닝 모델은 정적 관점(1973-2005년 기간의 모델 트레이닝과 2006-2018년 기간의 예측)과 동적 관점(1973-2005년 기간의 초기 모델 트레이닝과 2006-2018년 기간의 지속적인 재학습으로 롤링 예측) 모두에서 더 나은 예측을 제공한다는 것은 오차 측정치와 관련 플롯을 모두 살펴보면 딥러닝 알고리즘이 실현된 트렌드를 더 잘 포착하고 있음을 알 수 있다.","Translated(src=en, dest=ko, text=특히, 우리는 거시 경제 시스템에서 기존의 피드백 효과의 모든 증거를 고려하여 재발 형태로 위험 전파의 주요 채널을 식별합니다.
모든 구성 요소를 다변량 구조로 정리합니다.우리의 접근 방식은 역동적 인 특성을 고려합니다
경제의 다변량 훈련을 통해
다변량 입력 및 출력을 사용하는 신경망
거시 경제 변수 사이의 교차 상관 관계를 포착 할 수있는 층.훈련은 다음과 같이 수행됩니다
추정 오류를 최소화하는 하나의 큰 복잡한 네트워크
다양한 재무 변수 간의 이중 계산 효과.
일련의 딥 러닝 알고리즘 대 벤치마킹
테스트 샘플에 대한 베이지안 모델 회귀
미국의 금융 난류 기간 (2008 - 2012)
딥 러닝 모델은 정적에서 더 나은 예측을 제공합니다.
원근법 (1973-2005 년의 모델 열차 및 예측
2006 - 2018 기간) 및 동적 관점 (초기 모델
1973-2005 년에 훈련하고 2006 년-2018 년 동안 지속적인 재 훈련으로 롤링 예측).검사
오류 메트릭과 관련 도표는 모두 깊은 곳에서 분명합니다.
학습 알고리즘은 특히 선형성이없고 동시 의존성이없는 경우 기존 모드가 오버 슈트를 일으키는 경우 실현 된 추세를 더 잘 포착합니다., pronunciation=teughi, ulineun geosi gyeongje siseutem-eseo gijon-ui pideubaeg hyogwaui modeun jeung-geoleul golyeohayeo jaebal hyeongtaelo wiheom jeonpaui juyo chaeneol-eul sigbyeolhabnida.
modeun guseong yosoleul dabyeonlyang gujolo jeonglihabnida. uliui jeobgeun bangsig-eun yeogdongjeog in teugseong-eul golyeohabnida
gyeongje-ui dabyeonlyang hunlyeon-eul tonghae
dabyeonlyang iblyeog mich chullyeog-eul sayonghaneun singyeongmang
geosi gyeongje byeonsu saiui gyocha sang-gwan gwangyeleul pochag hal su-issneun cheung. hunlyeon-eun da-eumgwa gat-i suhaengdoebnida
chujeong olyuleul choesohwahaneun hanaui keun bogjabhan neteuwokeu
dayanghan jaemu byeonsu gan-ui ijung gyesan hyogwa.
illyeon-ui dib leoning algolijeum dae benchimaking
teseuteu saempeul-e daehan beijian model hoegwi
migug-ui geum-yung nanlyu gigan (2008 - 2012)
dib leoning model-eun jeongjeog-eseo deo na-eun yecheug-eul jegonghabnida.
wongeunbeob (1973-2005 nyeon-ui model yeolcha mich yecheug
2006 - 2018 gigan) mich dongjeog gwanjeom (chogi model
1973-2005 nyeon-e hunlyeonhago 2006 nyeon-2018 nyeon dong-an jisogjeog-in jae hunlyeon-eulo lolling yecheug). geomsa
olyu meteuliggwa gwanlyeon dopyoneun modu gip-eun gos-eseo bunmyeonghabnida.
hagseub algolijeum-eun teughi seonhyeongseong-ieobsgo dongsi uijonseong-ieobsneun gyeong-u gijon modeuga obeo syuteuleul il-eukineun gyeong-u silhyeon doen chuseleul deo jal pochaghabnida., extra_data=""{'confiden..."")",,
14,"This first attempt at employing Deep Learning in macroeconomic time series forecasting shows that potential benefits may pave the ground for a wider spectrum of application in economic sciences. Of course, deep learning techniques even though they better address non-linear patterns
they are not a panacea, especially in such challenging problem as the prediction of the Sub-prime crisis in the US, but
they certainly lie in the correct path. Criticism could rely on
the black box nature of the algorithm which when compared
to traditional econometrics does not provide a clear view of
the economic relationships, but in any case, as it has been
proven empirically,the economy is not dominated by clear
linear patterns but from non-linear interactions which constantly evolve. Especially under the rolling forecast framework where both techniques follow the realized trend, one
could use the results of the 2 techniques (Linear models –
Deep Learning models) in a combined way, so that the linear
model provides a first order approximation of the problem at
hand revealing the economic rational and use also the more
precise non-linear model to correct for temporal fluctuations.","딥러닝을 거시경제 시계열 예측에 활용한 이 같은 첫 시도는 잠재적인 이점이 경제학 분야에서 더 넓은 스펙트럼의 적용을 가능하게 할 수 있음을 보여주는 것이고, 물론 딥러닝 기법은 비선형적 패턴을 보다 잘 다루고 있지만 특히 미국 서브프라임 위기 예측과 같은 어려운 문제에서는 정확히 옳은 길에 놓여 있다는 비판이 있을 수 있지만, 그럼에도 불구하고 실증적으로 증명된 바와 같이 경제는 명확한 선형적 패턴이 지배하는 선형적 패턴이 아니라 비선형적 상호작용이다. 특히 두 기법이 실현된 추세를 따르는 롤링 예측 프레임워크 하에서는 두 기법(선형 모형 – 딥러닝 모형)의 결과를 결합하여 선형 모형이 경제적 합리성을 드러내는 문제의 1차 근사식을 제공하고 보다 정교한 비선형 모형도 시간적 변동을 보정할 수 있다.","Translated(src=en, dest=ko, text=거시 경제 시계열에서 딥 러닝을 사용하려는 첫 번째 시도는 잠재적 이점이 경제 과학에 더 넓은 스펙트럼의 적용을위한 근거를 만들어 낼 수 있음을 보여줍니다.물론, 비선형 패턴을 더 잘 해결하더라도 딥 러닝 기술
그들은 만병 통치약이 아니며, 특히 미국의 서브 프라임 위기의 예측과 같은 도전적인 문제에서
그들은 확실히 올바른 길에 있습니다.비판은 의존 할 수 있습니다
비교할 때 알고리즘의 블랙 박스 특성
전통적인 경제학에 대한 명확한 견해를 제공하지 않습니다.
경제 관계, 그러나 어쨌든
경험적으로 입증 된 경제는 분명하게 지배되지 않습니다.
선형 패턴이지만 끊임없이 진화하는 비선형 상호 작용에서.특히 두 기술이 실현 된 트렌드를 따르는 롤링 예측 프레임 워크에서
두 가지 기술의 결과 (선형 모델 -
딥 러닝 모델)) 결합 된 방식으로 선형
모델은 문제의 1 차 근사치를 제공합니다.
경제적 합리성을 드러내고 더 많이 사용합니다.
시간적 변동을 수정하기위한 정확한 비선형 모델., pronunciation=geosi gyeongje sigyeyeol-eseo dib leoning-eul sayonghalyeoneun cheos beonjjae sidoneun jamjaejeog ijeom-i gyeongje gwahag-e deo neolb-eun seupegteuleom-ui jeog-yong-eul-wihan geungeoleul mandeul-eo nael su iss-eum-eul boyeojubnida. mullon, biseonhyeong paeteon-eul deo jal haegyeolhadeolado dib leoning gisul
geudeul-eun manbyeong tongchiyag-i animyeo, teughi migug-ui seobeu peulaim wigiui yecheuggwa gat-eun dojeonjeog-in munjeeseo
geudeul-eun hwagsilhi olbaleun gil-e issseubnida. bipan-eun uijon hal su issseubnida
bigyohal ttae algolijeum-ui beullaeg bagseu teugseong
jeontongjeog-in gyeongjehag-e daehan myeonghwaghan gyeonhaeleul jegonghaji anhseubnida.
gyeongje gwangye, geuleona eojjaessdeun
gyeongheomjeog-eulo ibjeung doen gyeongjeneun bunmyeonghage jibaedoeji anhseubnida.
seonhyeong paeteon-ijiman kkeunh-im-eobs-i jinhwahaneun biseonhyeong sangho jag-yong-eseo. teughi du gisul-i silhyeon doen teulendeuleul ttaleuneun lolling yecheug peuleim wokeueseo
du gaji gisul-ui gyeolgwa (seonhyeong model -
dib leoning model)) gyeolhab doen bangsig-eulo seonhyeong
model-eun munje-ui 1 cha geunsachileul jegonghabnida.
gyeongjejeog habliseong-eul deuleonaego deo manh-i sayonghabnida.
siganjeog byeondong-eul sujeonghagiwihan jeonghwaghan biseonhyeong model., extra_data=""{'confiden..."")",,
15,"The Coronavirus Disease 2019 or the COVID-19 pandemic has swept almost
all parts of the world since the first case was found in Wuhan, China, in December 2019. With the increasing number of COVID-19 cases in the world,
SARS-CoV-2 has mutated into various variants. Given the increasingly dangerous conditions of the pandemic, it is crucial to know when the pandemic
will stop by predicting confirmed cases of COVID-19. Therefore, many studies have raised COVID-19 as a case study to overcome the ongoing pandemic
using the Deep Learning method, namely LSTM, with reasonably accurate
results and small error values. LSTM training is used to predict confirmed
cases of COVID-19 based on variants that have been identified using ECDC’s
COVID-19 dataset containing confirmed cases of COVID-19 that have been
identified from 30 countries in Europe. Tests were conducted using the LSTM
and BiLSTM models with the addition of RNN as comparisons on hidden
size and layer size. The obtained result showed that in testing hidden sizes
25, 50, 75 to 100, the RNN model provided better results, with the minimum
MSE value of 0.01 and the RMSE value of 0.012 for B.1.427/B.1.429 variant
with hidden size 100. In further testing of layer sizes 2, 3, 4, and 5, the result
shows that the BiLSTM model provided better results, with minimum MSE
value of 0.01 and the RMSE of 0.01 for the B.1.427/B.1.429 variant with
hidden size 100 and layer size 2.","2019년 코로나 바이러스 감염증(코로나19) 또는 코로나19 팬데믹은 2019년 12월 중국 우한에서 첫 환자가 발생한 이후 전 세계 거의 모든 지역을 휩쓸고 있으며, 전 세계적으로 코로나19 환자가 증가하면서 사스-CoV-2가 다양한 변종으로 변종화되면서, 코로나19 확진자가 언제 멈출지 예측하는 것이 점점 더 위험해지는 상황을 감안할 때, 현재 진행 중인 팬데믹을 극복하기 위해 딥러닝(LSTM)을 활용한 사례 연구가 다수 제기되었다. LSTM 트레이닝은 유럽의 30개국에서 확인된 코로나19 확진 사례를 포함하는 ECDC의 코로나19 데이터셋을 이용하여 확인된 변종을 기반으로 코로나19 확진 사례를 예측하는데 LSTM 및 BiLSTM 모형을 이용하여 히든 사이즈와 층 사이즈에 대한 비교를 실시한 결과 히든 사이즈 25, 50, 75, 100에서 RNN 모형이 최소 MSE 값 0.01, RMSE 값 0.012로 히든 사이즈 100을 가진 B.1.427/B.1.429 변종이 더 우수한 결과를 보였다. 층 크기 2, 3, 4, 5의 추가 테스트에서 BiLSTM 모델이 hidden size 100과 층 크기 2를 갖는 B.1.427/B.1.429 변이체에 대해 최소 MSE 값 0.01, RMSE 값 0.01로 더 나은 결과를 제시하였다.","Translated(src=en, dest=ko, text=코로나 바이러스 질병 2019 또는 Covid-19 Pandemic은 거의 휩쓸 렸습니다.
첫 번째 사건 이후 세계의 모든 지역은 2019 년 12 월 중국 우한에서 발견되었습니다. 세계에서 Covid-19 사례가 증가함에 따라
SARS-COV-2는 다양한 변이체로 돌연변이되었다.전염병의 위험한 조건이 점점 더 위험한 상태를 감안할 때, 유행성이 언제인지 아는 것이 중요합니다.
COVID-19의 확인 된 사례를 예측하여 중단됩니다.따라서 많은 연구가 진행중인 유행성을 극복하기위한 사례 연구로 Covid-19를 제기했습니다.
딥 러닝 방법, 즉 LSTM을 사용하여 합리적으로 정확합니다.
결과 및 작은 오류 값.LSTM 교육은 확인 된 예측에 사용됩니다
ECDC를 사용하여 확인 된 변형에 기초한 Covid-19 사례
COVID-19 데이터 세트가 확인 된 CovID-19의 경우
유럽의 30 개국에서 확인되었습니다.LSTM을 사용하여 테스트를 수행 하였다
그리고 숨겨진 것에 대한 비교로 RNN을 추가 한 BILSTM 모델
크기와 레이어 크기.얻은 결과는 테스트에서 숨겨진 크기를 보여 주었다
25, 50, 75 ~ 100, RNN 모델은 최소값으로 더 나은 결과를 제공했습니다.
B.1.427/B.1.429 변형에 대해 0.01의 MSE 값 및 0.012의 RMSE 값
숨겨진 크기 100. 층 크기 2, 3, 4 및 5의 추가 테스트에서 결과
BILSTM 모델이 최소 MSE로 더 나은 결과를 제공했음을 보여줍니다.
B.1.427/B.1.429의 경우 0.01 값 및 0.01의 RMSE
숨겨진 크기 100 및 레이어 크기 2., pronunciation=kolona baileoseu jilbyeong 2019 ttoneun Covid-19 Pandemiceun geoui hwibsseul lyeossseubnida.
cheos beonjjae sageon ihu segyeui modeun jiyeog-eun 2019 nyeon 12 wol jung-gug uhan-eseo balgyeondoeeossseubnida. segyeeseo Covid-19 salyega jeung-gaham-e ttala
SARS-COV-2neun dayanghan byeon-ichelo dol-yeonbyeon-idoeeossda. jeon-yeombyeong-ui wiheomhan jogeon-i jeomjeom deo wiheomhan sangtaeleul gam-anhal ttae, yuhaengseong-i eonjeinji aneun geos-i jung-yohabnida.
COVID-19ui hwag-in doen salyeleul yecheughayeo jungdandoebnida. ttalaseo manh-eun yeonguga jinhaengjung-in yuhaengseong-eul geugboghagiwihan salye yeongulo Covid-19leul jegihaessseubnida.
dib leoning bangbeob, jeug LSTMeul sayonghayeo hablijeog-eulo jeonghwaghabnida.
gyeolgwa mich jag-eun olyu gabs. LSTM gyoyug-eun hwag-in doen yecheug-e sayongdoebnida
ECDCleul sayonghayeo hwag-in doen byeonhyeong-e gichohan Covid-19 salye
COVID-19 deiteo seteuga hwag-in doen CovID-19ui gyeong-u
yuleob-ui 30 gaegug-eseo hwag-indoeeossseubnida. LSTMeul sayonghayeo teseuteuleul suhaeng hayeossda
geuligo sumgyeojin geos-e daehan bigyolo RNNeul chuga han BILSTM model
keugiwa leieo keugi. eod-eun gyeolgwaneun teseuteueseo sumgyeojin keugileul boyeo jueossda
25, 50, 75 ~ 100, RNN model-eun choesogabs-eulo deo na-eun gyeolgwaleul jegonghaessseubnida.
B.1.427/B.1.429 byeonhyeong-e daehae 0.01ui MSE gabs mich 0.012ui RMSE gabs
sumgyeojin keugi 100. cheung keugi 2, 3, 4 mich 5ui chuga teseuteueseo gyeolgwa
BILSTM model-i choeso MSElo deo na-eun gyeolgwaleul jegonghaess-eum-eul boyeojubnida.
B.1.427/B.1.429ui gyeong-u 0.01 gabs mich 0.01ui RMSE
sumgyeojin keugi 100 mich leieo keugi 2., extra_data=""{'confiden..."")",,
16,"Our first test was conducted by testing the number of hidden sizes implemented, combined with layer size 1 for each test, to avoid bias. Hidden sizes
tested are 25, 50, 75, and 100. The optimal hidden size was then obtained
by examining which hidden size gave the largest frequency of minimum loss
value from each COVID-19 variant. Then the second test was carried out
by testing the number of layer sizes implemented starting from 2, 3, 4, to
5, combined with the optimal hidden size value obtained from the previous
test. The result of the layer size test was determined in the same approach
as the hidden size one. The final result is the optimal model configuration of
both hidden and layer sizes to be implemented.","우리의 첫 번째 테스트는 각 테스트마다 레이어 사이즈 1과 결합하여 구현되는 히든 사이즈의 개수를 테스트하여 편의를 회피하였고, 테스트된 히든 사이즈는 25, 50, 75, 100으로 각각의 코로나19 변이체로부터 최소 손실 값의 빈도가 어느 히든 사이즈가 가장 큰지를 검사하여 최적 히든 사이즈를 구하였다. 두 번째 테스트는 레이어 사이즈 테스트로부터 획득된 최적 히든 사이즈 값과 결합하여 구현되는 레이어 사이즈의 개수를 테스트하여 최종 결과가 결정되었다.","Translated(src=en, dest=ko, text=첫 번째 테스트는 바이어스를 피하기 위해 각 테스트마다 레이어 크기 1과 결합 된 숨겨진 크기의 수를 테스트하여 수행되었습니다.숨겨진 크기
테스트 된 25, 50, 75 및 100입니다. 최적의 숨겨진 크기를 얻었습니다.
어떤 숨겨진 크기가 가장 큰 최소 손실 주파수를 조사함으로써
각 covid-19 변형으로부터의 값.그런 다음 두 번째 테스트가 수행되었습니다
2, 3, 4에서 시작하여 구현 된 레이어 크기 수를 테스트하여
5, 이전에서 얻은 최적의 숨겨진 크기 값과 결합
시험.층 크기 테스트의 결과는 동일한 접근법에서 결정되었습니다.
숨겨진 크기 1으로.최종 결과는 최적의 모델 구성입니다
숨겨진 크기와 레이어 크기가 모두 구현됩니다., pronunciation=cheos beonjjae teseuteuneun baieoseuleul pihagi wihae gag teseuteumada leieo keugi 1gwa gyeolhab doen sumgyeojin keugiui suleul teseuteuhayeo suhaengdoeeossseubnida. sumgyeojin keugi
teseuteu doen 25, 50, 75 mich 100ibnida. choejeog-ui sumgyeojin keugileul eod-eossseubnida.
eotteon sumgyeojin keugiga gajang keun choeso sonsil jupasuleul josaham-eulosseo
gag covid-19 byeonhyeong-eulobuteoui gabs. geuleon da-eum du beonjjae teseuteuga suhaengdoeeossseubnida
2, 3, 4eseo sijaghayeo guhyeon doen leieo keugi suleul teseuteuhayeo
5, ijeon-eseo eod-eun choejeog-ui sumgyeojin keugi gabsgwa gyeolhab
siheom. cheung keugi teseuteuui gyeolgwaneun dong-ilhan jeobgeunbeob-eseo gyeoljeongdoeeossseubnida.
sumgyeojin keugi 1eulo. choejong gyeolgwaneun choejeog-ui model guseong-ibnida
sumgyeojin keugiwa leieo keugiga modu guhyeondoebnida., extra_data=""{'confiden..."")",,
17,"Noisy estimates are a major concern in affinity score computation. Keyword matching accuracy depends
on authors and reviewers using consistent terminology, and subtleties are lost in the process. Textual similarity scores rely on inherently noisy language models. Reviewers typically only bid on a small fraction of
papers; conferences often assume that the absence of a bid implies disinterest, but in conferences with many
thousands of papers this assumption is likely inaccurate. Finally, there is some inherent modeling uncertainty
in the entire process. The affinity scores are ultimately intended to predict downstream review quality, but it
is unclear how keyword matching, textual similarity, and bids correlate with review quality, or even how to
measure review quality.","잡음 추정은 친밀도 점수 계산에서 주요한 관심사인데, 키워드 매칭 정확도는 일관된 용어를 사용하는 저자와 심사자에게 달려 있고, 그 과정에서 미묘함이 사라지는데, 텍스트 유사도 점수는 본질적으로 잡음 잡음한 언어 모델에 의존하는데, 심사자는 일반적으로 소수의 논문에만 낙찰을 한다고 가정하는데, 심사자들은 종종 낙찰의 부재가 불미함을 내포한다고 가정하는데, 최종적으로는 일부 본질적인 모델링 불확실성이 존재한다. 친밀도 점수는 궁극적으로 하류의 심사 품질을 예측하는데 목적이 있지만, 키워드 매칭, 텍스트 유사도, 텍스트 유사도, 낙찰이 심사와 어떻게 상관관계가 있는지조차도 불분명하다.","Translated(src=en, dest=ko, text=시끄러운 추정치는 친화력 점수 계산의 주요 관심사입니다.키워드 일치 정확도는 달라집니다
일관된 용어를 사용하는 저자와 검토 자에게는 그 과정에서 미묘함이 손실됩니다.텍스트 유사성 점수는 본질적으로 시끄러운 언어 모델에 의존합니다.검토자는 일반적으로 작은 부분에만 입찰합니다
서류;회의는 종종 입찰의 부재가 무관심을 의미한다고 가정하지만 많은 사람들과의 회의에서
이 가정은 수천 개의 논문이 부정확 할 것입니다.마지막으로, 고유 한 모델링 불확실성이 있습니다
전체 과정에서.친화력 점수는 궁극적으로 다운 스트림 검토 품질을 예측하기위한 것이지만
키워드 일치, 텍스트 유사성 및 입찰이 검토 품질과 어떻게 관련이 있는지 또는 심지어
검토 품질을 측정하십시오., pronunciation=sikkeuleoun chujeongchineun chinhwalyeog jeomsu gyesan-ui juyo gwansimsaibnida. kiwodeu ilchi jeonghwagdoneun dallajibnida
ilgwandoen yong-eoleul sayonghaneun jeojawa geomto ja-egeneun geu gwajeong-eseo mimyoham-i sonsildoebnida. tegseuteu yusaseong jeomsuneun bonjiljeog-eulo sikkeuleoun eon-eo model-e uijonhabnida. geomtojaneun ilbanjeog-eulo jag-eun bubun-eman ibchalhabnida
seolyu; hoeuineun jongjong ibchal-ui bujaega mugwansim-eul uimihandago gajeonghajiman manh-eun salamdeulgwaui hoeuieseo
i gajeong-eun sucheon gaeui nonmun-i bujeonghwag hal geos-ibnida. majimag-eulo, goyu han modelling bulhwagsilseong-i issseubnida
jeonche gwajeong-eseo. chinhwalyeog jeomsuneun gung-geugjeog-eulo daun seuteulim geomto pumjil-eul yecheughagiwihan geos-ijiman
kiwodeu ilchi, tegseuteu yusaseong mich ibchal-i geomto pumjilgwa eotteohge gwanlyeon-i issneunji ttoneun simjieo
geomto pumjil-eul cheugjeonghasibsio., extra_data=""{'confiden..."")",,
18,"Because RRA optimizes against an uncertainty-aware adversary, we avoid assigning paper-reviewer pairs
with large amounts of uncertainty. Thus our approach broadly encourages assigning reviewers to papers for
which we are certain the affinity is high. Some special cases of uncertainty sets (singleton, rectangular, or
spherical) reduce to problems without uncertainty; these can all be solved via linear programming. However
these limited circumstances generally do not apply to most conference setups. In general, accounting for uncertainty yields significantly different solutions that are inherently more robust than optimizing to pointwise
affinity score estimates. Our methods satisfyingly address the convex relaxation of the reviewer allocation
problem, permitting fractional allocations, and we then approximate the optimal discrete allocations via randomized rounding techniques that preserve the cardinality constraints of papers and reviewers","RRA는 불확실성 인식 적대자에 대해 최적화하기 때문에 불확실성이 큰 논문-심사자 쌍을 배정하는 것을 회피하고, 우리의 접근법은 불확실성이 높은 논문에 대해 심사자를 배정하는 것을 광범위하게 권장한다. 불확실성 세트(singleton, 직사각형 또는 구형)의 일부 특례는 불확실성이 없는 문제로 감소하는데, 이들은 모두 선형 프로그래밍을 통해 해결될 수 있지만, 이러한 제한적인 상황은 일반적으로 대부분의 회의 세트에 적용되지 않는다. 불확실성을 고려하면 불확실성을 고려하는 것이 태생적으로 더 강건하다. 우리의 방법들은 검토자 할당 문제의 볼록 완화(convex relaxation)를 만족스럽게 해결하여 분할 할당을 허용하고, 이어서 논문과 검토자의 추상성 제약을 보존하는 무작위 라운드링(randomized rounding) 기법을 통해 최적의 이산적 할당을 근사한다.","Translated(src=en, dest=ko, text=RRA는 불확실성 인식 대적에 대해 최적화하기 때문에 종이 검토기 쌍을 할당하지 않습니다.
많은 양의 불확실성으로.따라서 우리의 접근 방식은 검토자가 종이에 배정하는 것을 광범위하게 권장합니다.
우리는 친화력이 높다고 확신합니다.불확실성 세트의 특별한 경우 (싱글 톤, 직사각형 또는
구형) 불확실성이없는 문제로 줄어 듭니다.이들은 모두 선형 프로그래밍을 통해 해결할 수 있습니다.하지만
이러한 제한된 상황은 일반적으로 대부분의 컨퍼런스 설정에 적용되지 않습니다.일반적으로 불확실성을 설명하면 지적을 최적화하는 것보다 본질적으로 더 강력한 솔루션이 상당히 다릅니다.
친화력 점수 추정.우리의 방법은 검토 자 할당의 볼록 완화를 만족스럽게 다루고 있습니다.
문제, 분수 할당을 허용 한 다음 논문 및 검토 자의 기타 제약 조건을 보존하는 무작위 반올림 기술을 통해 최적의 개별 할당을 근사화합니다., pronunciation=RRAneun bulhwagsilseong insig daejeog-e daehae choejeoghwahagi ttaemun-e jong-i geomtogi ssang-eul haldanghaji anhseubnida.
manh-eun yang-ui bulhwagsilseong-eulo. ttalaseo uliui jeobgeun bangsig-eun geomtojaga jong-ie baejeonghaneun geos-eul gwangbeom-wihage gwonjanghabnida.
ulineun chinhwalyeog-i nopdago hwagsinhabnida. bulhwagsilseong seteuui teugbyeolhan gyeong-u (sing-geul ton, jigsagaghyeong ttoneun
guhyeong) bulhwagsilseong-ieobsneun munjelo jul-eo deubnida. ideul-eun modu seonhyeong peulogeulaeming-eul tonghae haegyeolhal su issseubnida. hajiman
ileohan jehandoen sanghwang-eun ilbanjeog-eulo daebubun-ui keonpeoleonseu seoljeong-e jeog-yongdoeji anhseubnida. ilbanjeog-eulo bulhwagsilseong-eul seolmyeonghamyeon jijeog-eul choejeoghwahaneun geosboda bonjiljeog-eulo deo ganglyeoghan sollusyeon-i sangdanghi daleubnida.
chinhwalyeog jeomsu chujeong. uliui bangbeob-eun geomto ja haldang-ui bollog wanhwaleul manjogseuleobge dalugo issseubnida.
munje, bunsu haldang-eul heoyong han da-eum nonmun mich geomto jaui gita jeyag jogeon-eul bojonhaneun mujag-wi ban-ollim gisul-eul tonghae choejeog-ui gaebyeol haldang-eul geunsahwahabnida., extra_data=""{'confiden..."")",,
19,"Our work departs from the above by considering Q-Learning, and by lifting strong technical assumptions on the payoff functions. Specifically, we do not assume a form of the payoffs as in [26], or the
growth of the function as in [33]. In addition, we require no knowledge of the cost gradients as in [34]. Finally, our work considers the generalised class of weighted monotone games, rather than the unweighted
case considered by the above. This class of games is also considered in [35, 36], in which variations of
online gradient descent are analyzed. However, the former requires weighted strong monotonicity (which
is much more restrictive even than strict monotonicity) and the latter requires strong assumptions on the
parameters of the learning algorithm.","우리의 작업은 Q-Learning를 고려하여 위로부터 출발하고, 보수 함수에 대한 강한 기술적 가정을 해제함으로써, 구체적으로는 [26]과 같이 보수 함수의 형태를 가정하거나 [33]과 같이 함수의 성장을 가정하지 않으며, 또한 [34]과 같이 비용 기울기에 대한 지식을 고려하지 않고, 위에서 고려한 비중가중화된 게임의 클래스를 마지막으로 [35, 36]에서 분석하고 있다.","Translated(src=en, dest=ko, text=우리의 작업은 Q- 러닝을 고려하고 지불 기능에 대한 강력한 기술적 가정을 높이면서 위에서 벗어납니다.구체적으로, 우리는 [26] 또는
[33]에서와 같은 기능의 성장.또한 [34]에서와 같이 비용 기울기에 대한 지식이 필요하지 않습니다.마지막으로, 우리의 작업은 비가 중 더 가중치가 아닌 일반화 된 가중 모노톤 게임을 고려합니다.
위의 사례.이 클래스의 게임은 [35, 36]에서도 고려되며, 여기서
온라인 그라디언트 하강이 분석됩니다.그러나 전자는 가중 강력한 단조성을 요구합니다 (
엄격한 단조성보다 훨씬 더 제한적이며 후자는
학습 알고리즘의 매개 변수., pronunciation=uliui jag-eob-eun Q- leoning-eul golyeohago jibul gineung-e daehan ganglyeoghan gisuljeog gajeong-eul nop-imyeonseo wieseo beos-eonabnida. guchejeog-eulo, ulineun [26] ttoneun
[33]eseowa gat-eun gineung-ui seongjang. ttohan [34]eseowa gat-i biyong giulgie daehan jisig-i pil-yohaji anhseubnida. majimag-eulo, uliui jag-eob-eun biga jung deo gajungchiga anin ilbanhwa doen gajung monoton geim-eul golyeohabnida.
wiui salye. i keullaeseuui geim-eun [35, 36]eseodo golyeodoemyeo, yeogiseo
onlain geuladieonteu hagang-i bunseogdoebnida. geuleona jeonjaneun gajung ganglyeoghan danjoseong-eul yoguhabnida (
eomgyeoghan danjoseongboda hwolssin deo jehanjeog-imyeo hujaneun
hagseub algolijeum-ui maegae byeonsu., extra_data=""{'confiden..."")",,
20,"Though general in nature, these results are not without their limitations. They rely on the assumption
of a discrete action set, so that agent strategies all evolve on ∆. This allows us to assume the existence of
an Equilibrium, through the compactness of ∆. However, generalising to arbitrary continuous action sets
would widen the range of applications which our work encompasses. In addition, Theorem 3 is derived
for continuous time QL. This is a reasonable stance to take as it has been shown repeatedly that continuous
time approximations of algorithms provide a strong basis for analysing the algorithms themselves [47,
17]. However, the accuracy of discrete time algorithms is always dependent on parameters, most notably
step sizes. Such an analysis of the discrete variants presents a fruitful avenue for further research","본질적으로 일반적이지만, 이러한 결과는 그 한계가 없는 것은 아니지만, 에이전트 전략이 모두 ∆에 의존함으로써, equilibrium의 존재를 가정할 수 있게 되며, 또한, ∆의 압축성을 통해, 임의의 연속적 행동 집합으로 일반화하면, 우리의 연구가 포괄하는 응용의 범위가 넓어진다는 것을 반복적으로 보여준 바와 같이, 연속적 시간 QL에 대하여 도출되는 정리는 합리적인 입장이다[47, 17]. 그러나, 이산 시간 알고리즘의 정확성은 항상 매개변수에 의존하며, 특히 step 크기에 의존한다. 이와 같은 이산화탄소 변이체에 대한 분석은 추가 연구에 있어서도 유의미한 전망을 제시한다.","Translated(src=en, dest=ko, text=본질적으로는 일반적이지만 이러한 결과는 제한이 없습니다.그들은 가정에 의존합니다
이산 조치 세트의 에이전트 전략이 모두 ∆에서 진화합니다.이것은 우리가 존재를 가정 할 수있게한다
∆의 소형을 통한 평형.그러나 임의의 연속 조치 세트에 대한 일반화
우리의 작업이 포함하는 응용 프로그램의 범위를 넓힐 것입니다.또한 정리 3이 파생됩니다
연속 시간 동안 QL.이것은 반복적으로 그 연속적인 것으로 나타 났기 때문에 합리적인 입장입니다.
알고리즘의 시간 근사치는 알고리즘 자체를 분석하기위한 강력한 기초를 제공합니다 [47,
17].그러나 이산 시간 알고리즘의 정확도는 항상 매개 변수에 따라 다릅니다.
단계 크기.이산 변종에 대한 이러한 분석은 추가 연구를위한 유익한 길을 제시합니다., pronunciation=bonjiljeog-euloneun ilbanjeog-ijiman ileohan gyeolgwaneun jehan-i eobs-seubnida. geudeul-eun gajeong-e uijonhabnida
isan jochi seteuui eijeonteu jeonlyag-i modu ∆eseo jinhwahabnida. igeos-eun uliga jonjaeleul gajeong hal su-issgehanda
∆ui sohyeong-eul tonghan pyeonghyeong. geuleona im-uiui yeonsog jochi seteue daehan ilbanhwa
uliui jag-eob-i pohamhaneun eung-yong peulogeulaem-ui beom-wileul neolbhil geos-ibnida. ttohan jeongli 3i pasaengdoebnida
yeonsog sigan dong-an QL. igeos-eun banbogjeog-eulo geu yeonsogjeog-in geos-eulo nata nassgi ttaemun-e hablijeog-in ibjang-ibnida.
algolijeum-ui sigan geunsachineun algolijeum jacheleul bunseoghagiwihan ganglyeoghan gicholeul jegonghabnida [47,
17]. geuleona isan sigan algolijeum-ui jeonghwagdoneun hangsang maegae byeonsue ttala daleubnida.
dangye keugi. isan byeonjong-e daehan ileohan bunseog-eun chuga yeonguleul-wihan yuighan gil-eul jesihabnida., extra_data=""{'confiden..."")",,
